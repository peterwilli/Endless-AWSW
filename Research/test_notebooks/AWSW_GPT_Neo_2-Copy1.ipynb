{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:01.521748Z",
     "iopub.status.busy": "2021-11-15T03:02:01.521147Z",
     "iopub.status.idle": "2021-11-15T03:02:01.722640Z",
     "shell.execute_reply": "2021-11-15T03:02:01.721968Z"
    },
    "id": "2TJ-BqFtQ86M",
    "outputId": "f41c5626-6827-4d90-f0a9-8a11c01a366d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 15 03:02:01 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   58C    P0    38W /  N/A |   1309MiB / 16125MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:01.726756Z",
     "iopub.status.busy": "2021-11-15T03:02:01.726337Z",
     "iopub.status.idle": "2021-11-15T03:02:03.541850Z",
     "shell.execute_reply": "2021-11-15T03:02:03.541381Z"
    },
    "id": "oR9S63qiQt2b",
    "outputId": "b1303393-4c18-4510-da76-59e5f2590db8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/awsw-dev/.local/lib/python3.8/site-packages (4.12.3)\r\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (1.15.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/awsw-dev/.local/lib/python3.8/site-packages (from transformers) (0.1.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.26.0)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\r\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.45)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.8.28)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.0)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.12.2)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2021.8.1)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.0)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (2.0.2)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.4)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.2)\r\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (5.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2021.5.30)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.0.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.7.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (21.2.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (5.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2021.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:03.549213Z",
     "iopub.status.busy": "2021-11-15T03:02:03.548636Z",
     "iopub.status.idle": "2021-11-15T03:02:04.941659Z",
     "shell.execute_reply": "2021-11-15T03:02:04.941094Z"
    },
    "id": "GhhigZYMRK6N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from random import randrange\n",
    "import multiprocessing\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, GPTNeoForCausalLM\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:04.946142Z",
     "iopub.status.busy": "2021-11-15T03:02:04.945773Z",
     "iopub.status.idle": "2021-11-15T03:02:04.948214Z",
     "shell.execute_reply": "2021-11-15T03:02:04.948609Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:04.972111Z",
     "iopub.status.busy": "2021-11-15T03:02:04.951205Z",
     "iopub.status.idle": "2021-11-15T03:02:04.974695Z",
     "shell.execute_reply": "2021-11-15T03:02:04.974249Z"
    },
    "id": "MTduRlf-RQJa",
    "outputId": "296dba31-0af6-422c-eea8-f5c1130a5daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use cuda:0 for training with seed: 3851385137\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 2 ** 32 - 1)\n",
    "random.seed(seed)\n",
    "block_size = 64\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"Will use {device_name} for training with seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278,
     "referenced_widgets": [
      "641f6e8a671d4dfe9da34c7685520767",
      "c7eb14f1388e42f29fbac8eef195fede",
      "40b7d134ba7b40299f6bd16b1e781d09",
      "13e5a9ab53bc4e1d8c3c7905ebd05ddf",
      "bc3c9b957d93490c8f9773e7050fd4ef",
      "e702472021334098b47210f9f1395f21",
      "8ed62e486a664d1f83fee6f21cb4585d",
      "ad42520b822e4454947f5d466424c8ca",
      "917d4196cdd14970b7965652f3b950a2",
      "3d72f5178e404d8c948ef13fc81a2bb8",
      "0ae32803eeaf436ab66cb3da9ff5439d",
      "5f28c40531fc4f43b4b62fd04912100c",
      "3c7dd63eb62d44598666166a87c0ff88",
      "7ed9d2386d57488c890d1ec712d1bef0",
      "da3c1240d6a94f3c9ff9ec0545675446",
      "cc7c2ac1006b4b109f302d7d26e3a578",
      "9bb4dcdb6f434d8b8cc95ef80f2ffe00",
      "5af83a55a73b4f3481a7559951ebdf08",
      "c4384ee96a1a4ae4bc8b46fa640b9058",
      "5f2bf3aebfd64baf94df39964208af4c",
      "307da63f10ad4ae5ae5e7e387d176d39",
      "8a282a2414874cb588a386e1444951dd",
      "659e0ad7f39c4cdbbdc1345c07a2e3f4",
      "1481fbbc74544c4888c0d6f88d8b3c9d",
      "54b833977b424c25a690d1bfe5d8c3f1",
      "8bfab1db231a4ff1975775f81f8a84f1",
      "3af5019cfacf4afc8a15a14d9a121ea8",
      "0ac398931b04418cbd3d227b9c634883",
      "7789164b5ee245b3a537d9d61dae038f",
      "8abd99000df0453e86bc458be29a10c7",
      "7db95c969d94465aaffbe9139f0f2a90",
      "d21de377e9074df083556c68e7d7cdbb",
      "3a0ada2f620a40aaa6c33fcea7a0bb91",
      "38bc8c5a2a3447e7942e2058c35d5e5b",
      "79170285a49a450494f0bfbe6cb4788e",
      "9813f2bda5ce4ab6887683fe63d0629e",
      "5c6ca36359bb4a1982c599f6a3f23b9f",
      "6e2cfa054082449e942297afd2f50f73",
      "5f3133f5c7254c248dad8e75534ea920",
      "eef06f8bc02749f4b04af47d92eb91b2",
      "2c60d924ee7b4fad984e2072973c3af3",
      "c1132ffe6c684c7ebeb3f285a101536b",
      "aa2a21546de1463f88a7feb8b697f9d3",
      "0abba2f64b4a43abadb38d461a5c7f42",
      "f43e9d4fa52c48c087ffeef399bfbed6",
      "4f1eafd9553b4bd5b396bba8f3896a22",
      "bbd5e6b14d864c1eb6a4d9c4247fb520",
      "29b2e68155a947e0aca0760c7e8e4afa",
      "fa1b29c48993478f9c2be4877ec675fc",
      "e4a6b2af572e44eda537e74847c58709",
      "8110f29e7b134607b38b29fad32ab1a1",
      "470ba5ce89ee4927a9be685adbc08675",
      "d10fbe074d4b42c9a8a6522252ef016f",
      "5af59ca6be4a427c8f83a906bbb6ce93",
      "17c9592d03324c4797abc812044e7500",
      "f6153c2ce07b4f4097a2a0486fb896ac",
      "3daedb3b85264408a8821ee2e480b356",
      "b2ce0d43567b4cc09e1f5571ee25263c",
      "c3a28b9889cd40e2ac0b9860e7f3932b",
      "99e992b8d4564ddd9c7634ac7663053a",
      "ae627a7820d4450a97b47d63dd5fbd92",
      "56a8c3c144404bd793e29a023032a09f",
      "2a9348d6cd674898ac4c9a303a254f26",
      "bafebeb0fba04d148ae3adf44171e0fe",
      "a8e19067a3ad411cad1aba489d390516",
      "6cf4e15a56fc4ac188609b806f11afcd"
     ]
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:04.981947Z",
     "iopub.status.busy": "2021-11-15T03:02:04.977185Z",
     "iopub.status.idle": "2021-11-15T03:02:14.048477Z",
     "shell.execute_reply": "2021-11-15T03:02:14.047959Z"
    },
    "id": "QSVYD7o_eL2o",
    "outputId": "4f570825-b314-421b-b1ee-07449f7787f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading empty, pre-trained model with 76 parameters.\n",
      "Model attached to cuda:0\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(\"/opt/awsw\"):\n",
    "  # In case we run this locally (in Docker)\n",
    "  work_dir = os.path.join(\"/opt\", \"awsw\")\n",
    "else:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  work_dir = os.path.join(\"/content\", \"drive\", \"MyDrive\", \"endless_awsw\")\n",
    "\n",
    "models_dir = os.path.join(work_dir, \"models_2\")\n",
    "\n",
    "if not os.path.isdir(models_dir):\n",
    "    pathlib.Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "#model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M', pad_token_id = tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "#model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B', pad_token_id = tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2', pad_token_id = tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "named_parameters = list(model.named_parameters())\n",
    "\n",
    "# Freeze a part\n",
    "for name, param in named_parameters[:-20]:\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.config.attention_dropout = 0.01\n",
    "model.config.embed_dropout = 0.01\n",
    "print(f\"Loading empty, pre-trained model with {len(named_parameters)} parameters.\")\n",
    "\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model attached to {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMEavxJ32gOH"
   },
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:14.127938Z",
     "iopub.status.busy": "2021-11-15T03:02:14.089758Z",
     "iopub.status.idle": "2021-11-15T03:02:14.148788Z",
     "shell.execute_reply": "2021-11-15T03:02:14.148316Z"
    },
    "id": "OzWBTuEj2gOJ",
    "outputId": "0668db46-b7fd-4806-e90e-a898d2a6b77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_lines: \n",
      "train_lines: PlayerReply c \"Hey, Remy!\" DragonReply Ry \"Hello, [player_name].\"\n",
      "PlayerReply c \"Is there any particular reason why you wanted to meet here?\" DragonReply Ry \"I enjoy Tatsu Park is all. Have you been here before?\"\n",
      "PlayerReply c \"Can't say I have.\" PlayerReply c \"A few times.\" PlayerReply c \"Once or twice.\" DragonReply Ry \"I see.\" DragonReply Ry \"Well, what do you think of it?\"\n",
      "PlayerReply c \"It's pretty idyllic.\" DragonReply Ry smile \"It is. I like it a lot here.\"\n",
      "PlayerReply c \"It's pretty romantic.\" DragonReply Ry shy \"You think so?\"\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(work_dir, \"awsw_story_input.txt\")) as f:\n",
    "    data = f.read()\n",
    "lines = data.split(\"\\n\")\n",
    "player_dragon_pairs = {}\n",
    "last_player_talk = []\n",
    "closed_player_talk = False\n",
    "re_player_talk = re.compile(r'c \"(.*?)\"')\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line_split = line.split(\" \")\n",
    "    if len(line_split) <= 1:\n",
    "        continue\n",
    "    \n",
    "    if line_split[0] == \"c\":\n",
    "        if closed_player_talk:\n",
    "            closed_player_talk = False\n",
    "            last_player_talk = []\n",
    "        last_player_talk.append(re.sub(re_player_talk, r\"\\1\", line))\n",
    "    else:\n",
    "        if not closed_player_talk:\n",
    "            last_player_talk = json.dumps(last_player_talk)\n",
    "            if not last_player_talk in player_dragon_pairs:\n",
    "                player_dragon_pairs[last_player_talk] = []\n",
    "            closed_player_talk = True\n",
    "            \n",
    "        line = \"DragonReply \" + line\n",
    "        if last_player_talk is not None:\n",
    "            player_dragon_pairs[last_player_talk].append(line)\n",
    "    \n",
    "train_lines = []\n",
    "eval_lines = []\n",
    "eval_per_character = 0\n",
    "\n",
    "for player_line_str in player_dragon_pairs.keys():\n",
    "    player_lines = json.loads(player_line_str)\n",
    "    dragon_lines = player_dragon_pairs[player_line_str]\n",
    "    compiled_line = \" \".join([f'PlayerReply c \"{player_line}\"' for player_line in player_lines]) + \" \" + \" \".join(dragon_lines)\n",
    "    train_lines.append(compiled_line)\n",
    "    \n",
    "test_bucket = {}\n",
    "for l in train_lines:\n",
    "    l_split = l.split(\" \")\n",
    "    character = None\n",
    "    for i, ls in enumerate(l_split):\n",
    "        if ls == \"DragonReply\":\n",
    "            character = l_split[i + 1]\n",
    "            break\n",
    "    if not character in test_bucket:\n",
    "        test_bucket[character] = []\n",
    "    test_bucket[character].append(l)\n",
    "    \n",
    "for i in range(eval_per_character):\n",
    "    for character in test_bucket.keys():\n",
    "        random_line = test_bucket[character][randrange(len(test_bucket[character]))]\n",
    "        eval_lines.append(random_line)\n",
    "        for i2, t in enumerate(train_lines):\n",
    "            if t == random_line:\n",
    "                del train_lines[i2]\n",
    "                break\n",
    "    \n",
    "joined_eval_lines = \"\\n\".join(eval_lines[:5])\n",
    "print(f\"eval_lines: {joined_eval_lines}\")\n",
    "joined_train_lines = \"\\n\".join(train_lines[:5])\n",
    "print(f\"train_lines: {joined_train_lines}\")\n",
    "\n",
    "random.shuffle(train_lines)\n",
    "\n",
    "if not os.path.isfile(os.path.join(work_dir, \"data_train.txt\")):\n",
    "    with open(os.path.join(work_dir, \"data_train.txt\"), \"w\") as f:\n",
    "        for l in train_lines:\n",
    "            f.write(l + \"\\n\")\n",
    "            \n",
    "if not os.path.isfile(os.path.join(work_dir, \"data_test.txt\")):\n",
    "    with open(os.path.join(work_dir, \"data_test.txt\"), \"w\") as f:\n",
    "        for l in eval_lines:\n",
    "            f.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "a04e889a56e94ce9812a0e7d89d4d4f4",
      "3c2d0014295e47f9a07de8d4e36e7ba4",
      "76286c1e442541e0a6cb2d7cbbc9cc8f",
      "dcad41ec160e448b83d85c907a48facb",
      "27f2d980cb774902a82c0ad2545f549a",
      "131ce893dc784733b42f384b419145db",
      "607ee5c0b54042f3919d0588d66e31c1",
      "b2373342cb4942569088a5b20186795a",
      "11475ac58a594c6db1641401b1cc0d13",
      "f6625c167f7b4aa4bbecf2129e07c068",
      "ef9e25d7100e4e9ba064e938bd8157ea",
      "7bb369ca60f94acc99a4ee8e8f8fd261",
      "bc20a41ccda14aeeb72014b80a5ec53a",
      "c6ff5b9114424dbfafcbc3b3ab887af5",
      "1153397efee1426cac848341c0b88785",
      "37672ae801de4f42a9f6f49cc33fb88e",
      "8528f9fbbc504a44b9670a760256192d",
      "406b2f47896446a187725d4a1aa926f8",
      "cb33a87e718546f3a33b7deea817de56",
      "eeb54e278e184bc5aabf0283f1b276ff",
      "809a3780924641b8ace57e9141b0167f",
      "b805525a58804750ae56dad7e43ecb0e",
      "906b9daf9437432c81546f35256c7232",
      "0ce77d162a014a2fa17628ef8fe20846",
      "3fed2802cbab4a90a5b3d5a8c9bc8974",
      "a146d5c5588944368242c519984cbccc",
      "28d30b2eecf04ae6835cf4c35648dcfc",
      "65c48ca18ea14cf9bccbaa2495c2f120",
      "5067025a37bb42318ae93216b3205b17",
      "d18658b25f6f47778b984d0bf35be999",
      "6ade1603ab664b9f9d5ce44014bc5305",
      "fd91a671eeb7431d90d86b44beb276bd",
      "df0a544035814e1ca11d6191c10a5b62",
      "23a62f16d8de4f3ab9fa64ada07d1e35",
      "d50f5db4d1a1430caecace0510c1a24e",
      "d36395d1dcad4ead98f7f4756b8dbd37",
      "2cf737292a8343f5965c3eb0ace01875",
      "9ba5d1e5fab74947a328842b5105a28e",
      "073bc138772048729e04017123149e80",
      "ab0ce277776d4d218bf97fff5acc8e28",
      "b404b600842a4fc4b2b66c6b015235d6",
      "06dd02ec048945e5bf6b17d2b5558fb2",
      "437e050108fd46e1ba0f35674fa7314f",
      "a5725e8dc8104756bcba16b2c886a27f",
      "a31733df07ed4bb485d518b64634acfb",
      "1a2fe039b81a42c496ba363b2000bc41",
      "3f271f4469cf4796b92a78eba64c30b3",
      "fd200bcd96354e36b32ca82660eb0ef2",
      "33f9747c1f474c1790c7c293c853fad5",
      "e10acb8e2c8240409a19c61499576afd",
      "cc3c5e1ef5974710a06c1eab3d90cfb1",
      "6eda966317484df2a47fa0c4f2a0370c",
      "0cd6ff104b484203adda9e8414fd80fa",
      "c5a11599f37c4077a4ab0daec124a78c",
      "6cd8593a13f54c138c9adf5ec85f2d97"
     ]
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:14.164657Z",
     "iopub.status.busy": "2021-11-15T03:02:14.164170Z",
     "iopub.status.idle": "2021-11-15T03:02:31.407935Z",
     "shell.execute_reply": "2021-11-15T03:02:31.408486Z"
    },
    "id": "pWeL2qWd2gOK",
    "outputId": "efdd650f-d95e-47a9-ad2f-396593bb779e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa1f980f1f543f68bd1cd51d17e8105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('text', data_files={'train': os.path.join(work_dir, \"data_train.txt\"), 'test': os.path.join(work_dir, \"data_test.txt\")})\n",
    "\n",
    "class AWSWDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, dataset, dataset_type, do_shuffle=False):\n",
    "        self.current_dataset = dataset\n",
    "        self.dataset_type = dataset_type\n",
    "        self.do_shuffle = do_shuffle\n",
    "        self.shuffled_datasets = []\n",
    "        self.current_idx = 0\n",
    "        for i in range(1):\n",
    "            self.current_dataset = self.current_dataset.shuffle()\n",
    "            mapped_dataset = self.current_dataset.map(\n",
    "                group_texts,\n",
    "                batched=True,\n",
    "                batch_size=dataset_batch_size,\n",
    "                num_proc=dataset_map_cores\n",
    "            )\n",
    "            self.shuffled_datasets.append(mapped_dataset)\n",
    "        \n",
    "    def approx_len(self):\n",
    "        return len(self.shuffled_datasets[0][self.dataset_type])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.current_idx = (self.current_idx + 1) % len(self.shuffled_datasets)\n",
    "        return iter(self.shuffled_datasets[self.current_idx][self.dataset_type])\n",
    "    \n",
    "def encode(batch):\n",
    "    result = []\n",
    "    attention_mask = []\n",
    "    for item in batch['text']:\n",
    "        #tokens = [tokenizer.bos_token_id] + tokenizer.encode(item) + [tokenizer.eos_token_id]\n",
    "        #tokens = tokenizer.encode(item)\n",
    "        tokens = tokenizer.encode(item) + [tokenizer.eos_token_id]\n",
    "        result.append(tokens)\n",
    "        attention_mask.append([1] * len(tokens))\n",
    "    return {\n",
    "        'attention_mask': attention_mask,\n",
    "        'input_ids': result\n",
    "    }\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    #random_shift = random.randint(0, 64)\n",
    "    #concatenated_examples['input_ids'] = concatenated_examples['input_ids'][random_shift:]\n",
    "    #concatenated_examples['attention_mask'] = concatenated_examples['attention_mask'][random_shift:]\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Pad the end\n",
    "    to_add = (math.ceil(total_length / block_size) * block_size) - total_length\n",
    "    if to_add > 0:\n",
    "        concatenated_examples['input_ids'] += [tokenizer.pad_token_id] * to_add\n",
    "        concatenated_examples['attention_mask'] += [0] * to_add\n",
    "        total_length += to_add\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def map_dragon_reply_text(batch):\n",
    "    result = {'text': []}\n",
    "    for item in batch['text']:\n",
    "        item_split = item.split(\" \")\n",
    "        player_replies = []\n",
    "        dragon_replies = []\n",
    "        current_reply = []\n",
    "        handling_reply = None\n",
    "        for token in item_split:\n",
    "            if token == \"PlayerReply\":\n",
    "                if handling_reply is None:\n",
    "                    handling_reply = \"PlayerReply\"\n",
    "                else:\n",
    "                    if handling_reply == \"PlayerReply\":\n",
    "                        # We need to store the PlayerReply\n",
    "                        player_replies.append(\" \".join(current_reply))\n",
    "                        current_reply = []\n",
    "            elif token == \"DragonReply\":\n",
    "                if handling_reply == \"DragonReply\":\n",
    "                    # We need to store the DragonReply\n",
    "                    dragon_replies.append(\" \".join(current_reply))\n",
    "                    current_reply = []\n",
    "                    \n",
    "                if handling_reply == \"PlayerReply\":\n",
    "                    # We need to store the PlayerReply\n",
    "                    player_replies.append(\" \".join(current_reply))\n",
    "                    current_reply = []\n",
    "                    \n",
    "                handling_reply = \"DragonReply\"\n",
    "                current_reply = []\n",
    "                    \n",
    "            if handling_reply is not None:\n",
    "                current_reply.append(token)\n",
    "                \n",
    "        # There's always a dragon reply at the end.\n",
    "        dragon_replies.append(\" \".join(current_reply))\n",
    "        for player_idx in range(len(player_replies)):\n",
    "            for dragon_idx in range(len(dragon_replies)):\n",
    "                result['text'].append(player_replies[player_idx] + \" \" + dragon_replies[dragon_idx])\n",
    "                \n",
    "    return result\n",
    "\n",
    "dataset_map_cores = min(multiprocessing.cpu_count(), 10)\n",
    "dataset_batch_size = 1000\n",
    "\n",
    "dataset = dataset.map(\n",
    "    map_dragon_reply_text,\n",
    "    batched=True,\n",
    "    batch_size=dataset_batch_size,\n",
    "    num_proc=dataset_map_cores\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    encode,\n",
    "    batched=True,\n",
    "    batch_size=dataset_batch_size,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc=dataset_map_cores\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=dataset_batch_size,\n",
    "    num_proc=dataset_map_cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:31.414438Z",
     "iopub.status.busy": "2021-11-15T03:02:31.413798Z",
     "iopub.status.idle": "2021-11-15T03:02:31.416701Z",
     "shell.execute_reply": "2021-11-15T03:02:31.416309Z"
    },
    "id": "PhiZIfn02gOL",
    "outputId": "47e5768d-8b9d-4ea8-c5ac-cc392abba402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len: 8658 num_training_steps: 136 num_total_steps: 13600\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_len = len(dataset['train'])\n",
    "num_training_steps = math.ceil(train_len / batch_size)\n",
    "num_epoch = 100\n",
    "num_total_steps = num_training_steps * num_epoch\n",
    "num_warmup_steps = num_training_steps * 2\n",
    "print(f\"train_len: {train_len} num_training_steps: {num_training_steps} num_total_steps: {num_total_steps}\")\n",
    "def get_optimizer_and_scheduler(params):\n",
    "    optimizer = AdamW(params, lr=0.001)\n",
    "    #scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_total_steps)\n",
    "    #scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps, num_total_steps, power=0.5, lr_end=1e-10)\n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps, num_total_steps, 4)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:31.421528Z",
     "iopub.status.busy": "2021-11-15T03:02:31.420729Z",
     "iopub.status.idle": "2021-11-15T03:02:31.756717Z",
     "shell.execute_reply": "2021-11-15T03:02:31.756317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = []\n",
    "optimizer, scheduler = get_optimizer_and_scheduler([torch.tensor(0.1)])\n",
    "for i in range(num_total_steps):\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "plt.plot(lrs)\n",
    "plt.show()\n",
    "del lrs\n",
    "del optimizer\n",
    "del scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:31.828317Z",
     "iopub.status.busy": "2021-11-15T03:02:31.827715Z",
     "iopub.status.idle": "2021-11-15T03:02:31.901562Z",
     "shell.execute_reply": "2021-11-15T03:02:31.901089Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<'EOT' > ds_config_zero3.json\n",
    "{\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"overlap_comm\": true,\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"sub_group_size\": 1e9,\n",
    "        \"reduce_bucket_size\": \"auto\",\n",
    "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "        \"stage3_param_persistence_threshold\": \"auto\",\n",
    "        \"stage3_max_live_parameters\": 1e9,\n",
    "        \"stage3_max_reuse_distance\": 1e9,\n",
    "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 2000,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": false\n",
    "}\n",
    "EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T03:02:31.910396Z",
     "iopub.status.busy": "2021-11-15T03:02:31.909838Z",
     "iopub.status.idle": "2021-11-15T03:51:40.788275Z",
     "shell.execute_reply": "2021-11-15T03:51:40.787795Z"
    },
    "id": "AdPIW0xSTpRY",
    "outputId": "01338ca7-7ed2-4d8e-dd7b-5c235e4c5e30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8658\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13600' max='13600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13600/13600 49:08, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.451100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.267200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.099700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>0.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.136400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>0.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.088600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-1000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-1000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-1500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-1500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-2000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-2000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-2500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-2500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-3000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-3000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-3500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-3500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-4000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-4000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-4500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-4500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-5000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-5000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-5500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-5500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-6000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-6000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-6500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-6500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-7000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-7000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-7500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-7500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-8000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-8000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-8500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-8500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-9000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-9000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-9500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-9500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-10000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-10000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-10500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-10500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-11000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-11000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-11500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-11500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-12000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-12000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-12500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-12500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-13000\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-13000/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_2/checkpoint-13500\n",
      "Configuration saved in /opt/awsw/models_2/checkpoint-13500/config.json\n",
      "Model weights saved in /opt/awsw/models_2/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_2/checkpoint-12500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AWSWTrainer(Trainer):\n",
    "    def _get_train_sampler(self):\n",
    "        return None\n",
    "    \n",
    "class AWSWTrainerCallback(TrainerCallback):\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        learning_rate_history = [h['learning_rate'] for h in state.log_history if 'loss' in h]\n",
    "        loss_history = [h['loss'] for h in state.log_history if 'loss' in h]\n",
    "        fig, axs = plt.subplots(2)\n",
    "        fig.suptitle('Learning rate and loss')\n",
    "        axs[0].plot(learning_rate_history)\n",
    "        axs[1].plot(loss_history)\n",
    "        \n",
    "def train(model):\n",
    "    optimizer, scheduler = get_optimizer_and_scheduler(model.parameters())\n",
    "    training_args = TrainingArguments(\n",
    "        models_dir,\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epoch,\n",
    "        save_total_limit=2,\n",
    "        save_steps=500,\n",
    "        logging_steps=250,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        #deepspeed=\"ds_config_zero3.json\"\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=dataset['train'],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        callbacks=[AWSWTrainerCallback]\n",
    "    )\n",
    "    checkpoint_dirs = [os.path.join(models_dir, d) for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]\n",
    "    if len(checkpoint_dirs) > 0:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "        trainer.train(latest_checkpoint)\n",
    "    else:\n",
    "        trainer.train()\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:51:40.796210Z",
     "iopub.status.busy": "2021-11-15T03:51:40.795809Z",
     "iopub.status.idle": "2021-11-15T03:51:41.148358Z",
     "shell.execute_reply": "2021-11-15T03:51:41.147771Z"
    },
    "id": "5UePGmLD2gON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_dragon_reply(past, prompt, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    prompt = f'{past} PlayerReply c \"{prompt}\" DragonReply'\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        generated, \n",
    "        do_sample=(top_k is not None and top_p is not None),\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=block_size,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return tokenizer.decode(sample_outputs[0], skip_special_tokens=False)[len(prompt):].strip()\n",
    "\n",
    "prompts = [\n",
    "    ('PlayerReply c \"Hey Remy!\" DragonReply Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('PlayerReply c \"I was with Lorem today.\" DragonReply Ad \"That\\'s awesome. He\\'s a cute fellow.\"', \"What do you think of Lorem?\"),\n",
    "    ('DragonReply m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('DragonReply m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "# Set a fixed seed to make sure we get the same response every time.\n",
    "torch.manual_seed(80085)\n",
    "for (past, prompt) in prompts:\n",
    "    reply = generate_dragon_reply(past, prompt)\n",
    "    print(f\"Prompt: {prompt}\\nReply: {reply}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OZarUHg2gON"
   },
   "source": [
    "# Sampling test\n",
    "\n",
    "Which combination is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:51:41.154527Z",
     "iopub.status.busy": "2021-11-15T03:51:41.154090Z",
     "iopub.status.idle": "2021-11-15T03:52:20.765564Z",
     "shell.execute_reply": "2021-11-15T03:52:20.765160Z"
    },
    "id": "bWoLzL9B2gON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test 1 top_k: 21, top_p: 0.44] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 1 top_k: 21, top_p: 0.44] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 1 top_k: 21, top_p: 0.44] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 1 top_k: 21, top_p: 0.44] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 2 top_k: 3, top_p: 0.7] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 2 top_k: 3, top_p: 0.7] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 2 top_k: 3, top_p: 0.7] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 2 top_k: 3, top_p: 0.7] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 3 top_k: 37, top_p: 0.65] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 3 top_k: 37, top_p: 0.65] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 3 top_k: 37, top_p: 0.65] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 3 top_k: 37, top_p: 0.65] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 4 top_k: 54, top_p: 0.82] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 4 top_k: 54, top_p: 0.82] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 4 top_k: 54, top_p: 0.82] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 4 top_k: 54, top_p: 0.82] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 5 top_k: 9, top_p: 0.74] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 5 top_k: 9, top_p: 0.74] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think he was an acquired taste, so I think he'll be very happy to hear it.\"<|endoftext|>\n",
      "\n",
      "[Test 5 top_k: 9, top_p: 0.74] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 5 top_k: 9, top_p: 0.74] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 6 top_k: 82, top_p: 0.7] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 6 top_k: 82, top_p: 0.7] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 6 top_k: 82, top_p: 0.7] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 6 top_k: 82, top_p: 0.7] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 7 top_k: 93, top_p: 0.88] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 7 top_k: 93, top_p: 0.88] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 7 top_k: 93, top_p: 0.88] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 7 top_k: 93, top_p: 0.88] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 8 top_k: 48, top_p: 0.05] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 8 top_k: 48, top_p: 0.05] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 8 top_k: 48, top_p: 0.05] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 8 top_k: 48, top_p: 0.05] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 9 top_k: 31, top_p: 0.24] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 9 top_k: 31, top_p: 0.24] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 9 top_k: 31, top_p: 0.24] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 9 top_k: 31, top_p: 0.24] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 10 top_k: 38, top_p: 0.96] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 10 top_k: 38, top_p: 0.96] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 10 top_k: 38, top_p: 0.96] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 10 top_k: 38, top_p: 0.96] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could figure something out for you with your PDAs, but I think we should do something for you, at least.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 11 top_k: 14, top_p: 0.74] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 11 top_k: 14, top_p: 0.74] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think he was an acquired taste, so I think he'll be very happy to hear it.\"<|endoftext|>\n",
      "\n",
      "[Test 11 top_k: 14, top_p: 0.74] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 11 top_k: 14, top_p: 0.74] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 12 top_k: 5, top_p: 0.14] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 12 top_k: 5, top_p: 0.14] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 12 top_k: 5, top_p: 0.14] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 12 top_k: 5, top_p: 0.14] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 13 top_k: 98, top_p: 0.94] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 13 top_k: 98, top_p: 0.94] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 13 top_k: 98, top_p: 0.94] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 13 top_k: 98, top_p: 0.94] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 14 top_k: 45, top_p: 0.89] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 14 top_k: 45, top_p: 0.89] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 14 top_k: 45, top_p: 0.89] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 14 top_k: 45, top_p: 0.89] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 15 top_k: 61, top_p: 0.9] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 15 top_k: 61, top_p: 0.9] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 15 top_k: 61, top_p: 0.9] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 15 top_k: 61, top_p: 0.9] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 16 top_k: 33, top_p: 0.86] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 16 top_k: 33, top_p: 0.86] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 16 top_k: 33, top_p: 0.86] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 16 top_k: 33, top_p: 0.86] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 17 top_k: 5, top_p: 0.66] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 17 top_k: 5, top_p: 0.66] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 17 top_k: 5, top_p: 0.66] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 17 top_k: 5, top_p: 0.66] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 18 top_k: 86, top_p: 0.21] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 18 top_k: 86, top_p: 0.21] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 18 top_k: 86, top_p: 0.21] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 18 top_k: 86, top_p: 0.21] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 19 top_k: 72, top_p: 0.96] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 19 top_k: 72, top_p: 0.96] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 19 top_k: 72, top_p: 0.96] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 19 top_k: 72, top_p: 0.96] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could figure something out for you with your PDAs, but I think we should do something for you, at least.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 20 top_k: 29, top_p: 0.56] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 20 top_k: 29, top_p: 0.56] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 20 top_k: 29, top_p: 0.56] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 20 top_k: 29, top_p: 0.56] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 21 top_k: 13, top_p: 0.38] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 21 top_k: 13, top_p: 0.38] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 21 top_k: 13, top_p: 0.38] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 21 top_k: 13, top_p: 0.38] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 22 top_k: 71, top_p: 0.34] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 22 top_k: 71, top_p: 0.34] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 22 top_k: 71, top_p: 0.34] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 22 top_k: 71, top_p: 0.34] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 23 top_k: 32, top_p: 0.3] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 23 top_k: 32, top_p: 0.3] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 23 top_k: 32, top_p: 0.3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 23 top_k: 32, top_p: 0.3] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 24 top_k: 30, top_p: 0.28] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 24 top_k: 30, top_p: 0.28] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 24 top_k: 30, top_p: 0.28] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 24 top_k: 30, top_p: 0.28] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 25 top_k: 88, top_p: 0.94] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 25 top_k: 88, top_p: 0.94] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 25 top_k: 88, top_p: 0.94] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 25 top_k: 88, top_p: 0.94] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 26 top_k: 34, top_p: 0.93] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 26 top_k: 34, top_p: 0.93] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 26 top_k: 34, top_p: 0.93] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 26 top_k: 34, top_p: 0.93] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 27 top_k: 82, top_p: 0.04] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 27 top_k: 82, top_p: 0.04] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 27 top_k: 82, top_p: 0.04] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 27 top_k: 82, top_p: 0.04] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 28 top_k: 94, top_p: 0.63] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 28 top_k: 94, top_p: 0.63] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 28 top_k: 94, top_p: 0.63] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 28 top_k: 94, top_p: 0.63] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 29 top_k: 64, top_p: 0.64] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 29 top_k: 64, top_p: 0.64] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 29 top_k: 64, top_p: 0.64] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 29 top_k: 64, top_p: 0.64] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 30 top_k: 15, top_p: 0.07] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 30 top_k: 15, top_p: 0.07] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 30 top_k: 15, top_p: 0.07] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 30 top_k: 15, top_p: 0.07] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 31 top_k: 48, top_p: 0.64] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 31 top_k: 48, top_p: 0.64] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 31 top_k: 48, top_p: 0.64] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 31 top_k: 48, top_p: 0.64] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 32 top_k: 72, top_p: 0.54] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 32 top_k: 72, top_p: 0.54] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 32 top_k: 72, top_p: 0.54] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 32 top_k: 72, top_p: 0.54] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 33 top_k: 83, top_p: 0.64] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 33 top_k: 83, top_p: 0.64] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 33 top_k: 83, top_p: 0.64] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 33 top_k: 83, top_p: 0.64] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 34 top_k: 22, top_p: 0.38] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 34 top_k: 22, top_p: 0.38] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 34 top_k: 22, top_p: 0.38] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 34 top_k: 22, top_p: 0.38] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 35 top_k: 59, top_p: 0.41] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 35 top_k: 59, top_p: 0.41] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 35 top_k: 59, top_p: 0.41] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 35 top_k: 59, top_p: 0.41] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 36 top_k: 65, top_p: 0.77] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 36 top_k: 65, top_p: 0.77] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 36 top_k: 65, top_p: 0.77] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 36 top_k: 65, top_p: 0.77] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 37 top_k: 29, top_p: 0.41] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 37 top_k: 29, top_p: 0.41] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 37 top_k: 29, top_p: 0.41] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 37 top_k: 29, top_p: 0.41] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 38 top_k: 87, top_p: 0.21] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 38 top_k: 87, top_p: 0.21] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 38 top_k: 87, top_p: 0.21] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 38 top_k: 87, top_p: 0.21] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 39 top_k: 51, top_p: 0.54] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 39 top_k: 51, top_p: 0.54] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 39 top_k: 51, top_p: 0.54] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 39 top_k: 51, top_p: 0.54] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 40 top_k: 47, top_p: 0.25] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 40 top_k: 47, top_p: 0.25] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 40 top_k: 47, top_p: 0.25] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 40 top_k: 47, top_p: 0.25] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 41 top_k: 68, top_p: 0.62] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 41 top_k: 68, top_p: 0.62] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 41 top_k: 68, top_p: 0.62] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 41 top_k: 68, top_p: 0.62] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 42 top_k: 78, top_p: 0.95] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 42 top_k: 78, top_p: 0.95] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 42 top_k: 78, top_p: 0.95] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 42 top_k: 78, top_p: 0.95] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 43 top_k: 19, top_p: 0.69] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 43 top_k: 19, top_p: 0.69] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 43 top_k: 19, top_p: 0.69] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 43 top_k: 19, top_p: 0.69] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 44 top_k: 50, top_p: 0.24] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 44 top_k: 50, top_p: 0.24] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 44 top_k: 50, top_p: 0.24] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 44 top_k: 50, top_p: 0.24] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 45 top_k: 88, top_p: 0.58] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 45 top_k: 88, top_p: 0.58] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 45 top_k: 88, top_p: 0.58] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 45 top_k: 88, top_p: 0.58] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 46 top_k: 42, top_p: 0.77] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 46 top_k: 42, top_p: 0.77] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 46 top_k: 42, top_p: 0.77] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 46 top_k: 42, top_p: 0.77] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 47 top_k: 68, top_p: 0.02] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 47 top_k: 68, top_p: 0.02] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 47 top_k: 68, top_p: 0.02] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 47 top_k: 68, top_p: 0.02] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 48 top_k: 93, top_p: 0.59] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 48 top_k: 93, top_p: 0.59] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 48 top_k: 93, top_p: 0.59] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 48 top_k: 93, top_p: 0.59] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 49 top_k: 67, top_p: 0.45] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 49 top_k: 67, top_p: 0.45] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 49 top_k: 67, top_p: 0.45] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 49 top_k: 67, top_p: 0.45] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 50 top_k: 45, top_p: 0.17] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 50 top_k: 45, top_p: 0.17] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 50 top_k: 45, top_p: 0.17] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 50 top_k: 45, top_p: 0.17] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 51 top_k: 52, top_p: 0.85] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 51 top_k: 52, top_p: 0.85] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 51 top_k: 52, top_p: 0.85] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 51 top_k: 52, top_p: 0.85] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 52 top_k: 100, top_p: 0.28] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 52 top_k: 100, top_p: 0.28] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 52 top_k: 100, top_p: 0.28] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 52 top_k: 100, top_p: 0.28] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 53 top_k: 36, top_p: 0.44] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 53 top_k: 36, top_p: 0.44] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 53 top_k: 36, top_p: 0.44] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 53 top_k: 36, top_p: 0.44] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 54 top_k: 99, top_p: 0.12] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 54 top_k: 99, top_p: 0.12] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 54 top_k: 99, top_p: 0.12] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 54 top_k: 99, top_p: 0.12] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 55 top_k: 85, top_p: 0.69] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 55 top_k: 85, top_p: 0.69] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 55 top_k: 85, top_p: 0.69] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 55 top_k: 85, top_p: 0.69] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 56 top_k: 68, top_p: 0.66] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 56 top_k: 68, top_p: 0.66] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 56 top_k: 68, top_p: 0.66] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 56 top_k: 68, top_p: 0.66] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 57 top_k: 19, top_p: 0.52] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 57 top_k: 19, top_p: 0.52] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 57 top_k: 19, top_p: 0.52] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 57 top_k: 19, top_p: 0.52] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 58 top_k: 7, top_p: 0.19] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 58 top_k: 7, top_p: 0.19] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 58 top_k: 7, top_p: 0.19] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 58 top_k: 7, top_p: 0.19] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 59 top_k: 45, top_p: 0.15] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 59 top_k: 45, top_p: 0.15] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 59 top_k: 45, top_p: 0.15] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 59 top_k: 45, top_p: 0.15] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 60 top_k: 84, top_p: 0.93] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 60 top_k: 84, top_p: 0.93] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 60 top_k: 84, top_p: 0.93] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 60 top_k: 84, top_p: 0.93] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 61 top_k: 20, top_p: 0.4] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 61 top_k: 20, top_p: 0.4] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 61 top_k: 20, top_p: 0.4] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 61 top_k: 20, top_p: 0.4] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 62 top_k: 44, top_p: 0.57] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 62 top_k: 44, top_p: 0.57] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 62 top_k: 44, top_p: 0.57] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 62 top_k: 44, top_p: 0.57] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 63 top_k: 64, top_p: 0.86] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 63 top_k: 64, top_p: 0.86] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 63 top_k: 64, top_p: 0.86] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 63 top_k: 64, top_p: 0.86] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 64 top_k: 74, top_p: 0.88] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 64 top_k: 74, top_p: 0.88] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 64 top_k: 74, top_p: 0.88] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 64 top_k: 74, top_p: 0.88] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 65 top_k: 7, top_p: 0.38] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 65 top_k: 7, top_p: 0.38] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 65 top_k: 7, top_p: 0.38] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 65 top_k: 7, top_p: 0.38] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 66 top_k: 13, top_p: 0.48] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 66 top_k: 13, top_p: 0.48] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 66 top_k: 13, top_p: 0.48] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 66 top_k: 13, top_p: 0.48] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 67 top_k: 84, top_p: 0.93] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 67 top_k: 84, top_p: 0.93] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 67 top_k: 84, top_p: 0.93] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 67 top_k: 84, top_p: 0.93] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 68 top_k: 54, top_p: 0.57] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 68 top_k: 54, top_p: 0.57] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 68 top_k: 54, top_p: 0.57] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 68 top_k: 54, top_p: 0.57] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 69 top_k: 54, top_p: 0.63] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 69 top_k: 54, top_p: 0.63] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 69 top_k: 54, top_p: 0.63] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 69 top_k: 54, top_p: 0.63] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 70 top_k: 93, top_p: 0.67] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 70 top_k: 93, top_p: 0.67] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 70 top_k: 93, top_p: 0.67] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 70 top_k: 93, top_p: 0.67] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 71 top_k: 73, top_p: 0.52] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 71 top_k: 73, top_p: 0.52] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 71 top_k: 73, top_p: 0.52] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 71 top_k: 73, top_p: 0.52] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 72 top_k: 98, top_p: 0.8] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 72 top_k: 98, top_p: 0.8] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 72 top_k: 98, top_p: 0.8] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 72 top_k: 98, top_p: 0.8] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 73 top_k: 12, top_p: 0.24] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 73 top_k: 12, top_p: 0.24] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 73 top_k: 12, top_p: 0.24] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 73 top_k: 12, top_p: 0.24] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 74 top_k: 41, top_p: 0.47] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 74 top_k: 41, top_p: 0.47] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 74 top_k: 41, top_p: 0.47] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 74 top_k: 41, top_p: 0.47] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 75 top_k: 43, top_p: 0.74] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 75 top_k: 43, top_p: 0.74] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think he was an acquired taste, so I think he'll be very happy to hear it.\"<|endoftext|>\n",
      "\n",
      "[Test 75 top_k: 43, top_p: 0.74] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 75 top_k: 43, top_p: 0.74] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 76 top_k: 66, top_p: 0.55] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 76 top_k: 66, top_p: 0.55] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 76 top_k: 66, top_p: 0.55] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 76 top_k: 66, top_p: 0.55] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 77 top_k: 28, top_p: 0.7] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 77 top_k: 28, top_p: 0.7] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 77 top_k: 28, top_p: 0.7] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 77 top_k: 28, top_p: 0.7] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 78 top_k: 0, top_p: 0.56] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 78 top_k: 0, top_p: 0.56] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 78 top_k: 0, top_p: 0.56] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 78 top_k: 0, top_p: 0.56] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 79 top_k: 93, top_p: 0.2] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 79 top_k: 93, top_p: 0.2] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 79 top_k: 93, top_p: 0.2] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 79 top_k: 93, top_p: 0.2] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 80 top_k: 74, top_p: 0.88] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 80 top_k: 74, top_p: 0.88] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 80 top_k: 74, top_p: 0.88] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 80 top_k: 74, top_p: 0.88] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 81 top_k: 10, top_p: 0.42] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 81 top_k: 10, top_p: 0.42] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 81 top_k: 10, top_p: 0.42] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 81 top_k: 10, top_p: 0.42] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 82 top_k: 5, top_p: 0.3] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 82 top_k: 5, top_p: 0.3] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 82 top_k: 5, top_p: 0.3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 82 top_k: 5, top_p: 0.3] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 83 top_k: 66, top_p: 0.7] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 83 top_k: 66, top_p: 0.7] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 83 top_k: 66, top_p: 0.7] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 83 top_k: 66, top_p: 0.7] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 84 top_k: 34, top_p: 0.11] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 84 top_k: 34, top_p: 0.11] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 84 top_k: 34, top_p: 0.11] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 84 top_k: 34, top_p: 0.11] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 85 top_k: 64, top_p: 0.08] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 85 top_k: 64, top_p: 0.08] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 85 top_k: 64, top_p: 0.08] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 85 top_k: 64, top_p: 0.08] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 86 top_k: 53, top_p: 0.46] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 86 top_k: 53, top_p: 0.46] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 86 top_k: 53, top_p: 0.46] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 86 top_k: 53, top_p: 0.46] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 87 top_k: 65, top_p: 0.46] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 87 top_k: 65, top_p: 0.46] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 87 top_k: 65, top_p: 0.46] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 87 top_k: 65, top_p: 0.46] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 88 top_k: 57, top_p: 0.05] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 88 top_k: 57, top_p: 0.05] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 88 top_k: 57, top_p: 0.05] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 88 top_k: 57, top_p: 0.05] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 89 top_k: 78, top_p: 0.34] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 89 top_k: 78, top_p: 0.34] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 89 top_k: 78, top_p: 0.34] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 89 top_k: 78, top_p: 0.34] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 90 top_k: 14, top_p: 0.09] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 90 top_k: 14, top_p: 0.09] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 90 top_k: 14, top_p: 0.09] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 90 top_k: 14, top_p: 0.09] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 91 top_k: 65, top_p: 0.57] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 91 top_k: 65, top_p: 0.57] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 91 top_k: 65, top_p: 0.57] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 91 top_k: 65, top_p: 0.57] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 92 top_k: 72, top_p: 0.57] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 92 top_k: 72, top_p: 0.57] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 92 top_k: 72, top_p: 0.57] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 92 top_k: 72, top_p: 0.57] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 93 top_k: 56, top_p: 0.74] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 93 top_k: 56, top_p: 0.74] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think he was an acquired taste, so I think he'll be very happy to hear it.\"<|endoftext|>\n",
      "\n",
      "[Test 93 top_k: 56, top_p: 0.74] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 93 top_k: 56, top_p: 0.74] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 94 top_k: 72, top_p: 0.71] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 94 top_k: 72, top_p: 0.71] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 94 top_k: 72, top_p: 0.71] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let's see...\"<|endoftext|>\n",
      "\n",
      "[Test 94 top_k: 72, top_p: 0.71] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 95 top_k: 27, top_p: 0.37] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 95 top_k: 27, top_p: 0.37] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 95 top_k: 27, top_p: 0.37] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 95 top_k: 27, top_p: 0.37] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 96 top_k: 58, top_p: 0.47] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 96 top_k: 58, top_p: 0.47] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 96 top_k: 58, top_p: 0.47] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 96 top_k: 58, top_p: 0.47] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 97 top_k: 68, top_p: 0.49] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 97 top_k: 68, top_p: 0.49] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 97 top_k: 68, top_p: 0.49] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 97 top_k: 68, top_p: 0.49] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 98 top_k: 98, top_p: 0.29] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 98 top_k: 98, top_p: 0.29] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 98 top_k: 98, top_p: 0.29] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 98 top_k: 98, top_p: 0.29] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 99 top_k: 95, top_p: 0.01] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 99 top_k: 95, top_p: 0.01] -> Prompt: What do you think of Lorem?\n",
      "Reply: Ry \"I think I've got some very bad news.\"<|endoftext|>\n",
      "\n",
      "[Test 99 top_k: 95, top_p: 0.01] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look around your apartment, then.\"<|endoftext|>\n",
      "\n",
      "[Test 99 top_k: 95, top_p: 0.01] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n",
      "[Test 100 top_k: 31, top_p: 0.84] -> Prompt: How are you?\n",
      "Reply: Ry \"Hey, [player_name].\"<|endoftext|>\n",
      "\n",
      "[Test 100 top_k: 31, top_p: 0.84] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo normal \"I think we should go inside.\"<|endoftext|>\n",
      "\n",
      "[Test 100 top_k: 31, top_p: 0.84] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ad \"Let me take a look!\"<|endoftext|>\n",
      "\n",
      "[Test 100 top_k: 31, top_p: 0.84] -> Prompt: What will we do here?\n",
      "Reply: Ad think b \"I thought we could really pull it off. And once I saw that they could survive without my guidance - and also govern themselves - I knew my plan was a success\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    torch.manual_seed(80085)\n",
    "    top_k = random.randint(0, 100)\n",
    "    top_p = round(random.uniform(0, 1), 2)\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = generate_dragon_reply(past, prompt, top_k = top_k, top_p = top_p)\n",
    "        print(f\"[Test {i + 1} top_k: {top_k}, top_p: {top_p}] -> Prompt: {prompt}\\nReply: {reply}\\n\")\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:52:20.773275Z",
     "iopub.status.busy": "2021-11-15T03:52:20.772754Z",
     "iopub.status.idle": "2021-11-15T03:52:20.880121Z",
     "shell.execute_reply": "2021-11-15T03:52:20.880483Z"
    },
    "id": "FgM9Awn7acpG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What to say?\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6084/61523620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What to say?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \"\"\"\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             raise StdinNotImplementedError(\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "def generate_reply(prompt):\n",
    "    model.eval()\n",
    "    prompt = f'PlayerReply c \"{prompt}\" DragonReply'\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "    print(prompt, generated)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        generated, \n",
    "        do_sample=True,   \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        top_length = block_size,\n",
    "        top_p=0.95, \n",
    "        num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))\n",
    "\n",
    "print(\"What to say?\")\n",
    "print(generate_reply(input()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXKM4uLM2gOO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AWSW_GPT-Neo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "28ca9457071a49eb87a714fd7ebb997d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "29fb7d329264429f8dbc0172bb7a780f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2edfa1d0e61d4216be35cfd30ffdcd68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3064abfa636d41d7866dba345c3d7400": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2edfa1d0e61d4216be35cfd30ffdcd68",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e182f876bc554a45887f88d2a46d8398",
       "value": 2.0
      }
     },
     "862bae56b0ef4ec490f4d51b8cf6fb4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "970422d73f2945f09187a407091513cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aff88ec512cb4ccfa986f761b8e88a19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_29fb7d329264429f8dbc0172bb7a780f",
       "placeholder": "",
       "style": "IPY_MODEL_862bae56b0ef4ec490f4d51b8cf6fb4a",
       "value": "100%"
      }
     },
     "d5cba065c1d64116acffc6a913ef3a42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dfa1f980f1f543f68bd1cd51d17e8105": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_aff88ec512cb4ccfa986f761b8e88a19",
        "IPY_MODEL_3064abfa636d41d7866dba345c3d7400",
        "IPY_MODEL_e7f7c85907304b08af44543311a3da92"
       ],
       "layout": "IPY_MODEL_d5cba065c1d64116acffc6a913ef3a42"
      }
     },
     "e182f876bc554a45887f88d2a46d8398": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e7f7c85907304b08af44543311a3da92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_28ca9457071a49eb87a714fd7ebb997d",
       "placeholder": "",
       "style": "IPY_MODEL_970422d73f2945f09187a407091513cd",
       "value": " 2/2 [00:00&lt;00:00, 103.80it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

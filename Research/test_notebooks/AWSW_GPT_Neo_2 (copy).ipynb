{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:17.476762Z",
     "iopub.status.busy": "2021-11-15T01:52:17.476259Z",
     "iopub.status.idle": "2021-11-15T01:52:17.678496Z",
     "shell.execute_reply": "2021-11-15T01:52:17.677807Z"
    },
    "id": "2TJ-BqFtQ86M",
    "outputId": "f41c5626-6827-4d90-f0a9-8a11c01a366d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 15 01:52:17 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   55C    P0    37W /  N/A |   1088MiB / 16125MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:17.685617Z",
     "iopub.status.busy": "2021-11-15T01:52:17.683061Z",
     "iopub.status.idle": "2021-11-15T01:52:19.500425Z",
     "shell.execute_reply": "2021-11-15T01:52:19.499857Z"
    },
    "id": "oR9S63qiQt2b",
    "outputId": "b1303393-4c18-4510-da76-59e5f2590db8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/awsw-dev/.local/lib/python3.8/site-packages (4.12.3)\r\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (1.15.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.4.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.2)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/awsw-dev/.local/lib/python3.8/site-packages (from transformers) (0.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.8.28)\r\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.45)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.26.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.0)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.4)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (2.0.2)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.12.2)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.2)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2021.8.1)\r\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (5.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.1)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.0.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2021.5.30)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (21.2.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.7.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (5.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2021.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:19.507652Z",
     "iopub.status.busy": "2021-11-15T01:52:19.506806Z",
     "iopub.status.idle": "2021-11-15T01:52:20.918688Z",
     "shell.execute_reply": "2021-11-15T01:52:20.918266Z"
    },
    "id": "GhhigZYMRK6N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from random import randrange\n",
    "import multiprocessing\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, GPTNeoForCausalLM\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:20.923610Z",
     "iopub.status.busy": "2021-11-15T01:52:20.923078Z",
     "iopub.status.idle": "2021-11-15T01:52:20.924643Z",
     "shell.execute_reply": "2021-11-15T01:52:20.924923Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:20.947751Z",
     "iopub.status.busy": "2021-11-15T01:52:20.947182Z",
     "iopub.status.idle": "2021-11-15T01:52:20.949367Z",
     "shell.execute_reply": "2021-11-15T01:52:20.949633Z"
    },
    "id": "MTduRlf-RQJa",
    "outputId": "296dba31-0af6-422c-eea8-f5c1130a5daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use cuda:0 for training with seed: 3787302372\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 2 ** 32 - 1)\n",
    "random.seed(seed)\n",
    "block_size = 64\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"Will use {device_name} for training with seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278,
     "referenced_widgets": [
      "641f6e8a671d4dfe9da34c7685520767",
      "c7eb14f1388e42f29fbac8eef195fede",
      "40b7d134ba7b40299f6bd16b1e781d09",
      "13e5a9ab53bc4e1d8c3c7905ebd05ddf",
      "bc3c9b957d93490c8f9773e7050fd4ef",
      "e702472021334098b47210f9f1395f21",
      "8ed62e486a664d1f83fee6f21cb4585d",
      "ad42520b822e4454947f5d466424c8ca",
      "917d4196cdd14970b7965652f3b950a2",
      "3d72f5178e404d8c948ef13fc81a2bb8",
      "0ae32803eeaf436ab66cb3da9ff5439d",
      "5f28c40531fc4f43b4b62fd04912100c",
      "3c7dd63eb62d44598666166a87c0ff88",
      "7ed9d2386d57488c890d1ec712d1bef0",
      "da3c1240d6a94f3c9ff9ec0545675446",
      "cc7c2ac1006b4b109f302d7d26e3a578",
      "9bb4dcdb6f434d8b8cc95ef80f2ffe00",
      "5af83a55a73b4f3481a7559951ebdf08",
      "c4384ee96a1a4ae4bc8b46fa640b9058",
      "5f2bf3aebfd64baf94df39964208af4c",
      "307da63f10ad4ae5ae5e7e387d176d39",
      "8a282a2414874cb588a386e1444951dd",
      "659e0ad7f39c4cdbbdc1345c07a2e3f4",
      "1481fbbc74544c4888c0d6f88d8b3c9d",
      "54b833977b424c25a690d1bfe5d8c3f1",
      "8bfab1db231a4ff1975775f81f8a84f1",
      "3af5019cfacf4afc8a15a14d9a121ea8",
      "0ac398931b04418cbd3d227b9c634883",
      "7789164b5ee245b3a537d9d61dae038f",
      "8abd99000df0453e86bc458be29a10c7",
      "7db95c969d94465aaffbe9139f0f2a90",
      "d21de377e9074df083556c68e7d7cdbb",
      "3a0ada2f620a40aaa6c33fcea7a0bb91",
      "38bc8c5a2a3447e7942e2058c35d5e5b",
      "79170285a49a450494f0bfbe6cb4788e",
      "9813f2bda5ce4ab6887683fe63d0629e",
      "5c6ca36359bb4a1982c599f6a3f23b9f",
      "6e2cfa054082449e942297afd2f50f73",
      "5f3133f5c7254c248dad8e75534ea920",
      "eef06f8bc02749f4b04af47d92eb91b2",
      "2c60d924ee7b4fad984e2072973c3af3",
      "c1132ffe6c684c7ebeb3f285a101536b",
      "aa2a21546de1463f88a7feb8b697f9d3",
      "0abba2f64b4a43abadb38d461a5c7f42",
      "f43e9d4fa52c48c087ffeef399bfbed6",
      "4f1eafd9553b4bd5b396bba8f3896a22",
      "bbd5e6b14d864c1eb6a4d9c4247fb520",
      "29b2e68155a947e0aca0760c7e8e4afa",
      "fa1b29c48993478f9c2be4877ec675fc",
      "e4a6b2af572e44eda537e74847c58709",
      "8110f29e7b134607b38b29fad32ab1a1",
      "470ba5ce89ee4927a9be685adbc08675",
      "d10fbe074d4b42c9a8a6522252ef016f",
      "5af59ca6be4a427c8f83a906bbb6ce93",
      "17c9592d03324c4797abc812044e7500",
      "f6153c2ce07b4f4097a2a0486fb896ac",
      "3daedb3b85264408a8821ee2e480b356",
      "b2ce0d43567b4cc09e1f5571ee25263c",
      "c3a28b9889cd40e2ac0b9860e7f3932b",
      "99e992b8d4564ddd9c7634ac7663053a",
      "ae627a7820d4450a97b47d63dd5fbd92",
      "56a8c3c144404bd793e29a023032a09f",
      "2a9348d6cd674898ac4c9a303a254f26",
      "bafebeb0fba04d148ae3adf44171e0fe",
      "a8e19067a3ad411cad1aba489d390516",
      "6cf4e15a56fc4ac188609b806f11afcd"
     ]
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:20.958550Z",
     "iopub.status.busy": "2021-11-15T01:52:20.956770Z",
     "iopub.status.idle": "2021-11-15T01:52:30.502825Z",
     "shell.execute_reply": "2021-11-15T01:52:30.502409Z"
    },
    "id": "QSVYD7o_eL2o",
    "outputId": "4f570825-b314-421b-b1ee-07449f7787f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading empty, pre-trained model with 160 parameters.\n",
      "Model attached to cuda:0\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(\"/opt/awsw\"):\n",
    "  # In case we run this locally (in Docker)\n",
    "  work_dir = os.path.join(\"/opt\", \"awsw\")\n",
    "else:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  work_dir = os.path.join(\"/content\", \"drive\", \"MyDrive\", \"endless_awsw\")\n",
    "\n",
    "models_dir = os.path.join(work_dir, \"models_3\")\n",
    "\n",
    "if not os.path.isdir(models_dir):\n",
    "    pathlib.Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M', pad_token_id = tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "#model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B', pad_token_id = tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "#model = GPT2LMHeadModel.from_pretrained('distilgpt2', pad_token_id = tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "named_parameters = list(model.named_parameters())\n",
    "\n",
    "# Freeze a part\n",
    "for name, param in named_parameters[:-10]:\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.config.attention_dropout = 0.01\n",
    "model.config.embed_dropout = 0.01\n",
    "print(f\"Loading empty, pre-trained model with {len(named_parameters)} parameters.\")\n",
    "\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model attached to {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMEavxJ32gOH"
   },
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:30.606702Z",
     "iopub.status.busy": "2021-11-15T01:52:30.570263Z",
     "iopub.status.idle": "2021-11-15T01:52:30.609758Z",
     "shell.execute_reply": "2021-11-15T01:52:30.609358Z"
    },
    "id": "OzWBTuEj2gOJ",
    "outputId": "0668db46-b7fd-4806-e90e-a898d2a6b77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_lines: \n",
      "train_lines: PlayerReply c \"Hey, Remy!\" DragonReply Ry \"Hello, [player_name].\"\n",
      "PlayerReply c \"Is there any particular reason why you wanted to meet here?\" DragonReply Ry \"I enjoy Tatsu Park is all. Have you been here before?\"\n",
      "PlayerReply c \"Can't say I have.\" PlayerReply c \"A few times.\" PlayerReply c \"Once or twice.\" DragonReply Ry \"I see.\" DragonReply Ry \"Well, what do you think of it?\"\n",
      "PlayerReply c \"It's pretty idyllic.\" DragonReply Ry smile \"It is. I like it a lot here.\"\n",
      "PlayerReply c \"It's pretty romantic.\" DragonReply Ry shy \"You think so?\"\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(work_dir, \"awsw_story_input.txt\")) as f:\n",
    "    data = f.read()\n",
    "lines = data.split(\"\\n\")\n",
    "player_dragon_pairs = {}\n",
    "last_player_talk = []\n",
    "closed_player_talk = False\n",
    "re_player_talk = re.compile(r'c \"(.*?)\"')\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line_split = line.split(\" \")\n",
    "    if len(line_split) <= 1:\n",
    "        continue\n",
    "    \n",
    "    if line_split[0] == \"c\":\n",
    "        if closed_player_talk:\n",
    "            closed_player_talk = False\n",
    "            last_player_talk = []\n",
    "        last_player_talk.append(re.sub(re_player_talk, r\"\\1\", line))\n",
    "    else:\n",
    "        if not closed_player_talk:\n",
    "            last_player_talk = json.dumps(last_player_talk)\n",
    "            if not last_player_talk in player_dragon_pairs:\n",
    "                player_dragon_pairs[last_player_talk] = []\n",
    "            closed_player_talk = True\n",
    "            \n",
    "        line = \"DragonReply \" + line\n",
    "        if last_player_talk is not None:\n",
    "            player_dragon_pairs[last_player_talk].append(line)\n",
    "    \n",
    "train_lines = []\n",
    "eval_lines = []\n",
    "eval_per_character = 0\n",
    "\n",
    "for player_line_str in player_dragon_pairs.keys():\n",
    "    player_lines = json.loads(player_line_str)\n",
    "    dragon_lines = player_dragon_pairs[player_line_str]\n",
    "    compiled_line = \" \".join([f'PlayerReply c \"{player_line}\"' for player_line in player_lines]) + \" \" + \" \".join(dragon_lines)\n",
    "    train_lines.append(compiled_line)\n",
    "    \n",
    "test_bucket = {}\n",
    "for l in train_lines:\n",
    "    l_split = l.split(\" \")\n",
    "    character = None\n",
    "    for i, ls in enumerate(l_split):\n",
    "        if ls == \"DragonReply\":\n",
    "            character = l_split[i + 1]\n",
    "            break\n",
    "    if not character in test_bucket:\n",
    "        test_bucket[character] = []\n",
    "    test_bucket[character].append(l)\n",
    "    \n",
    "for i in range(eval_per_character):\n",
    "    for character in test_bucket.keys():\n",
    "        random_line = test_bucket[character][randrange(len(test_bucket[character]))]\n",
    "        eval_lines.append(random_line)\n",
    "        for i2, t in enumerate(train_lines):\n",
    "            if t == random_line:\n",
    "                del train_lines[i2]\n",
    "                break\n",
    "    \n",
    "joined_eval_lines = \"\\n\".join(eval_lines[:5])\n",
    "print(f\"eval_lines: {joined_eval_lines}\")\n",
    "joined_train_lines = \"\\n\".join(train_lines[:5])\n",
    "print(f\"train_lines: {joined_train_lines}\")\n",
    "\n",
    "random.shuffle(train_lines)\n",
    "\n",
    "if not os.path.isfile(os.path.join(work_dir, \"data_train.txt\")):\n",
    "    with open(os.path.join(work_dir, \"data_train.txt\"), \"w\") as f:\n",
    "        for l in train_lines:\n",
    "            f.write(l + \"\\n\")\n",
    "            \n",
    "if not os.path.isfile(os.path.join(work_dir, \"data_test.txt\")):\n",
    "    with open(os.path.join(work_dir, \"data_test.txt\"), \"w\") as f:\n",
    "        for l in eval_lines:\n",
    "            f.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "a04e889a56e94ce9812a0e7d89d4d4f4",
      "3c2d0014295e47f9a07de8d4e36e7ba4",
      "76286c1e442541e0a6cb2d7cbbc9cc8f",
      "dcad41ec160e448b83d85c907a48facb",
      "27f2d980cb774902a82c0ad2545f549a",
      "131ce893dc784733b42f384b419145db",
      "607ee5c0b54042f3919d0588d66e31c1",
      "b2373342cb4942569088a5b20186795a",
      "11475ac58a594c6db1641401b1cc0d13",
      "f6625c167f7b4aa4bbecf2129e07c068",
      "ef9e25d7100e4e9ba064e938bd8157ea",
      "7bb369ca60f94acc99a4ee8e8f8fd261",
      "bc20a41ccda14aeeb72014b80a5ec53a",
      "c6ff5b9114424dbfafcbc3b3ab887af5",
      "1153397efee1426cac848341c0b88785",
      "37672ae801de4f42a9f6f49cc33fb88e",
      "8528f9fbbc504a44b9670a760256192d",
      "406b2f47896446a187725d4a1aa926f8",
      "cb33a87e718546f3a33b7deea817de56",
      "eeb54e278e184bc5aabf0283f1b276ff",
      "809a3780924641b8ace57e9141b0167f",
      "b805525a58804750ae56dad7e43ecb0e",
      "906b9daf9437432c81546f35256c7232",
      "0ce77d162a014a2fa17628ef8fe20846",
      "3fed2802cbab4a90a5b3d5a8c9bc8974",
      "a146d5c5588944368242c519984cbccc",
      "28d30b2eecf04ae6835cf4c35648dcfc",
      "65c48ca18ea14cf9bccbaa2495c2f120",
      "5067025a37bb42318ae93216b3205b17",
      "d18658b25f6f47778b984d0bf35be999",
      "6ade1603ab664b9f9d5ce44014bc5305",
      "fd91a671eeb7431d90d86b44beb276bd",
      "df0a544035814e1ca11d6191c10a5b62",
      "23a62f16d8de4f3ab9fa64ada07d1e35",
      "d50f5db4d1a1430caecace0510c1a24e",
      "d36395d1dcad4ead98f7f4756b8dbd37",
      "2cf737292a8343f5965c3eb0ace01875",
      "9ba5d1e5fab74947a328842b5105a28e",
      "073bc138772048729e04017123149e80",
      "ab0ce277776d4d218bf97fff5acc8e28",
      "b404b600842a4fc4b2b66c6b015235d6",
      "06dd02ec048945e5bf6b17d2b5558fb2",
      "437e050108fd46e1ba0f35674fa7314f",
      "a5725e8dc8104756bcba16b2c886a27f",
      "a31733df07ed4bb485d518b64634acfb",
      "1a2fe039b81a42c496ba363b2000bc41",
      "3f271f4469cf4796b92a78eba64c30b3",
      "fd200bcd96354e36b32ca82660eb0ef2",
      "33f9747c1f474c1790c7c293c853fad5",
      "e10acb8e2c8240409a19c61499576afd",
      "cc3c5e1ef5974710a06c1eab3d90cfb1",
      "6eda966317484df2a47fa0c4f2a0370c",
      "0cd6ff104b484203adda9e8414fd80fa",
      "c5a11599f37c4077a4ab0daec124a78c",
      "6cd8593a13f54c138c9adf5ec85f2d97"
     ]
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:30.625320Z",
     "iopub.status.busy": "2021-11-15T01:52:30.624687Z",
     "iopub.status.idle": "2021-11-15T01:52:47.805262Z",
     "shell.execute_reply": "2021-11-15T01:52:47.805666Z"
    },
    "id": "pWeL2qWd2gOK",
    "outputId": "efdd650f-d95e-47a9-ad2f-396593bb779e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56793d9167e34fec942e7a5ea185bb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('text', data_files={'train': os.path.join(work_dir, \"data_train.txt\"), 'test': os.path.join(work_dir, \"data_test.txt\")})\n",
    "\n",
    "class AWSWDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, dataset, dataset_type, do_shuffle=False):\n",
    "        self.current_dataset = dataset\n",
    "        self.dataset_type = dataset_type\n",
    "        self.do_shuffle = do_shuffle\n",
    "        self.shuffled_datasets = []\n",
    "        self.current_idx = 0\n",
    "        for i in range(1):\n",
    "            self.current_dataset = self.current_dataset.shuffle()\n",
    "            mapped_dataset = self.current_dataset.map(\n",
    "                group_texts,\n",
    "                batched=True,\n",
    "                batch_size=dataset_batch_size,\n",
    "                num_proc=dataset_map_cores\n",
    "            )\n",
    "            self.shuffled_datasets.append(mapped_dataset)\n",
    "        \n",
    "    def approx_len(self):\n",
    "        return len(self.shuffled_datasets[0][self.dataset_type])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.current_idx = (self.current_idx + 1) % len(self.shuffled_datasets)\n",
    "        return iter(self.shuffled_datasets[self.current_idx][self.dataset_type])\n",
    "    \n",
    "def encode(batch):\n",
    "    result = []\n",
    "    attention_mask = []\n",
    "    for item in batch['text']:\n",
    "        #tokens = [tokenizer.bos_token_id] + tokenizer.encode(item) + [tokenizer.eos_token_id]\n",
    "        #tokens = tokenizer.encode(item)\n",
    "        tokens = tokenizer.encode(item) + [tokenizer.eos_token_id]\n",
    "        result.append(tokens)\n",
    "        attention_mask.append([1] * len(tokens))\n",
    "    return {\n",
    "        'attention_mask': attention_mask,\n",
    "        'input_ids': result\n",
    "    }\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    #random_shift = random.randint(0, 64)\n",
    "    #concatenated_examples['input_ids'] = concatenated_examples['input_ids'][random_shift:]\n",
    "    #concatenated_examples['attention_mask'] = concatenated_examples['attention_mask'][random_shift:]\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Pad the end\n",
    "    to_add = (math.ceil(total_length / block_size) * block_size) - total_length\n",
    "    if to_add > 0:\n",
    "        concatenated_examples['input_ids'] += [tokenizer.pad_token_id] * to_add\n",
    "        concatenated_examples['attention_mask'] += [0] * to_add\n",
    "        total_length += to_add\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def map_dragon_reply_text(batch):\n",
    "    result = {'text': []}\n",
    "    for item in batch['text']:\n",
    "        item_split = item.split(\" \")\n",
    "        player_replies = []\n",
    "        dragon_replies = []\n",
    "        current_reply = []\n",
    "        handling_reply = None\n",
    "        for token in item_split:\n",
    "            if token == \"PlayerReply\":\n",
    "                if handling_reply is None:\n",
    "                    handling_reply = \"PlayerReply\"\n",
    "                else:\n",
    "                    if handling_reply == \"PlayerReply\":\n",
    "                        # We need to store the PlayerReply\n",
    "                        player_replies.append(\" \".join(current_reply))\n",
    "                        current_reply = []\n",
    "            elif token == \"DragonReply\":\n",
    "                if handling_reply == \"DragonReply\":\n",
    "                    # We need to store the DragonReply\n",
    "                    dragon_replies.append(\" \".join(current_reply))\n",
    "                    current_reply = []\n",
    "                    \n",
    "                if handling_reply == \"PlayerReply\":\n",
    "                    # We need to store the PlayerReply\n",
    "                    player_replies.append(\" \".join(current_reply))\n",
    "                    current_reply = []\n",
    "                    \n",
    "                handling_reply = \"DragonReply\"\n",
    "                current_reply = []\n",
    "                    \n",
    "            if handling_reply is not None:\n",
    "                current_reply.append(token)\n",
    "                \n",
    "        # There's always a dragon reply at the end.\n",
    "        dragon_replies.append(\" \".join(current_reply))\n",
    "        for player_idx in range(len(player_replies)):\n",
    "            for dragon_idx in range(len(dragon_replies)):\n",
    "                result['text'].append(player_replies[player_idx] + \" \" + dragon_replies[dragon_idx])\n",
    "                \n",
    "    return result\n",
    "\n",
    "dataset_map_cores = min(multiprocessing.cpu_count(), 10)\n",
    "dataset_batch_size = 1000\n",
    "\n",
    "dataset = dataset.map(\n",
    "    map_dragon_reply_text,\n",
    "    batched=True,\n",
    "    batch_size=dataset_batch_size,\n",
    "    num_proc=dataset_map_cores\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    encode,\n",
    "    batched=True,\n",
    "    batch_size=dataset_batch_size,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc=dataset_map_cores\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=dataset_batch_size,\n",
    "    num_proc=dataset_map_cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:47.811443Z",
     "iopub.status.busy": "2021-11-15T01:52:47.810867Z",
     "iopub.status.idle": "2021-11-15T01:52:47.813404Z",
     "shell.execute_reply": "2021-11-15T01:52:47.812899Z"
    },
    "id": "PhiZIfn02gOL",
    "outputId": "47e5768d-8b9d-4ea8-c5ac-cc392abba402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len: 8658 num_training_steps: 136 num_total_steps: 13600\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_len = len(dataset['train'])\n",
    "num_training_steps = math.ceil(train_len / batch_size)\n",
    "num_epoch = 100\n",
    "num_total_steps = num_training_steps * num_epoch\n",
    "num_warmup_steps = num_training_steps * 2\n",
    "print(f\"train_len: {train_len} num_training_steps: {num_training_steps} num_total_steps: {num_total_steps}\")\n",
    "def get_optimizer_and_scheduler(params):\n",
    "    optimizer = AdamW(params, lr=0.001)\n",
    "    #scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_total_steps)\n",
    "    #scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps, num_total_steps, power=0.5, lr_end=1e-10)\n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps, num_total_steps, 4)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:47.818373Z",
     "iopub.status.busy": "2021-11-15T01:52:47.817848Z",
     "iopub.status.idle": "2021-11-15T01:52:48.161718Z",
     "shell.execute_reply": "2021-11-15T01:52:48.161377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = []\n",
    "optimizer, scheduler = get_optimizer_and_scheduler([torch.tensor(0.1)])\n",
    "for i in range(num_total_steps):\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "plt.plot(lrs)\n",
    "plt.show()\n",
    "del lrs\n",
    "del optimizer\n",
    "del scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:48.237288Z",
     "iopub.status.busy": "2021-11-15T01:52:48.236686Z",
     "iopub.status.idle": "2021-11-15T01:52:48.313361Z",
     "shell.execute_reply": "2021-11-15T01:52:48.313761Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<'EOT' > ds_config_zero3.json\n",
    "{\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"overlap_comm\": true,\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"sub_group_size\": 1e9,\n",
    "        \"reduce_bucket_size\": \"auto\",\n",
    "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "        \"stage3_param_persistence_threshold\": \"auto\",\n",
    "        \"stage3_max_live_parameters\": 1e9,\n",
    "        \"stage3_max_reuse_distance\": 1e9,\n",
    "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 2000,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": false\n",
    "}\n",
    "EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "execution": {
     "iopub.execute_input": "2021-11-15T01:52:48.322468Z",
     "iopub.status.busy": "2021-11-15T01:52:48.321812Z",
     "iopub.status.idle": "2021-11-15T03:01:11.453898Z",
     "shell.execute_reply": "2021-11-15T03:01:11.453464Z"
    },
    "id": "AdPIW0xSTpRY",
    "outputId": "01338ca7-7ed2-4d8e-dd7b-5c235e4c5e30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8658\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13600' max='13600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13600/13600 1:08:22, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.928300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.894900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.773300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.723900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>5.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>6.800800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>6.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.765600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>6.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>6.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>6.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>6.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>5.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>5.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.941900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>5.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>5.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>4.900900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>4.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>4.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>4.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>4.351400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>4.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>4.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>3.859100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>3.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.534100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>3.458100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>3.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.329900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-1000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-1000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-1500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-1500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-2000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-2000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-2500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-2500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-3000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-3000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-3500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-3500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-4000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-4000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-4500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-4500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-5000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-5000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-5500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-5500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-6000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-6000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-6500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-6500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-7000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-7000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-7500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-7500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-8000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-8000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-8500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-8500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-9000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-9000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-9500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-9500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-10000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-10000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-10500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-10500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-11000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-11000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-11500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-11500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-12000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-12000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-12500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-12500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-13000\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-13000/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models_3/checkpoint-13500\n",
      "Configuration saved in /opt/awsw/models_3/checkpoint-13500/config.json\n",
      "Model weights saved in /opt/awsw/models_3/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models_3/checkpoint-12500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AWSWTrainer(Trainer):\n",
    "    def _get_train_sampler(self):\n",
    "        return None\n",
    "    \n",
    "class AWSWTrainerCallback(TrainerCallback):\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        learning_rate_history = [h['learning_rate'] for h in state.log_history if 'loss' in h]\n",
    "        loss_history = [h['loss'] for h in state.log_history if 'loss' in h]\n",
    "        fig, axs = plt.subplots(2)\n",
    "        fig.suptitle('Learning rate and loss')\n",
    "        axs[0].plot(learning_rate_history)\n",
    "        axs[1].plot(loss_history)\n",
    "        \n",
    "def train(model):\n",
    "    optimizer, scheduler = get_optimizer_and_scheduler(model.parameters())\n",
    "    training_args = TrainingArguments(\n",
    "        models_dir,\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epoch,\n",
    "        save_total_limit=2,\n",
    "        save_steps=500,\n",
    "        logging_steps=250,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        #deepspeed=\"ds_config_zero3.json\"\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=dataset['train'],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        callbacks=[AWSWTrainerCallback]\n",
    "    )\n",
    "    checkpoint_dirs = [os.path.join(models_dir, d) for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]\n",
    "    if len(checkpoint_dirs) > 0:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "        trainer.train(latest_checkpoint)\n",
    "    else:\n",
    "        trainer.train()\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:01:11.462443Z",
     "iopub.status.busy": "2021-11-15T03:01:11.460962Z",
     "iopub.status.idle": "2021-11-15T03:01:11.839147Z",
     "shell.execute_reply": "2021-11-15T03:01:11.838625Z"
    },
    "id": "5UePGmLD2gON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_dragon_reply(past, prompt, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    prompt = f'{past} PlayerReply c \"{prompt}\" DragonReply'\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        generated, \n",
    "        do_sample=(top_k is not None and top_p is not None),\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=block_size,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return tokenizer.decode(sample_outputs[0], skip_special_tokens=False)[len(prompt):].strip()\n",
    "\n",
    "prompts = [\n",
    "    ('PlayerReply c \"Hey Remy!\" DragonReply Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('PlayerReply c \"I was with Lorem today.\" DragonReply Ad \"That\\'s awesome. He\\'s a cute fellow.\"', \"What do you think of Lorem?\"),\n",
    "    ('DragonReply m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('DragonReply m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "# Set a fixed seed to make sure we get the same response every time.\n",
    "torch.manual_seed(80085)\n",
    "for (past, prompt) in prompts:\n",
    "    reply = generate_dragon_reply(past, prompt)\n",
    "    print(f\"Prompt: {prompt}\\nReply: {reply}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OZarUHg2gON"
   },
   "source": [
    "# Sampling test\n",
    "\n",
    "Which combination is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:01:11.845655Z",
     "iopub.status.busy": "2021-11-15T03:01:11.845104Z",
     "iopub.status.idle": "2021-11-15T03:01:58.525038Z",
     "shell.execute_reply": "2021-11-15T03:01:58.524574Z"
    },
    "id": "bWoLzL9B2gON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test 1 top_k: 6, top_p: 0.21] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 1 top_k: 6, top_p: 0.21] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 1 top_k: 6, top_p: 0.21] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 1 top_k: 6, top_p: 0.21] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 2 top_k: 74, top_p: 0.38] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. He had been mutual - I noticed.\"<|endoftext|>\n",
      "\n",
      "[Test 2 top_k: 74, top_p: 0.38] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I can't you.\"<|endoftext|>\n",
      "\n",
      "[Test 2 top_k: 74, top_p: 0.38] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"The dragons.\"<|endoftext|>\n",
      "\n",
      "[Test 2 top_k: 74, top_p: 0.38] -> Prompt: What will we do here?\n",
      "Reply: Ad \"I'm not a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 3 top_k: 89, top_p: 0.55] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had built a bit of work.\"<|endoftext|>\n",
      "\n",
      "[Test 3 top_k: 89, top_p: 0.55] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"Well, I'm not sure. Just.\"<|endoftext|>\n",
      "\n",
      "[Test 3 top_k: 89, top_p: 0.55] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"Not if you think that a lot of it.\"<|endoftext|>\n",
      "\n",
      "[Test 3 top_k: 89, top_p: 0.55] -> Prompt: What will we do here?\n",
      "Reply: Ad \"I see.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 4 top_k: 45, top_p: 0.34] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 4 top_k: 45, top_p: 0.34] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 4 top_k: 45, top_p: 0.34] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 4 top_k: 45, top_p: 0.34] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be able to do you to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 5 top_k: 51, top_p: 0.13] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 5 top_k: 51, top_p: 0.13] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the most person.\"<|endoftext|>\n",
      "\n",
      "[Test 5 top_k: 51, top_p: 0.13] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 5 top_k: 51, top_p: 0.13] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 6 top_k: 43, top_p: 0.33] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 6 top_k: 43, top_p: 0.33] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 6 top_k: 43, top_p: 0.33] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 6 top_k: 43, top_p: 0.33] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be able to do you to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 7 top_k: 75, top_p: 0.96] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 7 top_k: 75, top_p: 0.96] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 7 top_k: 75, top_p: 0.96] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 7 top_k: 75, top_p: 0.96] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That very about after it is the reality people, that he went to leave up to bring me started. She. I am I was the burden of my own, when me\n",
      "\n",
      "-------------\n",
      "[Test 8 top_k: 57, top_p: 0.64] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just stay.\"<|endoftext|>\n",
      "\n",
      "[Test 8 top_k: 57, top_p: 0.64] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 8 top_k: 57, top_p: 0.64] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 8 top_k: 57, top_p: 0.64] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We one was a lot of a lot of I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 9 top_k: 39, top_p: 0.3] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 9 top_k: 39, top_p: 0.3] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 9 top_k: 39, top_p: 0.3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 9 top_k: 39, top_p: 0.3] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a good amount of the portal, but it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 10 top_k: 56, top_p: 0.12] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 10 top_k: 56, top_p: 0.12] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the most person.\"<|endoftext|>\n",
      "\n",
      "[Test 10 top_k: 56, top_p: 0.12] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 10 top_k: 56, top_p: 0.12] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 11 top_k: 31, top_p: 0.74] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just about.\"<|endoftext|>\n",
      "\n",
      "[Test 11 top_k: 31, top_p: 0.74] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 11 top_k: 31, top_p: 0.74] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 11 top_k: 31, top_p: 0.74] -> Prompt: What will we do here?\n",
      "Reply: m \"We normal \"I'm that a lot, I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 12 top_k: 59, top_p: 0.75] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 12 top_k: 59, top_p: 0.75] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit.\"<|endoftext|>\n",
      "\n",
      "[Test 12 top_k: 59, top_p: 0.75] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"We're here?\"<|endoftext|>\n",
      "\n",
      "[Test 12 top_k: 59, top_p: 0.75] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I think it I was already had been outlawed that, that he had to leave out, so we'll see.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 13 top_k: 46, top_p: 0.14] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 13 top_k: 46, top_p: 0.14] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the most person.\"<|endoftext|>\n",
      "\n",
      "[Test 13 top_k: 46, top_p: 0.14] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 13 top_k: 46, top_p: 0.14] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 14 top_k: 69, top_p: 0.19] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I should I had been, but I can really.\"<|endoftext|>\n",
      "\n",
      "[Test 14 top_k: 69, top_p: 0.19] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I'm you, I'll see.\"<|endoftext|>\n",
      "\n",
      "[Test 14 top_k: 69, top_p: 0.19] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry, I could've made the day of the time.\"<|endoftext|>\n",
      "\n",
      "[Test 14 top_k: 69, top_p: 0.19] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a joke.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 15 top_k: 75, top_p: 0.81] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 15 top_k: 75, top_p: 0.81] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 15 top_k: 75, top_p: 0.81] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 15 top_k: 75, top_p: 0.81] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks led to leave up.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 16 top_k: 12, top_p: 0.12] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 16 top_k: 12, top_p: 0.12] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 16 top_k: 12, top_p: 0.12] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 16 top_k: 12, top_p: 0.12] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 17 top_k: 50, top_p: 0.3] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 17 top_k: 50, top_p: 0.3] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 17 top_k: 50, top_p: 0.3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 17 top_k: 50, top_p: 0.3] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a good amount of the portal, but it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 18 top_k: 40, top_p: 0.3] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 18 top_k: 40, top_p: 0.3] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 18 top_k: 40, top_p: 0.3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 18 top_k: 40, top_p: 0.3] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a good amount of the portal, but it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 19 top_k: 91, top_p: 0.29] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 19 top_k: 91, top_p: 0.29] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 19 top_k: 91, top_p: 0.29] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 19 top_k: 91, top_p: 0.29] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be able to do you to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 20 top_k: 46, top_p: 0.85] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 20 top_k: 46, top_p: 0.85] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 20 top_k: 46, top_p: 0.85] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Maybe.\"<|endoftext|>\n",
      "\n",
      "[Test 20 top_k: 46, top_p: 0.85] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks led to leave up.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 21 top_k: 100, top_p: 0.33] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 21 top_k: 100, top_p: 0.33] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 21 top_k: 100, top_p: 0.33] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I was the end before he's the dragons of our whole in a while I had been.\"<|endoftext|>\n",
      "\n",
      "[Test 21 top_k: 100, top_p: 0.33] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 22 top_k: 38, top_p: 0.9] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 22 top_k: 38, top_p: 0.9] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 22 top_k: 38, top_p: 0.9] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Maybe.\"<|endoftext|>\n",
      "\n",
      "[Test 22 top_k: 38, top_p: 0.9] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks led to leave up.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 23 top_k: 54, top_p: 0.22] -> Prompt: How are you?\n",
      "Reply: Br \"I'm sorry.\"<|endoftext|>\n",
      "\n",
      "[Test 23 top_k: 54, top_p: 0.22] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I'm you, I'll see.\"<|endoftext|>\n",
      "\n",
      "[Test 23 top_k: 54, top_p: 0.22] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 23 top_k: 54, top_p: 0.22] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a few' council- a lot of our PDA.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 24 top_k: 71, top_p: 0.44] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. He had been mutual just.\"<|endoftext|>\n",
      "\n",
      "[Test 24 top_k: 71, top_p: 0.44] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo think \"I don't even if I'll, you, I can do you.\"<|endoftext|>\n",
      "\n",
      "[Test 24 top_k: 71, top_p: 0.44] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"Well, so I was the day.\"<|endoftext|>\n",
      "\n",
      "[Test 24 top_k: 71, top_p: 0.44] -> Prompt: What will we do here?\n",
      "Reply: Ad \"It's not the burden of the message.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 25 top_k: 52, top_p: 0.72] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we talked. He.\"<|endoftext|>\n",
      "\n",
      "[Test 25 top_k: 52, top_p: 0.72] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 25 top_k: 52, top_p: 0.72] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 25 top_k: 52, top_p: 0.72] -> Prompt: What will we do here?\n",
      "Reply: m \"We normal \"I'm that a lot, I'm the portal before I didn't mind the message that was the burden of our gun in the/ than it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 26 top_k: 26, top_p: 0.44] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 26 top_k: 26, top_p: 0.44] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo think this?\"<|endoftext|>\n",
      "\n",
      "[Test 26 top_k: 26, top_p: 0.44] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You have the day of us.\"<|endoftext|>\n",
      "\n",
      "[Test 26 top_k: 26, top_p: 0.44] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That was to make a number of me to the message a few biumi.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 27 top_k: 70, top_p: 0.75] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 27 top_k: 70, top_p: 0.75] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit.\"<|endoftext|>\n",
      "\n",
      "[Test 27 top_k: 70, top_p: 0.75] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"We're here?\"<|endoftext|>\n",
      "\n",
      "[Test 27 top_k: 70, top_p: 0.75] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I think it I was already had been outlawed that, that he had to think it.\" DragonReply Ad \"No. That.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 28 top_k: 60, top_p: 0.87] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 28 top_k: 60, top_p: 0.87] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 28 top_k: 60, top_p: 0.87] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 28 top_k: 60, top_p: 0.87] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks led to leave up.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 29 top_k: 87, top_p: 0.23] -> Prompt: How are you?\n",
      "Reply: Br \"I'm sorry.\"<|endoftext|>\n",
      "\n",
      "[Test 29 top_k: 87, top_p: 0.23] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I'm you, I'll see.\"<|endoftext|>\n",
      "\n",
      "[Test 29 top_k: 87, top_p: 0.23] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 29 top_k: 87, top_p: 0.23] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a good amount of the portal, but it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 30 top_k: 48, top_p: 0.05] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 30 top_k: 48, top_p: 0.05] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 30 top_k: 48, top_p: 0.05] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 30 top_k: 48, top_p: 0.05] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 31 top_k: 57, top_p: 0.99] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 31 top_k: 57, top_p: 0.99] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 31 top_k: 57, top_p: 0.99] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: c \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 31 top_k: 57, top_p: 0.99] -> Prompt: What will we do here?\n",
      "Reply: Sb smileze- her it it someone decided are, that he could're I'm.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 32 top_k: 64, top_p: 0.3] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 32 top_k: 64, top_p: 0.3] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 32 top_k: 64, top_p: 0.3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 32 top_k: 64, top_p: 0.3] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be able to do you to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 33 top_k: 75, top_p: 0.02] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 33 top_k: 75, top_p: 0.02] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 33 top_k: 75, top_p: 0.02] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 33 top_k: 75, top_p: 0.02] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 34 top_k: 29, top_p: 0.53] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 34 top_k: 29, top_p: 0.53] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 34 top_k: 29, top_p: 0.53] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 34 top_k: 29, top_p: 0.53] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm the burden of my destination of the other than the same the same to do you about the other side, it out to go to meet the message a portal, it\n",
      "\n",
      "-------------\n",
      "[Test 35 top_k: 15, top_p: 0.82] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 35 top_k: 15, top_p: 0.82] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I'm they be recognized?\"<|endoftext|>\n",
      "\n",
      "[Test 35 top_k: 15, top_p: 0.82] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"That, we're leaving.\"<|endoftext|>\n",
      "\n",
      "[Test 35 top_k: 15, top_p: 0.82] -> Prompt: What will we do here?\n",
      "Reply: Ad \"That soundsI'm not the reality one, that he could see I can do you.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 36 top_k: 42, top_p: 0.82] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 36 top_k: 42, top_p: 0.82] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit.\"<|endoftext|>\n",
      "\n",
      "[Test 36 top_k: 42, top_p: 0.82] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"We're here?\"<|endoftext|>\n",
      "\n",
      "[Test 36 top_k: 42, top_p: 0.82] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I think about the message it it this was a whole remained, you're I felt the portal- he didn't let me.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 37 top_k: 66, top_p: 0.47] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 37 top_k: 66, top_p: 0.47] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 37 top_k: 66, top_p: 0.47] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 37 top_k: 66, top_p: 0.47] -> Prompt: What will we do here?\n",
      "Reply: Ry \"You have to the portal to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 38 top_k: 95, top_p: 0.39] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. He had been mutual just.\"<|endoftext|>\n",
      "\n",
      "[Test 38 top_k: 95, top_p: 0.39] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo think \"I don't even if I'll, you, I can do you.\"<|endoftext|>\n",
      "\n",
      "[Test 38 top_k: 95, top_p: 0.39] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"Well, so I was the day.\"<|endoftext|>\n",
      "\n",
      "[Test 38 top_k: 95, top_p: 0.39] -> Prompt: What will we do here?\n",
      "Reply: Ad \"It's not the burden of the message.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 39 top_k: 70, top_p: 0.07] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 39 top_k: 70, top_p: 0.07] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 39 top_k: 70, top_p: 0.07] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 39 top_k: 70, top_p: 0.07] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 40 top_k: 85, top_p: 0.29] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 40 top_k: 85, top_p: 0.29] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 40 top_k: 85, top_p: 0.29] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 40 top_k: 85, top_p: 0.29] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be able to do you to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 41 top_k: 59, top_p: 0.24] -> Prompt: How are you?\n",
      "Reply: Br \"I'm sorry.\"<|endoftext|>\n",
      "\n",
      "[Test 41 top_k: 59, top_p: 0.24] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I'm you, I'll see.\"<|endoftext|>\n",
      "\n",
      "[Test 41 top_k: 59, top_p: 0.24] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 41 top_k: 59, top_p: 0.24] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a good amount of the portal, but it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 42 top_k: 59, top_p: 0.1] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 42 top_k: 59, top_p: 0.1] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 42 top_k: 59, top_p: 0.1] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I'm not sure what I could have to us is, I was the time?\"<|endoftext|>\n",
      "\n",
      "[Test 42 top_k: 59, top_p: 0.1] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a joke.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 43 top_k: 42, top_p: 0.59] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be sent.\"<|endoftext|>\n",
      "\n",
      "[Test 43 top_k: 42, top_p: 0.59] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 43 top_k: 42, top_p: 0.59] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"Well, I was already came, I don't have a lot to leave.\"<|endoftext|>\n",
      "\n",
      "[Test 43 top_k: 42, top_p: 0.59] -> Prompt: What will we do here?\n",
      "Reply: Ad \"I see.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 44 top_k: 3, top_p: 0.12] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 44 top_k: 3, top_p: 0.12] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 44 top_k: 3, top_p: 0.12] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 44 top_k: 3, top_p: 0.12] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 45 top_k: 18, top_p: 0.1] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 45 top_k: 18, top_p: 0.1] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 45 top_k: 18, top_p: 0.1] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 45 top_k: 18, top_p: 0.1] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 46 top_k: 73, top_p: 0.61] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just stay.\"<|endoftext|>\n",
      "\n",
      "[Test 46 top_k: 73, top_p: 0.61] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 46 top_k: 73, top_p: 0.61] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 46 top_k: 73, top_p: 0.61] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We one was a lot of a lot of I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 47 top_k: 76, top_p: 0.51] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be sent.\"<|endoftext|>\n",
      "\n",
      "[Test 47 top_k: 76, top_p: 0.51] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 47 top_k: 76, top_p: 0.51] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"Well, I was already came, I don't have a lot, I can do.\"<|endoftext|>\n",
      "\n",
      "[Test 47 top_k: 76, top_p: 0.51] -> Prompt: What will we do here?\n",
      "Reply: Ad \"It's no.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 48 top_k: 31, top_p: 0.2] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 48 top_k: 31, top_p: 0.2] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the most person.\"<|endoftext|>\n",
      "\n",
      "[Test 48 top_k: 31, top_p: 0.2] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry, I could've made the day.\"<|endoftext|>\n",
      "\n",
      "[Test 48 top_k: 31, top_p: 0.2] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We turned a few, I'm not sure if I was a few the first, I was a good amount of the next to meet the portal, I was a good amount\n",
      "\n",
      "-------------\n",
      "[Test 49 top_k: 92, top_p: 0.81] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 49 top_k: 92, top_p: 0.81] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 49 top_k: 92, top_p: 0.81] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 49 top_k: 92, top_p: 0.81] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks led to leave up.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 50 top_k: 87, top_p: 0.32] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 50 top_k: 87, top_p: 0.32] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 50 top_k: 87, top_p: 0.32] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 50 top_k: 87, top_p: 0.32] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a bit of nowhere, I was to see you can do you think this that was the very a few, I was the next to the reality\n",
      "\n",
      "-------------\n",
      "[Test 51 top_k: 41, top_p: 0.28] -> Prompt: How are you?\n",
      "Reply: Br \"I'm sorry.\"<|endoftext|>\n",
      "\n",
      "[Test 51 top_k: 41, top_p: 0.28] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I'm you, I'll see.\"<|endoftext|>\n",
      "\n",
      "[Test 51 top_k: 41, top_p: 0.28] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 51 top_k: 41, top_p: 0.28] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a good amount of the portal, but it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 52 top_k: 35, top_p: 0.78] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 52 top_k: 35, top_p: 0.78] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit.\"<|endoftext|>\n",
      "\n",
      "[Test 52 top_k: 35, top_p: 0.78] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"That, we're leaving.\"<|endoftext|>\n",
      "\n",
      "[Test 52 top_k: 35, top_p: 0.78] -> Prompt: What will we do here?\n",
      "Reply: Ad \"After the other side is one was a lot to make you to leave out.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 53 top_k: 4, top_p: 0.5] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 53 top_k: 4, top_p: 0.5] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the most person.\"<|endoftext|>\n",
      "\n",
      "[Test 53 top_k: 4, top_p: 0.5] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry, I could've got six bullets.\"<|endoftext|>\n",
      "\n",
      "[Test 53 top_k: 4, top_p: 0.5] -> Prompt: What will we do here?\n",
      "Reply: Ad \"I'm not sure what's again.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 54 top_k: 80, top_p: 0.46] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 54 top_k: 80, top_p: 0.46] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 54 top_k: 80, top_p: 0.46] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 54 top_k: 80, top_p: 0.46] -> Prompt: What will we do here?\n",
      "Reply: Ry \"You have to the portal to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 55 top_k: 29, top_p: 0.18] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 55 top_k: 29, top_p: 0.18] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the most person.\"<|endoftext|>\n",
      "\n",
      "[Test 55 top_k: 29, top_p: 0.18] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 55 top_k: 29, top_p: 0.18] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 56 top_k: 79, top_p: 0.59] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just stay.\"<|endoftext|>\n",
      "\n",
      "[Test 56 top_k: 79, top_p: 0.59] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 56 top_k: 79, top_p: 0.59] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 56 top_k: 79, top_p: 0.59] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We one was a lot of a lot of I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 57 top_k: 7, top_p: 0.92] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 57 top_k: 7, top_p: 0.92] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I'm they be recognized?\"<|endoftext|>\n",
      "\n",
      "[Test 57 top_k: 7, top_p: 0.92] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"Just I can do?\"<|endoftext|>\n",
      "\n",
      "[Test 57 top_k: 7, top_p: 0.92] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 58 top_k: 58, top_p: 0.43] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. He had been mutual just.\"<|endoftext|>\n",
      "\n",
      "[Test 58 top_k: 58, top_p: 0.43] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo think \"I don't even if I'll, you, I can do you.\"<|endoftext|>\n",
      "\n",
      "[Test 58 top_k: 58, top_p: 0.43] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"Well, so I was the day.\"<|endoftext|>\n",
      "\n",
      "[Test 58 top_k: 58, top_p: 0.43] -> Prompt: What will we do here?\n",
      "Reply: Ad \"It's not the burden of the message.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 59 top_k: 68, top_p: 0.68] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we talked. He.\"<|endoftext|>\n",
      "\n",
      "[Test 59 top_k: 68, top_p: 0.68] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We change to go.\"<|endoftext|>\n",
      "\n",
      "[Test 59 top_k: 68, top_p: 0.68] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"Well, I was already came, and then, that a lot before I am.\"<|endoftext|>\n",
      "\n",
      "[Test 59 top_k: 68, top_p: 0.68] -> Prompt: What will we do here?\n",
      "Reply: Ad \"I'm not a few what he'd the middle sun for me.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 60 top_k: 92, top_p: 0.8] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 60 top_k: 92, top_p: 0.8] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 60 top_k: 92, top_p: 0.8] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 60 top_k: 92, top_p: 0.8] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks led to leave up.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 61 top_k: 85, top_p: 0.92] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 61 top_k: 85, top_p: 0.92] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 61 top_k: 85, top_p: 0.92] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 61 top_k: 85, top_p: 0.92] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That very about Reza had been talking\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 62 top_k: 78, top_p: 0.99] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 62 top_k: 78, top_p: 0.99] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 62 top_k: 78, top_p: 0.99] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: c \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 62 top_k: 78, top_p: 0.99] -> Prompt: What will we do here?\n",
      "Reply: Sb smileze- her it it someone decided are, that he could're I may introduce in the same. She still had to make a joke into a reason about in front of\n",
      "\n",
      "-------------\n",
      "[Test 63 top_k: 91, top_p: 0.77] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 63 top_k: 91, top_p: 0.77] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 63 top_k: 91, top_p: 0.77] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 63 top_k: 91, top_p: 0.77] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks with least I'm.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 64 top_k: 54, top_p: 0.21] -> Prompt: How are you?\n",
      "Reply: Br \"I'm sorry.\"<|endoftext|>\n",
      "\n",
      "[Test 64 top_k: 54, top_p: 0.21] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I'm you, I'll see.\"<|endoftext|>\n",
      "\n",
      "[Test 64 top_k: 54, top_p: 0.21] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 64 top_k: 54, top_p: 0.21] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that, I'm not sure if I was a good amount of the portal to see.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 65 top_k: 35, top_p: 0.58] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 65 top_k: 35, top_p: 0.58] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 65 top_k: 35, top_p: 0.58] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 65 top_k: 35, top_p: 0.58] -> Prompt: What will we do here?\n",
      "Reply: Ry \"You have to the portal to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 66 top_k: 93, top_p: 0.03] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 66 top_k: 93, top_p: 0.03] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 66 top_k: 93, top_p: 0.03] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 66 top_k: 93, top_p: 0.03] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 67 top_k: 86, top_p: 0.75] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 67 top_k: 86, top_p: 0.75] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 67 top_k: 86, top_p: 0.75] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Maybe.\"<|endoftext|>\n",
      "\n",
      "[Test 67 top_k: 86, top_p: 0.75] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks with least I'm.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 68 top_k: 84, top_p: 0.87] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 68 top_k: 84, top_p: 0.87] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 68 top_k: 84, top_p: 0.87] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 68 top_k: 84, top_p: 0.87] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That very about Reza had been talking\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 69 top_k: 76, top_p: 0.58] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just stay.\"<|endoftext|>\n",
      "\n",
      "[Test 69 top_k: 76, top_p: 0.58] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 69 top_k: 76, top_p: 0.58] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 69 top_k: 76, top_p: 0.58] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We one was a lot of a lot of I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 70 top_k: 86, top_p: 0.6] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just stay.\"<|endoftext|>\n",
      "\n",
      "[Test 70 top_k: 86, top_p: 0.6] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 70 top_k: 86, top_p: 0.6] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 70 top_k: 86, top_p: 0.6] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We one was a lot of a lot of I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 71 top_k: 94, top_p: 0.0] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 71 top_k: 94, top_p: 0.0] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 71 top_k: 94, top_p: 0.0] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 71 top_k: 94, top_p: 0.0] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 72 top_k: 89, top_p: 0.31] -> Prompt: How are you?\n",
      "Reply: Br \"I'm.\"<|endoftext|>\n",
      "\n",
      "[Test 72 top_k: 89, top_p: 0.31] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a number of this?\"<|endoftext|>\n",
      "\n",
      "[Test 72 top_k: 89, top_p: 0.31] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 72 top_k: 89, top_p: 0.31] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a bit of nowhere, I was to see you can do you think this I was the ground.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 73 top_k: 3, top_p: 0.35] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 73 top_k: 3, top_p: 0.35] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 73 top_k: 3, top_p: 0.35] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 73 top_k: 3, top_p: 0.35] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 74 top_k: 79, top_p: 0.79] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 74 top_k: 79, top_p: 0.79] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 74 top_k: 79, top_p: 0.79] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 74 top_k: 79, top_p: 0.79] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks with least I'm.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 75 top_k: 36, top_p: 0.59] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 75 top_k: 36, top_p: 0.59] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 75 top_k: 36, top_p: 0.59] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 75 top_k: 36, top_p: 0.59] -> Prompt: What will we do here?\n",
      "Reply: Ry \"There was the burden of our last one's me to the whole standard of nowhere, we knew, we did to go out to go in the portal.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 76 top_k: 96, top_p: 0.53] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had built that.\"<|endoftext|>\n",
      "\n",
      "[Test 76 top_k: 96, top_p: 0.53] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 76 top_k: 96, top_p: 0.53] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 76 top_k: 96, top_p: 0.53] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We one was a lot of a lot of I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 77 top_k: 69, top_p: 0.61] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just stay.\"<|endoftext|>\n",
      "\n",
      "[Test 77 top_k: 69, top_p: 0.61] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 77 top_k: 69, top_p: 0.61] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 77 top_k: 69, top_p: 0.61] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We one was a lot of a lot of I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 78 top_k: 95, top_p: 0.82] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 78 top_k: 95, top_p: 0.82] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 78 top_k: 95, top_p: 0.82] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 78 top_k: 95, top_p: 0.82] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks led to leave up.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 79 top_k: 32, top_p: 0.69] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just about.\"<|endoftext|>\n",
      "\n",
      "[Test 79 top_k: 32, top_p: 0.69] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 79 top_k: 32, top_p: 0.69] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 79 top_k: 32, top_p: 0.69] -> Prompt: What will we do here?\n",
      "Reply: m \"We normal \"I'm that a lot, I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 80 top_k: 19, top_p: 0.65] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 80 top_k: 19, top_p: 0.65] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 80 top_k: 19, top_p: 0.65] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 80 top_k: 19, top_p: 0.65] -> Prompt: What will we do here?\n",
      "Reply: Ry \"You have to the portal to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 81 top_k: 49, top_p: 0.74] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we talked. He.\"<|endoftext|>\n",
      "\n",
      "[Test 81 top_k: 49, top_p: 0.74] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 81 top_k: 49, top_p: 0.74] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 81 top_k: 49, top_p: 0.74] -> Prompt: What will we do here?\n",
      "Reply: m \"We normal \"I'm that a lot, I'm the portal before I didn't mind the message that was the burden of our gun in the/ than it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 82 top_k: 25, top_p: 1.0] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 82 top_k: 25, top_p: 1.0] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 82 top_k: 25, top_p: 1.0] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: c \"Why.\"<|endoftext|>\n",
      "\n",
      "[Test 82 top_k: 25, top_p: 1.0] -> Prompt: What will we do here?\n",
      "Reply: Sb smile.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 83 top_k: 51, top_p: 0.26] -> Prompt: How are you?\n",
      "Reply: Br \"I'm sorry.\"<|endoftext|>\n",
      "\n",
      "[Test 83 top_k: 51, top_p: 0.26] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I'm you, I'll see.\"<|endoftext|>\n",
      "\n",
      "[Test 83 top_k: 51, top_p: 0.26] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 83 top_k: 51, top_p: 0.26] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a good amount of the portal, but it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 84 top_k: 61, top_p: 0.53] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be sent.\"<|endoftext|>\n",
      "\n",
      "[Test 84 top_k: 61, top_p: 0.53] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 84 top_k: 61, top_p: 0.53] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"Well, I was already came, I don't have a lot, I can do.\"<|endoftext|>\n",
      "\n",
      "[Test 84 top_k: 61, top_p: 0.53] -> Prompt: What will we do here?\n",
      "Reply: Ad \"It's no.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 85 top_k: 54, top_p: 0.64] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had just stay.\"<|endoftext|>\n",
      "\n",
      "[Test 85 top_k: 54, top_p: 0.64] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"We can really?\"<|endoftext|>\n",
      "\n",
      "[Test 85 top_k: 54, top_p: 0.64] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"You do.\"<|endoftext|>\n",
      "\n",
      "[Test 85 top_k: 54, top_p: 0.64] -> Prompt: What will we do here?\n",
      "Reply: Ry \"We one was a lot of a lot of I'm the portal to help them.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 86 top_k: 100, top_p: 0.78] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 86 top_k: 100, top_p: 0.78] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 86 top_k: 100, top_p: 0.78] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 86 top_k: 100, top_p: 0.78] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks with least I'm.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 87 top_k: 85, top_p: 0.99] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 87 top_k: 85, top_p: 0.99] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 87 top_k: 85, top_p: 0.99] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: c \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 87 top_k: 85, top_p: 0.99] -> Prompt: What will we do here?\n",
      "Reply: Sb smileze- her it it someone decided are, that he could're I may introduce in the same. She still had to make a joke into a reason about in front of\n",
      "\n",
      "-------------\n",
      "[Test 88 top_k: 31, top_p: 0.6] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 88 top_k: 31, top_p: 0.6] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 88 top_k: 31, top_p: 0.6] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 88 top_k: 31, top_p: 0.6] -> Prompt: What will we do here?\n",
      "Reply: Ry \"You have to the portal to do it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 89 top_k: 13, top_p: 0.73] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 89 top_k: 13, top_p: 0.73] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 89 top_k: 13, top_p: 0.73] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 89 top_k: 13, top_p: 0.73] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I was the burden of our last one.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 90 top_k: 96, top_p: 0.72] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you talking about the dawn, but you're not go about him. This told any place, there.\"<|endoftext|>\n",
      "\n",
      "[Test 90 top_k: 96, top_p: 0.72] -> Prompt: What do you think of Lorem?\n",
      "Reply: m \"But you talking about, that he went to leave up, because we'll. You can't really place. A\n",
      "\n",
      "[Test 90 top_k: 96, top_p: 0.72] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"You do?\" DragonReply Br \"The voice that you'd have to them. If we can't you know what's quite.\"<|endoftext|>\n",
      "\n",
      "[Test 90 top_k: 96, top_p: 0.72] -> Prompt: What will we do here?\n",
      "Reply: Ad \"Yeah series m \"I have to work.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 91 top_k: 94, top_p: 0.68] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we talked before I'm just not not.\"<|endoftext|>\n",
      "\n",
      "[Test 91 top_k: 94, top_p: 0.68] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 91 top_k: 94, top_p: 0.68] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Ry \"Well, I was already came, and then, that he away, I am.\"<|endoftext|>\n",
      "\n",
      "[Test 91 top_k: 94, top_p: 0.68] -> Prompt: What will we do here?\n",
      "Reply: Ad \"I'm not a few what he'd the middle sun for me.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 92 top_k: 69, top_p: 1.0] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 92 top_k: 69, top_p: 1.0] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 92 top_k: 69, top_p: 1.0] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: c \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 92 top_k: 69, top_p: 1.0] -> Prompt: What will we do here?\n",
      "Reply: Sb smileze- her it it someone decided are, that he could're I'm.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 93 top_k: 17, top_p: 0.59] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, and saw, I don't be able to go. It I'll decided to go. He could be able to make her.\"<|endoftext|>\n",
      "\n",
      "[Test 93 top_k: 17, top_p: 0.59] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"You'll.\"<|endoftext|>\n",
      "\n",
      "[Test 93 top_k: 17, top_p: 0.59] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 93 top_k: 17, top_p: 0.59] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm the burden of my destination of the other than the same the same to do it, I was a good batch of my whole} was a lot to the most boring\n",
      "\n",
      "-------------\n",
      "[Test 94 top_k: 2, top_p: 0.64] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 94 top_k: 2, top_p: 0.64] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the most person.\"<|endoftext|>\n",
      "\n",
      "[Test 94 top_k: 2, top_p: 0.64] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry, I could've got six bullets.\"<|endoftext|>\n",
      "\n",
      "[Test 94 top_k: 2, top_p: 0.64] -> Prompt: What will we do here?\n",
      "Reply: Ad \"I'm not sure what I'm not sure what I was a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 95 top_k: 50, top_p: 0.98] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 95 top_k: 50, top_p: 0.98] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 95 top_k: 50, top_p: 0.98] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: c \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 95 top_k: 50, top_p: 0.98] -> Prompt: What will we do here?\n",
      "Reply: Sb smile.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 96 top_k: 68, top_p: 0.11] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 96 top_k: 68, top_p: 0.11] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the most person.\"<|endoftext|>\n",
      "\n",
      "[Test 96 top_k: 68, top_p: 0.11] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I don't worry.\"<|endoftext|>\n",
      "\n",
      "[Test 96 top_k: 68, top_p: 0.11] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a few.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 97 top_k: 83, top_p: 0.79] -> Prompt: How are you?\n",
      "Reply: Br brow.\"<|endoftext|>\n",
      "\n",
      "[Test 97 top_k: 83, top_p: 0.79] -> Prompt: What do you think of Lorem?\n",
      "Reply: An \"I've was a bit by the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 97 top_k: 83, top_p: 0.79] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: Br \"Yeah.\"<|endoftext|>\n",
      "\n",
      "[Test 97 top_k: 83, top_p: 0.79] -> Prompt: What will we do here?\n",
      "Reply: Ry \"That wasn't make it is worth mentioning, at the fireworks with least I'm.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 98 top_k: 47, top_p: 0.25] -> Prompt: How are you?\n",
      "Reply: Br \"I'm sorry.\"<|endoftext|>\n",
      "\n",
      "[Test 98 top_k: 47, top_p: 0.25] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I'm you, I'll see.\"<|endoftext|>\n",
      "\n",
      "[Test 98 top_k: 47, top_p: 0.25] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 98 top_k: 47, top_p: 0.25] -> Prompt: What will we do here?\n",
      "Reply: Ry \"Not it, I'm not a lot to be a good amount of the portal, but it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 99 top_k: 36, top_p: 0.64] -> Prompt: How are you?\n",
      "Reply: Br \"I'm you, we had built a bit of the other occasionally and light. They.\"<|endoftext|>\n",
      "\n",
      "[Test 99 top_k: 36, top_p: 0.64] -> Prompt: What do you think of Lorem?\n",
      "Reply: Sb \"That's Reza is the best are, and a burgundy it.\"<|endoftext|>\n",
      "\n",
      "[Test 99 top_k: 36, top_p: 0.64] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"L than.\"<|endoftext|>\n",
      "\n",
      "[Test 99 top_k: 36, top_p: 0.64] -> Prompt: What will we do here?\n",
      "Reply: Ry \"There was the burden of our last one.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 100 top_k: 35, top_p: 0.11] -> Prompt: How are you?\n",
      "Reply: Br \"I'm I'm not.\"<|endoftext|>\n",
      "\n",
      "[Test 100 top_k: 35, top_p: 0.11] -> Prompt: What do you think of Lorem?\n",
      "Reply: Lo sad \"I don't be a few kind of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 100 top_k: 35, top_p: 0.11] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: m \"I'm not sure what I could have to us is, I was the time?\"<|endoftext|>\n",
      "\n",
      "[Test 100 top_k: 35, top_p: 0.11] -> Prompt: What will we do here?\n",
      "Reply: Ry \"I'm not sure that a joke.\"<|endoftext|>\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    torch.manual_seed(80085)\n",
    "    top_k = random.randint(0, 100)\n",
    "    top_p = round(random.uniform(0, 1), 2)\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = generate_dragon_reply(past, prompt, top_k = top_k, top_p = top_p)\n",
    "        print(f\"[Test {i + 1} top_k: {top_k}, top_p: {top_p}] -> Prompt: {prompt}\\nReply: {reply}\\n\")\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T03:01:58.532380Z",
     "iopub.status.busy": "2021-11-15T03:01:58.531813Z",
     "iopub.status.idle": "2021-11-15T03:01:58.644051Z",
     "shell.execute_reply": "2021-11-15T03:01:58.644535Z"
    },
    "id": "FgM9Awn7acpG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What to say?\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5951/61523620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What to say?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \"\"\"\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             raise StdinNotImplementedError(\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "def generate_reply(prompt):\n",
    "    model.eval()\n",
    "    prompt = f'PlayerReply c \"{prompt}\" DragonReply'\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "    print(prompt, generated)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        generated, \n",
    "        do_sample=True,   \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        top_length = block_size,\n",
    "        top_p=0.95, \n",
    "        num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))\n",
    "\n",
    "print(\"What to say?\")\n",
    "print(generate_reply(input()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXKM4uLM2gOO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AWSW_GPT-Neo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1bb3a95f15ed461e9c8e02b9a7819f32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27742ac127fe41f9b9e44fd6078a31ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4db46e40ef004a2eb3d3ad78cb1adcd8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56793d9167e34fec942e7a5ea185bb09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eddb6d2bca484ecfb8ddb730d9933741",
        "IPY_MODEL_c6b82b893841407c9850c89722b3f613",
        "IPY_MODEL_cd8369f284f2418ea140703fa17f1cdd"
       ],
       "layout": "IPY_MODEL_f222c0f2f33043a5bbe54c9115b1e851"
      }
     },
     "9ae68d8307da4f1b95acb04fe81054c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a51382864682417fa4c9a3cb54952f5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6b82b893841407c9850c89722b3f613": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4db46e40ef004a2eb3d3ad78cb1adcd8",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_27742ac127fe41f9b9e44fd6078a31ec",
       "value": 2.0
      }
     },
     "cd8369f284f2418ea140703fa17f1cdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1bb3a95f15ed461e9c8e02b9a7819f32",
       "placeholder": "​",
       "style": "IPY_MODEL_9ae68d8307da4f1b95acb04fe81054c5",
       "value": " 2/2 [00:00&lt;00:00, 102.72it/s]"
      }
     },
     "dea98cdec0484e568ed38166af6a6336": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "eddb6d2bca484ecfb8ddb730d9933741": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a51382864682417fa4c9a3cb54952f5a",
       "placeholder": "​",
       "style": "IPY_MODEL_dea98cdec0484e568ed38166af6a6336",
       "value": "100%"
      }
     },
     "f222c0f2f33043a5bbe54c9115b1e851": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

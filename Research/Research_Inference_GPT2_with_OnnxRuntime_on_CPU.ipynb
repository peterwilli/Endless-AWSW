{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch GPT2 Model with ONNX Runtime on CPU\n",
    "\n",
    "In this tutorial, you'll be introduced to how to load a GPT2 model from PyTorch, convert it to ONNX, and inference it using ONNX Runtime using IO Binding. Note that past state is used to get better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites ##\n",
    "\n",
    "If you have Jupyter Notebook, you may directly run this notebook. We will use pip to install or upgrade [PyTorch](https://pytorch.org/), [OnnxRuntime](https://microsoft.github.io/onnxruntime/) and other required packages.\n",
    "\n",
    "Otherwise, you can setup a new environment. First, we install [AnaConda](https://www.anaconda.com/distribution/). Then open an AnaConda prompt window and run the following commands:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.8\n",
    "conda activate cpu_env\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Install PyTorch 1.6.0 and OnnxRuntime 1.5.1 for CPU-only.\n",
    "import sys\n",
    "if sys.platform == 'darwin': # Mac\n",
    "    !{sys.executable} -m pip install --upgrade torch torchvision\n",
    "else:\n",
    "    !{sys.executable} -m pip install --upgrade torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!{sys.executable} -m pip install onnxruntime==1.5.1\n",
    "\n",
    "# Install other packages used in this notebook.\n",
    "!{sys.executable} -m pip install transformers==3.0.2\n",
    "!{sys.executable} -m pip install onnx onnxconverter_common psutil pytz pandas py-cpuinfo py3nvml netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a cache directory to store pretrained model.\n",
    "cache_dir = os.path.join(\".\", \"cache_models\")\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: coloredlogs in /home/awsw-dev/.local/lib/python3.8/site-packages (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/awsw-dev/.local/lib/python3.8/site-packages (from coloredlogs) (10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install coloredlogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GPT2 model from PyTorch to ONNX ##\n",
    "\n",
    "We have a script [convert_to_onnx.py](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/convert_to_onnx.py) that could help you to convert GPT2 with past state to ONNX. \n",
    "\n",
    "The script accepts a pretrained model name or path of a checkpoint directory as input, and converts the model to ONNX. It also verifies that the ONNX model could generate same input as the pytorch model. The usage is like \n",
    "```\n",
    "python -m onnxruntime.transformers.convert_to_onnx -m model_name_or_path --output gpt2.onnx -o -p fp32|fp16|int8\n",
    "```\n",
    "The -p option can be used to choose the precision: fp32 (float32), fp16 (mixed precision) or int8 (quantization). The -o option will generate optimized model, which is required for fp16 or int8.\n",
    "\n",
    "Here we use a pretrained model as example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.15.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.transformers.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import numpy\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "model = MyGPT2LMHeadModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n",
    "device = torch.device(\"cpu\")\n",
    "model.eval().to(device)\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "num_attention_heads = model.config.n_head\n",
    "hidden_size = model.config.n_embd\n",
    "num_layer = model.config.n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:100: UserWarning: `example_outputs' is deprecated and ignored. Will be removed in next PyTorch release.\n",
      "  warnings.warn(\"`example_outputs' is deprecated and ignored. Will be removed in \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/opt/awsw/models/transformers/src/transformers/models/gpt2/modeling_gpt2.py:794: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n",
      "/opt/awsw/models/transformers/src/transformers/models/gpt2/modeling_gpt2.py:324: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  past_key, past_value = layer_past\n",
      "/opt/awsw/models/transformers/src/transformers/models/gpt2/modeling_gpt2.py:196: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"gpt2.onnx\"\n",
    "Gpt2Helper.export_onnx(model, device, onnx_model_path) # add parameter use_external_data_format=True when model size > 2 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Inference using Huggingface Transformers##\n",
    "\n",
    "In the following, we will use an example input to get the output from PyTorch for comparison purpose.\n",
    "For the first inference, there is no any past state. We can prepare empty state for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeArg(name='input_ids', type='tensor(int64)', shape=['batch_size', 'seq_len'])\n",
      "NodeArg(name='position_ids', type='tensor(int64)', shape=['batch_size', 'seq_len'])\n",
      "NodeArg(name='attention_mask', type='tensor(float)', shape=['batch_size', 'total_seq_len'])\n",
      "NodeArg(name='past_0', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_1', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_2', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_3', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_4', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_5', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_6', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_7', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_8', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_9', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_10', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "NodeArg(name='past_11', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "Outputs:\n",
      "NodeArg(name='logits', type='tensor(float)', shape=['batch_size', 'seq_len', 50257])\n",
      "NodeArg(name='present_0', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_1', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_2', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_3', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_4', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_5', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_6', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_7', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_8', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_9', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_10', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n",
      "NodeArg(name='present_11', type='tensor(float)', shape=[2, 'batch_size', 12, 'total_seq_len', 64])\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort_session = ort.InferenceSession(\"gpt2.onnx\")\n",
    "for input in ort_session.get_inputs():\n",
    "    print(input)\n",
    "print(\"Outputs:\")\n",
    "for output in ort_session.get_outputs():\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name_or_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_97/2309837507.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempty_past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempty_past\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_example_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_97/2309837507.py\u001b[0m in \u001b[0;36mget_example_inputs\u001b[0;34m(prompt_text)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_example_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEXAMPLE_Text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mencodings_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name_or_path' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "EXAMPLE_Text = ['best hotel in bay area', 'here is an example of gpt2 model']\n",
    "\n",
    "def get_tokenizer(model_name_or_path, cache_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    return tokenizer\n",
    "\n",
    "def get_example_inputs(prompt_text=EXAMPLE_Text):    \n",
    "    tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
    "    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
    "\n",
    "    input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n",
    "    attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n",
    "    position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
    "    position_ids.masked_fill_(position_ids < 0, 0)\n",
    "\n",
    "    #Empty Past State for generating first word\n",
    "    empty_past = []\n",
    "    batch_size = input_ids.size(0)\n",
    "    print(\"batch_size\", batch_size)\n",
    "    sequence_length = input_ids.size(1)\n",
    "    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n",
    "    for i in range(num_layer):\n",
    "        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n",
    "       \n",
    "    return input_ids, attention_mask, position_ids, empty_past\n",
    "\n",
    "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
    "print(\"input_ids\", input_ids)\n",
    "print(\"attention_mask\", attention_mask)\n",
    "print(\"position_ids\", position_ids)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    torch_output = torch_model(input_ids, past=empty_past, attention_mask=attention_mask, position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy.ascontiguousarray(input_ids.cpu().numpy())## ONNX Runtime Inference ##\n",
    "\n",
    "We can use ONNX Runtime to inference. The inputs are dictionary with name and numpy array as value, and the output is list of numpy array. Note that both input and output are in CPU. When you run the inference in GPU, it will involve data copy between CPU and GPU for input and output.\n",
    "\n",
    "Let's create an inference session for ONNX Runtime given the exported ONNX model, and see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n",
      "NodeArg(name='input_ids', type='tensor(int64)', shape=['batch_size', 'seq_len'])\n",
      "processed_shape [1, 20]\n",
      "NodeArg(name='position_ids', type='tensor(int64)', shape=['batch_size', 'seq_len'])\n",
      "processed_shape [1, 20]\n",
      "NodeArg(name='attention_mask', type='tensor(float)', shape=['batch_size', 'total_seq_len'])\n",
      "processed_shape [1, 20]\n",
      "NodeArg(name='past_0', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_1', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_2', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_3', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_4', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_5', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_6', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_7', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_8', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_9', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_10', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n",
      "NodeArg(name='past_11', type='tensor(float)', shape=[2, 'batch_size', 12, 'past_seq_len', 64])\n",
      "processed_shape [2, 1, 12, 0, 64]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
    "\n",
    "onnx_model_path = \"gpt2.onnx\"\n",
    "session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "inputs = {}\n",
    "shape_name_mapping = {\n",
    "    'seq_len': 20,\n",
    "    'total_seq_len': 20,\n",
    "    'past_seq_len': 0,\n",
    "    'batch_size': 1\n",
    "}\n",
    "type_name_mapping = {\n",
    "    'tensor(int64)': np.int64,\n",
    "    'tensor(float)': np.float32\n",
    "}\n",
    "def map_shape(x):\n",
    "    if type(x) is str:\n",
    "        return shape_name_mapping[x]\n",
    "    return x\n",
    "for input in ort_session.get_inputs():\n",
    "    print(input)\n",
    "    processed_shape = list(map(map_shape, input.shape))\n",
    "    print(\"processed_shape\", processed_shape)\n",
    "    inputs[input.name] = np.zeros(processed_shape, dtype = type_name_mapping[input.type])\n",
    "\n",
    "ort_inputs = inputs\n",
    "ort_outputs = session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n",
      "{'input_ids': array([[50256, 50256, 50256, 50256, 13466,  7541,   287, 15489,  1989],\n",
      "       [ 1456,   318,   281,  1672,   286,   308,   457,    17,  2746]]), 'attention_mask': array([[0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32), 'position_ids': array([[0, 0, 0, 0, 0, 1, 2, 3, 4],\n",
      "       [0, 1, 2, 3, 4, 5, 6, 7, 8]]), 'past_0': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_1': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_2': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_3': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_4': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_5': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_6': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_7': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_8': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_9': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_10': array([], shape=(2, 2, 12, 0, 64), dtype=float32), 'past_11': array([], shape=(2, 2, 12, 0, 64), dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
    "\n",
    "onnx_model_path = \"gpt2.onnx\"\n",
    "session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "ort_inputs = {'input_ids': input_ids.numpy(),\n",
    "              'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
    "              'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy())\n",
    "             }\n",
    "for i, past_i in enumerate(empty_past):\n",
    "    ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
    "print(ort_inputs)\n",
    "ort_outputs = session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the outputs from PyTorch and ONNX Runtime. Logits are very close (max difference is 1E-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits_masked_diff = (torch_output[0] - ort_outputs[0]) * attention_mask.unsqueeze(2)\n",
    "max_logits_diff = logits_masked_diff.abs().max()\n",
    "print(\"max logits diff (ignored padding)\", max_logits_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Runtime Inference with IO Binding ##\n",
    "\n",
    "To avoid data copy for input and output, ONNX Runtime also supports IO Binding. User could provide some buffer for input and outputs. For GPU inference, the buffer can be in GPU to reduce memory copy between CPU and GPU. This is helpful for high performance inference in GPU. For GPT-2, IO Binding might help the performance when batch size or (past) sequence length is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past):\n",
    "    output_shapes = Gpt2Helper.get_output_shapes(batch_size=input_ids.size(0),\n",
    "                                                 past_sequence_length=past[0].size(3),\n",
    "                                                 sequence_length=input_ids.size(1),\n",
    "                                                 config=config)\n",
    "    output_buffers = Gpt2Helper.get_output_buffers(output_shapes, device)\n",
    "\n",
    "    io_binding = Gpt2Helper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past,\n",
    "                                               output_buffers, output_shapes)\n",
    "    session.run_with_iobinding(io_binding)\n",
    "\n",
    "    outputs = Gpt2Helper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes,\n",
    "                                                            return_numpy=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the result is exactly same with/without IO Binding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
    "outputs = inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, empty_past)\n",
    "for i in range(len(outputs)):\n",
    "    assert torch.eq(outputs[i], torch.from_numpy(ort_outputs[i])).all()\n",
    "print(\"IO Binding result is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Text Generation ##\n",
    "\n",
    "Here is an example for text generation using ONNX Runtime or PyTorch. For ONNX Runtime, IO Binding is used for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation using OnnxRuntime ...\n",
      "batch_size 2\n",
      "------------\n",
      "best hotel in bay area.\n",
      "\n",
      "The hotel is located in the historic Bayview neighborhood of San Francisco.\n",
      "\n",
      "The hotel is open daily from 9 a.m.\n",
      "------------\n",
      "here is an example of gpt2 model.\n",
      "\n",
      "The gpt2 model is a simple, but powerful, way to generate a GPT2-like data structure. It is a\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
    "input_text = EXAMPLE_Text\n",
    "test_generation(tokenizer, input_text, ort_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use PyTorch to run again and we can see that the result is exactly same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation(tokenizer, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Int8 Quantization ##\n",
    "Next, we will apply dynamic quantization to the model. We optimize the model before quantization to get better performance.\n",
    "\n",
    "Note that text generation result from fp32 and int8 models could be quite different. User shall evaluate the precision metric for your application for both fp32 and int8 models. If the quality of int8 model result is acceptable, you will be glad to find that it is faster than fp32 model in inference. \n",
    "\n",
    "Note that you can leverage [quantization aware training (QAT)](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/) for accuracy improvement if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers.quantize_helper import QuantizeHelper\n",
    "\n",
    "optimized_fp32_model_path = \"gpt2_fp32.onnx\"\n",
    "quantized_int8_model_path = \"gpt2_int8.onnx\"\n",
    "Gpt2Helper.optimize_onnx(\"gpt2.onnx\", optimized_fp32_model_path, False, model.config.num_attention_heads, model.config.hidden_size)\n",
    "QuantizeHelper.quantize_onnx_model(optimized_fp32_model_path, quantized_int8_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_int8 = onnxruntime.InferenceSession(quantized_int8_model_path)\n",
    "input_text = ['bert model optimization']\n",
    "test_generation(tokenizer, input_text, ort_session=session_int8, num_tokens_to_produce=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark ##\n",
    "There is a tool benchmark_gpt2.py, which can be used to measure the performance of GPT-2 by PyTorch, ONNX Runtime without/with IO Binding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m onnxruntime.transformers.benchmark_gpt2 -m gpt2 -o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m onnxruntime.transformers.benchmark_gpt2 -m gpt2 -o --precision int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that quantized model has significant speed up (close to 2x).\n",
    "\n",
    "### Test Environment ###\n",
    "The following is the hardware of the test machine, and software version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m onnxruntime.transformers.machine_info --silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

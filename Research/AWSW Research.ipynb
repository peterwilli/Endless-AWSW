{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b97b376-9ac5-4c70-ba13-769649099a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset, ModelSeeder\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887d9f81-96c3-408f-bd1c-7b126becc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use cuda:0 for training with seed: 3218885689\n"
     ]
    }
   ],
   "source": [
    "# seed = random.randint(0, 2 ** 32 - 1)\n",
    "seed = 3218885689\n",
    "random.seed(seed)\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"Will use {device_name} for training with seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e95113e-dfc1-40a9-a39b-5d56f3bf32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(os.path.join(Config.work_dir, \"awsw_story_input.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb65c62-47c6-4d36-bd1e-2adfef8ff1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": 6e-4,\n",
    "    \"warmup_factor\": 0,\n",
    "    \"scheduler\": \"polynomial_decay_schedule_with_warmup\",\n",
    "    \"lr_end\": 2e-6,\n",
    "    \"power\": 0.6,\n",
    "    #\"freeze_layer_rate\": 1e-4,\n",
    "    \"freeze_from_steps\": -1,\n",
    "    \"seed\": seed,\n",
    "    \"num_epoch\": 300\n",
    "}\n",
    "\n",
    "optuna_result_attachement = {\n",
    "    'lr': 0.001,\n",
    "    'scheduler': 'cosine_schedule_with_warmup',\n",
    "    'to_freeze_count': 0,\n",
    "    #\"to_freeze_gpt_blocks\": 11,\n",
    "    'warmup_factor': 1\n",
    "}\n",
    "config.update(optuna_result_attachement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ffae5b-e197-4417-a4cd-e6befc6ee393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded empty model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_main\")\n",
    "if os.path.exists(os.path.join(saved_model_path, \"pytorch_model.bin\")):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "    model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "    print(\"Pretrained model loaded\")\n",
    "else:\n",
    "    model, tokenizer = get_model(\"EleutherAI/gpt-neo-125M\")\n",
    "    print(\"Loaded empty model\")\n",
    "model.to(device)\n",
    "# set_pretrained_model_dropout(model.transformer.h[-1:], 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f3315-8f5d-4101-8e7e-2c0129204c94",
   "metadata": {},
   "source": [
    "# Test before training on a pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f68c3f1-4c0a-41a2-a3fc-b8b3c36e418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my dreams, I'm a dragon-god. I'm the dragon-god of the dragons, and I'm the dragon-god of the gods.\n",
      "\n",
      "A dragon is a god, a god-god. A dragon-god is a god, a god-god.\n",
      "\n",
      "I am the dragon-god of the gods, and I'm the dragon-god of the gods.\n",
      "\n",
      "I am the dragon-god of the gods, and I'm the dragon-god of the gods.\n",
      "\n",
      "My god is a dragon, a god-god. A god-god is a god, a god-\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "def test_regular_sampler():\n",
    "    print(model_manager.say_raw(\"In my dreams, I'm a dragon\", 50, 0.7))\n",
    "test_regular_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f74aa-5030-4ffd-ad2f-ae013647975e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reviewing our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0974ac13-c420-4275-b34a-3816f580cb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/awsw-dev/.cache/huggingface/datasets/text/default-62627411737808a5/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9c4bbdc46041b68d81b39c8e810a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85daef60fd714d18b3ceec9ae5bc1daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/awsw-dev/.cache/huggingface/datasets/text/default-62627411737808a5/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dcd38a857842349ebcea745a9469d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2159 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3376 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset demo snapshot:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2159 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3376 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<d><scn>black<msg>n \"Without electrical power, the quality of health care plummeted, and water and sewage systems were crippled. Diseases we thought defeated centuries ago made their comeback in a most unsightly fashion.\"<|endoftext|><d><scn>facin3<msg>m \"At the bottom of the door, the corner of an envelope was noticeably sticking out.\"<|endoftext|><d><scn>np2x<msg>Ad \"...\"<|endoftext|><p><msg>c \"They changed their minds, so I suppose you'll be stuck with me for a little while longer.\"<|endoftext|><p><\n",
      "RP review!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2159 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3376 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(tokenizer, path_train = os.path.join(Config.work_dir, \"data_train.txt\"))\n",
    "print(\"Dataset demo snapshot:\")\n",
    "for item in dataset['train']:\n",
    "    print(tokenizer.decode(item['input_ids']))\n",
    "    break\n",
    "\n",
    "print(\"RP review!\")\n",
    "has_seen_rp = False\n",
    "for item in dataset['train']:\n",
    "    decoded = tokenizer.decode(item['input_ids'])\n",
    "    if 'c \"Fight ' in decoded: \n",
    "        print(decoded)\n",
    "        has_seen_rp = True\n",
    "        continue        \n",
    "    if has_seen_rp:\n",
    "        print(decoded)\n",
    "        break\n",
    "# Clean up\n",
    "del has_seen_rp\n",
    "# dataset['model_seeder'].stop_worker()\n",
    "# del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e9e10-b2eb-421a-9948-dc3590e1f46d",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Model is put in training mode and we begin training. The `train_results` will contain all data after training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41750b03-f114-46c4-948e-637db79efe15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] set freeze_part_layers: True (freezing 0 out of 160 layers.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31200' max='31200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31200/31200 3:50:05, Epoch 300/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>1.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>0.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>0.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>0.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2184</td>\n",
       "      <td>0.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2496</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2808</td>\n",
       "      <td>0.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3432</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3744</td>\n",
       "      <td>0.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4056</td>\n",
       "      <td>0.627100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4368</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>0.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4992</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5304</td>\n",
       "      <td>0.602900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5616</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5928</td>\n",
       "      <td>0.572300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>0.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6552</td>\n",
       "      <td>0.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6864</td>\n",
       "      <td>0.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7176</td>\n",
       "      <td>0.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7488</td>\n",
       "      <td>0.557000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8112</td>\n",
       "      <td>0.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8424</td>\n",
       "      <td>0.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8736</td>\n",
       "      <td>0.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9048</td>\n",
       "      <td>0.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9360</td>\n",
       "      <td>4.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9672</td>\n",
       "      <td>2.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9984</td>\n",
       "      <td>2.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10296</td>\n",
       "      <td>1.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10608</td>\n",
       "      <td>1.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10920</td>\n",
       "      <td>1.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11232</td>\n",
       "      <td>1.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11544</td>\n",
       "      <td>1.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11856</td>\n",
       "      <td>0.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12168</td>\n",
       "      <td>0.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12480</td>\n",
       "      <td>0.856700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12792</td>\n",
       "      <td>0.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13104</td>\n",
       "      <td>0.833500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13416</td>\n",
       "      <td>0.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13728</td>\n",
       "      <td>0.752600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14040</td>\n",
       "      <td>0.668100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14352</td>\n",
       "      <td>0.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14664</td>\n",
       "      <td>0.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14976</td>\n",
       "      <td>0.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15288</td>\n",
       "      <td>0.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15912</td>\n",
       "      <td>0.570600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16224</td>\n",
       "      <td>0.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16536</td>\n",
       "      <td>0.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16848</td>\n",
       "      <td>0.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17160</td>\n",
       "      <td>0.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17472</td>\n",
       "      <td>0.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17784</td>\n",
       "      <td>0.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18096</td>\n",
       "      <td>0.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18408</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18720</td>\n",
       "      <td>0.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19032</td>\n",
       "      <td>0.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19344</td>\n",
       "      <td>0.389100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19656</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19968</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20280</td>\n",
       "      <td>0.353400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20592</td>\n",
       "      <td>0.473500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20904</td>\n",
       "      <td>0.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21216</td>\n",
       "      <td>0.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21528</td>\n",
       "      <td>0.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21840</td>\n",
       "      <td>0.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22152</td>\n",
       "      <td>0.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22464</td>\n",
       "      <td>0.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22776</td>\n",
       "      <td>0.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23088</td>\n",
       "      <td>0.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23712</td>\n",
       "      <td>0.286700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24024</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24336</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24648</td>\n",
       "      <td>0.279200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24960</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25272</td>\n",
       "      <td>0.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25584</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25896</td>\n",
       "      <td>0.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26208</td>\n",
       "      <td>0.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26520</td>\n",
       "      <td>0.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26832</td>\n",
       "      <td>0.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27144</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27456</td>\n",
       "      <td>0.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27768</td>\n",
       "      <td>0.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28080</td>\n",
       "      <td>0.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28392</td>\n",
       "      <td>0.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28704</td>\n",
       "      <td>0.246100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29016</td>\n",
       "      <td>0.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29328</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29640</td>\n",
       "      <td>0.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29952</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30264</td>\n",
       "      <td>0.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30576</td>\n",
       "      <td>0.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30888</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = {}\n",
    "model.train()\n",
    "train_model(model, tokenizer, dataset, config, train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c410a601-c5e4-4cc1-af2b-44a7378f3bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f88c206ba60>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Learning rate and loss')\n",
    "axs[0].plot(train_results['learning_rate_history'])\n",
    "axs[1].plot(train_results['loss_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753228f0-b420-4b3d-bd69-53da7a87bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.save_pretrained(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e4666-c1ff-4695-ba55-894503925f9b",
   "metadata": {},
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b88e1f-f5c3-4a95-8b00-e791148d8c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\n",
      "Cloning into 'gpt-neo-125M'...\n",
      "remote: Enumerating objects: 38, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
      "remote: Total 38 (delta 16), reused 0 (delta 0)\u001b[K\n",
      "Unpacking objects: 100% (38/38), 542.60 KiB | 907.00 KiB/s, done.\n",
      "Using framework PyTorch: 1.10.0+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "Validating ONNX model...\n",
      "\t-[] ONNX model outputs' name match reference model ({'present.3.key', 'present.1.key', 'present.1.value', 'present.7.value', 'present.6.value', 'present.10.value', 'logits', 'present.0.value', 'present.10.key', 'present.11.value', 'present.2.value', 'present.0.key', 'present.8.value', 'present.5.key', 'present.8.key', 'present.9.key', 'present.6.key', 'present.4.key', 'present.2.key', 'present.4.value', 'present.11.key', 'present.3.value', 'present.5.value', 'present.7.key', 'present.9.value'}\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[] (2, 1, 50257) matches (2, 1, 50257)\n",
      "\t\t-[x] values not close enough (atol: 0.0001)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/onnx/__main__.py\", line 71, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/onnx/__main__.py\", line 64, in main\n",
      "    validate_model_outputs(onnx_config, tokenizer, model, args.output, onnx_outputs, args.atol)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/onnx/convert.py\", line 214, in validate_model_outputs\n",
      "    raise ValueError(\n",
      "ValueError: Outputs values doesn't match between reference model and ONNX exported model: Got max absolute difference of: 0.002452850341796875\n"
     ]
    }
   ],
   "source": [
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "    !cp -n $saved_model_path/gpt-neo-125M/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/gpt-neo-125M\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm-with-past $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86f7a12-9bcb-4c4b-a679-7007a7e2caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)\n",
    "        !rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "739405a4-ab2a-410f-8091-b6bd9a18e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f9e7adb-f258-471a-9139-70da9f2120d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon bit... With its distinct lack a river shape than playing all of theantasy.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon\".\"<d><scn>np2x<msg>Ad \"Now, with all that's happened, I don't know what to think anymore.\"<d><scn>np2x<msg>m \"She looked disappointed. Not just in me, but probably my entire race.\"<d><scn>np2x<msg>Ad \"Goodbye, [player_name].\"<p><msg>c \"Goodbye, Adine.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon, there. No one else to run.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon\".\"<d><scn>np3<msg>m \"As I returned to them, I heard Bryce audibly gasping.\"<d><scn>np3<msg>Br \"Damn it.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon bit instead of hearing your fighting night.\"< scales right outside right now.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon's part of our creator turning into a dragon, right?\"<p><msg>c \"I don't think I've heard that one before.\"<d><scn>loremapt<msg>Lo \"I told you about it last time...\"<d><scn>loremapt<msg>Lo \"The short version is that some believe we were created by a human. What happened to that human after we were made is a mystery, though. Some think they turned into a dragon and lived among us.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon on two.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon\".\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon, there could just bit pushed around in the public.\"<d><msgn>l<m \"As I got up myear about it, and he did his one that I made a good manner, I manner I realized I realized he would have done/_ no manner, but I would have no idea what would help but on the basis of the manner, but you've learned about him, and you coulderedt family andorpor told him to be larger than I've heard that much. I got that long but but I won't see that small.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon now.\"<d><scn>black<msg>n \" dreams of ways of a wild one of the delicious smell of steak wafted into the room.\"<d><scn>black<msg>m \"Soon, I heard the sizzling of the frying steaks as their delicious smell wafted into the room.\"<d><scn>black<msg>m \"Fueled by its fragrant odor, I slowly lost myself to daydreams as I closed my eyes.\"<d><scn>black<msg>Ry \"Is something wrong?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon bit of a shame, then try to find a few ways to make the old couple.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon\".\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon on two.\"<msgn>laring{Price and Prayer\". You should totally read that one.\"c \"That's fine with your drink, but.\"<d><scn.\"<raved<d><scn>park2>cafe<msg>Ad \"\"Problems\" may not have been the \"When I were met\".\"><|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon dreams of dreams had free now.\"<d><scn>np3<msg>m \"I was on my way to the police station when a voice called out to me.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon bit of others that could never visit.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon's pretty good at one of them supposed to be a human female.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon on two.\"Ancestors? Well, it does.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon's part of our creator turning into a dragon, right?\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon bit by now.\"<d><scn>emeraroom.{scn>emeraroom<msg>Em \"Not really.\\ treatment treatment? Are you know about unpleasantified toy.\"<d><scn>park2<msg>An \"I don't toy for.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon\".\"<d><scn>np2x<msg>Ad \"Now, with all that's happened, I don't know what to think anymore.\"<d><scn>np2x<msg>m \"She looked disappointed. Not just in me, but probably my entire race.\"<d><scn>np2x<msg>Ad \"Goodbye, [player_name].\"<p><msg>c \"Goodbye, Adine.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(10):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a578b-32cd-44b9-829b-5aef5ec37ab0",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d72d4876-df61-461a-b715-980a3763f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my dreams, I'm a dragon now.\"<d><scn>np3<msg>m \"Meanwhile, I put a few bullets into the gun I took from Reza before I aimed at Maverick and pulled the trigger several times.\"<d><scn>np1n<msg>m \"Maverick collapsed instantly and looked at me with a pained expression. Beneath him lay Izumi, dead.\"<d><scn>np1n<msg>Mv \"Why?\"<d><scn>np1n<msg>m \"He was trembling and took a few slow\n"
     ]
    }
   ],
   "source": [
    "test_regular_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f9a94-70c6-4a58-9ad1-3d6ea373dfbe",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b97230c-1d3a-4dd2-a253-727e451d539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Pytorch...\n",
      "Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I was trying to figure out why you wanted to hang out with me so often.\"<p><msg>c \"What are you talking about?\"<d><scn>park2<msg>Ry \"Since Amelia died, I've been alone. It's been years. I don't even have anyone I could consider a friend.\"<d><scn>park2<msg\n",
      "\n",
      "\n",
      "Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \"In order for a portal to function, a natural wormhole is required. It is trapped and becomes the portal's entry.\"<d><scn>loremapt<msg>Ip \"The portal manipulates the wormhole's exit. This way, something can be sent to whichever destination is chosen.\"<d><scn\n",
      "\n",
      "\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"The human on official business, huh?\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<|endoftext|>\n",
      "\n",
      "\n",
      "Test ONNX...\n",
      "Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I see.\"<d><scn>park2<msg>Ry \"Well, what do you think?\"<p><msg>c \"It's yours.\"<d><scn>park2<msg>Ry \"I'm not sure if there is anything that can fill the void.\"<p><msg>c \"Have you ever talked to a professional about this?\"<dOkay.\"<p much bigger humanPeople about the map would not tall area, not tall adventure?\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: What do you think of Lorem?\n",
      "Reply: beach<msg>Ad \"I see. Sunning yourself like a Sunning yourself, though. Sunning yourself can be nice sometimes.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"You didn't even call me back when I left you a message. Is that a normal thing humans do? Not calling back?\"<d><scn>np2x<msg>Ad \"I thought you came here. As I given to our up. As I was given to me, I said, I was just like it to my back to my whole time, I felt my distinct humans would get the<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"I'm not sure if I could do that, I would be without the generators, or before and be without my flying on care of doing anything to get back to your world.\"<d><scn>beach<msg>Ad \"For me? Or was just about the first time, I was just about the first, my treatment you treatment real as with her hips now as soft.\"<d><scnift as well as well as well as well.\"<d>< in the {iMift as this might kind that everyone that moment that wonder as I feel like that...\"<\n",
      "\n",
      "\n",
      "PyTorch on cuda:0 took 1.4898 seconds\n",
      "ONNX on CPU took 3.9001 seconds\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "def sample_test(model_manager):\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = model_manager.say(past, prompt)\n",
    "        print(f\"Prompt: {prompt}\\nReply: {reply}\\n\\n\")\n",
    "print(\"Test Pytorch...\")\n",
    "start = time.time()\n",
    "sample_test(model_manager)\n",
    "end = time.time()\n",
    "pytorch_time = end - start\n",
    "print(\"Test ONNX...\")\n",
    "start = time.time()\n",
    "sample_test(onnx_model_manager)\n",
    "end = time.time()\n",
    "onnx_time = end - start\n",
    "print(f\"PyTorch on {device} took {pytorch_time:.4f} seconds\")\n",
    "print(f\"ONNX on CPU took {onnx_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3349bf-c777-4937-a023-ff0f4f21806d",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce4bb65f-e26e-4a62-9c67-aed885fe041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Pytorch...\n",
      "[Test 1] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't know. I don't want to face her. Whenever I see her, it's another reminder of what I lost. Besides, I don't think she wants to talk to me, either.\"<|endoftext|>\n",
      "\n",
      "[Test 1] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \"In order for a portal to function, a natural wormhole is required. It is trapped and becomes the portal's entry.\"<d><scn>loremapt<msg>Ip \"The portal manipulates the wormhole's exit. This way, something can be sent to whichever destination is chosen.\"<d><scn\n",
      "\n",
      "[Test 1] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"You didn't even call me back when I left you a message. Is that a normal thing humans do? Not calling back?\"<d><scn>np2x<msg>Ad \"I thought you had fun when we hung out in your apartment, but I guess I was wrong.\"<d><scn>np2x<msg>Ad \"Was I too\n",
      "\n",
      "[Test 1] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 2] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't know. I don't want to face her. Whenever I see her, it's another reminder of what I lost. Besides, I don't think she wants to talk to me, either.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \"In order for a portal to function, a natural wormhole is required. It is trapped and becomes the portal's entry.\"<d><scn>loremapt<msg>Ip \"The portal manipulates the wormhole's exit. This way, something can be sent to whichever destination is chosen.\"<d><scn\n",
      "\n",
      "[Test 2] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"I see.\"<p><msg>c \"Like an animal.\"<d><scn>loremapt<msg>Lo \"Pretty much. Most of us think those people are crazy, though.\"<p><msg>c \"I see.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<p><msg>c \"What would you like to know?\"<d><scn>o2<msg>Ad \"What did you do before you came here?\"<p><msg>c \"That's classified.\"<d><scn>o2<msg>Ad \"Of course it is.\"<p><msg>c \"What would you like to know?\"<\n",
      "\n",
      "-------------\n",
      "[Test 3] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I used to.\"<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \"In order for a portal to function, a natural wormhole is required. It is trapped and becomes the portal's entry.\"<d><scn>loremapt<msg>Ip \"The portal manipulates the wormhole's exit. This way, something can be sent to whichever destination is chosen.\"<d><scn\n",
      "\n",
      "[Test 3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"Well, is it true or not?\"<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 4] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I do.\"<|endoftext|>\n",
      "\n",
      "[Test 4] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \"In order for a portal to function, a natural wormhole is required. It is trapped and becomes the portal's entry.\"<d><scn>loremapt<msg>Ip \"The portal manipulates the wormhole's exit. This way, something can be sent to whichever destination is chosen.\"<d><scn\n",
      "\n",
      "[Test 4] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"You didn't even call me back when I left you a message. Is that a normal thing humans do? Not calling back?\"<d><scn>np2x<msg>Ad \"I thought you had fun when we hung out in your apartment, but I guess I was wrong.\"<d><scn>np2x<msg>Ad \"Was I too\n",
      "\n",
      "[Test 4] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<p><msg>c \"What's that?\"<d><scn>o2<msg>Ad \"It's the practice of doing flying maneuvers like rolls, spins or loops.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 5] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I used to.\"<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \" Lorem.\"<d><scn>loremapt<msg>Ip \" Lorem: Have you seen my Ixomen Sphere recently?\"<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"Well, is it true or not?\"<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 6] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I used to.\"<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \" Lorem.\"<d><scn>loremapt<msg>Ip \"Oh, right. [player_name] was going to visit today. I totally forgot about that. Are you still working with the police?\"<p><msg>c \"Yeah.\"<d><scn>loremapt<msg>\n",
      "\n",
      "[Test 6] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"The human on official business, huh?\"<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<p><msg>c \"What would you like to know?\"<d><scn>o2<msg>Ad \"What did you do before you came here?\"<p><msg>c \"That's classified.\"<d><scn>o2<msg>Ad \"Of course it is.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 7] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I was trying to figure out why you wanted to hang out with me so often.\"<p><msg>c \"What are you talking about?\"<d><scn>park2<msg>Ry \"Since Amelia died, I've been alone. It's been years. I don't even have anyone I could consider a friend.\"<d><scn>park2<msg\n",
      "\n",
      "[Test 7] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \"In order for a portal to function, a natural wormhole is required. It is trapped and becomes the portal's entry.\"<d><scn>loremapt<msg>Ip \"The portal manipulates the wormhole's exit. This way, something can be sent to whichever destination is chosen.\"<d><scn\n",
      "\n",
      "[Test 7] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"Well, is it true or not?\"<|endoftext|>\n",
      "\n",
      "[Test 7] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<p><msg>c \"What would you like to know?\"<d><scn>o2<msg>Ad \"What did you do before you came here?\"<p><msg>c \"That's classified.\"<d><scn>o2<msg>Ad \"Of course it is.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 8] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I was trying to figure out why you wanted to hang out with me so often.\"<p><msg>c \"What are you talking about?\"<d><scn>park2<msg>Ry \"Since Amelia died, I've been alone. It's been years. I don't even have anyone I could consider a friend.\"<d><scn>park2<msg\n",
      "\n",
      "[Test 8] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \" Lorem.\"<d><scn>loremapt<msg>Ip \" Lorem: Have you seen my Ixomen Sphere recently?\"<|endoftext|>\n",
      "\n",
      "[Test 8] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"I do Aerobatics, or stunt flying. I've been doing that on and off for a couple of years now.\"<p><msg>c \"That's a cool, but also pretty useless hobby.\"<d><scn>o2<msg>Ad \"That's what hobbies are for, though, right? It's just something I enjoy doing.\"<p><\n",
      "\n",
      "[Test 8] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 9] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I used to.\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \" Lorem.\"<d><scn>loremapt<msg>Ip \" Lorem: Have you seen my Ixomen Sphere recently?\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"Like what?\"<p><msg>c \"Dragons are noble creatures.\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<p><msg>c \"What would you like to know?\"<d><scn>o2<msg>Ad \"What did you do before you came here?\"<p><msg>c \"That's classified.\"<d><scn>o2<msg>Ad \"Of course it is.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 10] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I used to.\"<|endoftext|>\n",
      "\n",
      "[Test 10] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \"In order for a portal to function, a natural wormhole is required. It is trapped and becomes the portal's entry.\"<d><scn>loremapt<msg>Ip \"The portal manipulates the wormhole's exit. This way, something can be sent to whichever destination is chosen.\"<d><scn\n",
      "\n",
      "[Test 10] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"You didn't even call me back when I left you a message. Is that a normal thing humans do? Not calling back?\"<d><scn>np2x<msg>Ad \"I thought you had fun when we hung out in your apartment, but I guess I was wrong.\"<d><scn>np2x<msg>Ad \"Was I too\n",
      "\n",
      "[Test 10] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"What's that?\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "Test ONNX...\n",
      "[Test 1] -> Prompt: How are you?\n",
      "Reply: black?\"<p><msg>c city will stay in charge. It would take The whole[Say it.]\"< to other side of the wormholes connect a new edges.\"<d><scn>lones and apparently. When I get the space-time.\"<|endoftext|>\n",
      "\n",
      "[Test 1] -> Prompt: What do you think of Lorem?\n",
      "Reply: np4<msg>S44<msg>Sb [player_name] who nowity is in one of the other side dish and his other, his bowl before he lowered his proper in his pen.\"d><scn>c \"I thought you wanted to give it.\"<|endoftext|>\n",
      "\n",
      "[Test 1] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: park2<p \"I've already knew I was. I wish I had part of it.\"<|endoftext|>\n",
      "\n",
      "[Test 1] -> Prompt: What will we do here?\n",
      "Reply: np1x<msg>Sb>m \"I made the same, I heard the to take that she stole.\"<d><scn>np2y<msg( towards of wake you a sign of goodwill. It has to put the game as your generators, so everyone in our itself will. For her itself would reason, me know before Reza me!\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 2] -> Prompt: How are you?\n",
      "Reply: park?\"<p><msg>c \"It was actually the one that's pretty clear, meresent already mess if we get this if it matters.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What do you think of Lorem?\n",
      "Reply: black[Avoid the question by making a bad pun.]\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2< \"You know, what if that hadn told me about it, the world was a humanEFF what happened.\"<d \" Aisms, so I felt like I felt different.\"<d><scn>o4: What building a surprise surprise, but1n<msg>Emic were also the yours.\"<p><msg>c \"The<, [player_d><msg>lOf course I've peopleuhveBy have anM>LoThe placeThe place.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What will we do here?\n",
      "Reply: np2<msgn \"She turned her liking to me. I'm making her I neitheromy underNazis IIn some signs directing me to Emer toward their contact hobbies.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 3] -> Prompt: How are you?\n",
      "Reply: black<msg>Ry \"You know, you're doing yourself a drinking be a bad. drinking buddy.\"<d><sct]'<msg> buddy.\"<d>< community, but I know of The research I see.\"<d heard there is a lot of livestock and livestock to raise, and livestock cloth we see others. The other two several need everything behind just see by the see.\".]\"<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Ip \"It is a great crime.\"<d><scn>park2<p><msg>park2<d><'ve got a spot out of the facility. At least as its relations would. lot a very strong- ministers. If I ended a ministers. If I ended my body in the middle.\"<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np1n \"His wings.\"<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: What will we do here?\n",
      "Reply: cafe<p<msg>cafe>An \"I won't have a effect.\"<d><scn't.\"<d><scnun testing at this is very difficult for me my own species.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 4] -> Prompt: How are you?\n",
      "Reply: loremaptp \"That didn't make happen now: In any way fine, I guess it is my way for a littleblack well thank everything the [player I expected. No one Reza's the only one of you.\"c \"To be acted not, I did.\"<|endoftext|>\n",
      "\n",
      "[Test 4] -> Prompt: What do you think of Lorem?\n",
      "Reply: np2x<msg>AdSbb \"It's none of those horns.\"<p><msgn \"I still could see those horns, but in our real came and not so I did.\"loremors movements that it better.\"<|endoftext|>\n",
      "\n",
      "[Test 4] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: remyapt<p\"<p><msg>c \"(Where is my kind of fun! Where am I opinion is everyone involved is your kind of thing. Your health is your life here. They wanted to make their family.\"<p><'s going to be.\"<|endoftext|>\n",
      "\n",
      "[Test 4] -> Prompt: What will we do here?\n",
      "Reply: o4<msg>n \"The4<msg>S quickly opened's every second\" is the portal to collapse or other people in condition and other sure of happen.\"<d><scn>black<msg>n \"I wasnt sure if I didn't necessarily have any to just eyes, but I can Nolucent as other thanigsaw am not have a moment.\"<p><msg>c \"It wasn enough to think how it to helpdNeed a few seems like that that mean- out- out it all it all the words until I was unheard one that him unheard that his words living all my\n",
      "\n",
      "-------------\n",
      "[Test 5] -> Prompt: How are you?\n",
      "Reply: o2<msg>Em \"You, working as the p see that, of my family fortune. And what would make a to make myear with a good idea of the public - my jobs through the corner of my in the office.\"<d><sc>np3<d><scn>np3x<msg><msg>Mv \"Then you few minutes?\"<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: What do you think of Lorem?\n",
      "Reply: beach< culminating to, just didn't seem any more than you< pH nap.\"c \"I haven't have'orphaning any more than I thought a uniform. Would you mind helping those people, just like that, that would see.\"<p than you wanted to teach you one thing, that must heard of maybe I'm working with both just a way to get back for itcano thaton that everything prematurely that you what you wouldn certainlyletico reason that!] two very hot these the<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2< \"Nun?un would be with this.\"<p<d><scn>o<msg>SnSnacks, and other stuff I do?\"d><scn>o<d><scn>o2<msg>Sb \"First up is the primary witness in the production, production production facility.\"-o2<d><msg>Sb \"First up is the witness for one w murder un heard witness from the latest is her not helpful that witness inaaws and come forward to theaws back usual noticing theaws from its old and can see serious tableaws back in it usual\n",
      "\n",
      "[Test 5] -> Prompt: What will we do here?\n",
      "Reply: cafe<msg>An adventurer who values a while, though.\"<d><scn>cafe<>An \"Ohn \"Oh.\"<d><sc \"Oh, right>c><msg>An \"I'm not prepared for the sheer advantage. It's my kind of book.\"<d><scn><scnic Desirex<msgn<msg>m \"As the place was why my studies was did. When I did. Ad returned, I did. turn with.\" once my fact in my committee, so, having no longer I not return was decided to was already was did that would\n",
      "\n",
      "-------------\n",
      "[Test 6] -> Prompt: How are you?\n",
      "Reply: park2<msg>Br \"\" Deal with it.\"<d><sc.\"<d><msg>Br's.\"<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<d><msg>Lo \"I donts just cope, people will do with an other. No I feel the other.\"<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ka \"Yeah, I like that, had a spot, but rather thanDon't have a date with an interesting evening. With a rule.\"<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: What will we do here?\n",
      "Reply: o2<msg>Ad \"I'm not so sure if I can do that.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 7] -> Prompt: How are you?\n",
      "Reply: black<msg<d><scnnn>park3ch<msg>m \"The flames weakened and grew smaller until everyHowever, they had become smaller and body.\" - a dragons started out so.\"<d><scn>loremapt<msg>Ip \"How does not even mean I thought maybe a moment to make this going to first place.\"cafe<p \"I thought you wanted to just... would say but just like me Will Just Will Well.\"<p should have a Well see.\"<|endoftext|>\n",
      "\n",
      "[Test 7] -> Prompt: What do you think of Lorem?\n",
      "Reply: lorem leaned of a seat, holding up. I had a lot.\"m \"My the little horns, and What is yourordes got his gun here, I can no sign of you.\"<|endoftext|>\n",
      "\n",
      "[Test 7] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2<msgn \"The problem?\"<d>np2y<d><scn><msg>m \"I took the first place, making my place and your own body is inhabited, and creatures until the creatures becomes your species way. If our PDAs and read a spiritual being.\"<d><scn>facza while we already opened a spiritual being mostly them.\"<|endoftext|>\n",
      "\n",
      "[Test 7] -> Prompt: What will we do here?\n",
      "Reply: cafe.\"cafe?\"<p><msg<msg>An \"What exactly are you late?\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 8] -> Prompt: How are you?\n",
      "Reply: black<msg>Ry \"Is that so?\"<p><scn><?\"<|endoftext|>\n",
      "\n",
      "[Test 8] -> Prompt: What do you think of Lorem?\n",
      "Reply: beach<msg>Ad \"I practice just keep those on the practice.\"<|endoftext|>\n",
      "\n",
      "[Test 8] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2x<msg.. What do you think the public has a stone. At home - my min his busy more of him, I'll leave my theaturally it with a natural wormhole.\"<|endoftext|>\n",
      "\n",
      "[Test 8] -> Prompt: What will we do here?\n",
      "Reply: np3! Are you still comprised the comprised of any natural number number number, which, but IWon, It our the different number, and I have no number by many different species, because in fact because in our people back, but that they have all other.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 9] -> Prompt: How are you?\n",
      "Reply: parkr?\" night<d><scm \"The night was he's surface, I did wondering what she would see if he had been up to call him family.\"<d><scn>loremapt<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: What do you think of Lorem?\n",
      "Reply: lorem leaned over, and holding it.\"Ad sat down on the opposite is proof.\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: npIf I was a bad.\"<d><msg>An \"You know what way to please all the circumstances.\"<dscd><sc work.\"<p><msg>cafe>An \"We get caught, and caught caught several while working with an emergency one.\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: What will we do here?\n",
      "Reply: black<msg>Ry \"Well, the food food, of vandalism parts of will parts of it.\"<d><scn< positive things, but you will be said: \"But that's like the difference?\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 10] -> Prompt: How are you?\n",
      "Reply: park3? What are you still going to help us?\"<d><sca\"?\"<p><msg>c \"[It wasn't really my kind of a relationship time ago, but in my attempt, is to save it was rendered rendered speechless.\"p><d><d \"T demoral.\"<p><scn>npBy their author] late and says or it's good \" weren't too late.\"cBy the patience I don't you like someone of an Sometimes, it that an truth.\"<d<d be here, toward an Ad than happy it whatThat so but when you could really be too\n",
      "\n",
      "[Test 10] -> Prompt: What do you think of Lorem?\n",
      "Reply: beach<msg>Ad \"I like the dextrousearing I did not sure what it seemed like I get a little, but half was how I guess I have enough wasuggish tea. That was a torus.\"<|endoftext|>\n",
      "\n",
      "[Test 10] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np1n<msg>Em \"As than time, I knew that one would change. I would see on this. I made the more public will follow what you do.\"< plans to move in together, or while we have.\"d><msg>An \"We can remember anything. That's just where the lives of The people is all this is.\"<|endoftext|>\n",
      "\n",
      "[Test 10] -> Prompt: What will we do here?\n",
      "Reply: o<msg>Br \"I can, but I can't take any more time for making things with the chief of evidence before might contain while this an electronic components, too busy, like in the middle of an entirely different level and was about things. The truth.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "PyTorch on cuda:0 took 19.9670 seconds\n",
      "ONNX on CPU took 26.4791 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Pytorch...\")\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "        print(f\"[Test {i + 1}] -> Prompt: {prompt}\\nReply: {reply}\\n\")\n",
    "    print(\"-------------\")\n",
    "end = time.time()\n",
    "pytorch_time = end - start\n",
    "print(\"Test ONNX...\")\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "        print(f\"[Test {i + 1}] -> Prompt: {prompt}\\nReply: {reply}\\n\")\n",
    "    print(\"-------------\")\n",
    "end = time.time()\n",
    "onnx_time = end - start\n",
    "print(f\"PyTorch on {device} took {pytorch_time:.4f} seconds\")\n",
    "print(f\"ONNX on CPU took {onnx_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c956842-43ba-4f9a-937a-13fda9ad1439",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85435494-6b29-4b18-aa34-328e33caa9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit Lorem -> o2<msg>Ad \"What were you thinking when you first heard about us?\"<p><msg>c \"I thought it was a joke.\"<p><msg>c \"I thought it was a joke. Of course, all of that changed once I came through the portal.\"<d><scn>o2<msg>Ad \"See, having something you've only heard about in stories turn out to be real is quite special.\"<|endoftext|>\n",
      "Meet with Lorem -> park2<msg>Ry \"I'm not sure about that.\"<p><msg>c \"So, what are you going to do?\"<d><scn>facin2<msg>Br \"Yeah, probably not.\"<|endoftext|>\n",
      "Visit Adine -> o2<msg>Ad \"That's not funny, [player_name].\"<|endoftext|>\n",
      "Fight Maverick -> np2x<msg>Br \"Oh, really?\"<|endoftext|>\n",
      "Fight Adine -> o2<msg>Ad \"Oh, and... areestagename!t] it is, then.\"<p><msg>c \"Hmm, let me think.\"<p><msg>c \"How about...\"<d><scn>beach<msg>Ad \"Try again.\"<d><scn>beach<msg>Ad \"[adinestagename!t]? Are you sure?\"<p><msg>c \"No.\"<|endoftext|>\n",
      "Attack Adine -> np2x<msg>Ad \"No, I don't think I could dislike it, a little, but it probably the time is, but humans have been fun of something I realized you, and you didn't expect your have to see some of the game was a game a lot of my first day, you might think about him.\"<d><scn>np2x<msg>Ad \"Kind of a shame I never got to know you.\"<d><scn>np2x<msg>Ad \"Wait, does\n"
     ]
    }
   ],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight Maverick\",\n",
    "    \"Fight Adine\",\n",
    "    \"Attack Adine\"\n",
    "]\n",
    "for rp in test_rps:\n",
    "    print(f'{rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

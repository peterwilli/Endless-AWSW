{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b97b376-9ac5-4c70-ba13-769649099a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887d9f81-96c3-408f-bd1c-7b126becc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use cuda:0 for training with seed: 1880371636\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 2 ** 32 - 1)\n",
    "random.seed(seed)\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"Will use {device_name} for training with seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e95113e-dfc1-40a9-a39b-5d56f3bf32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(os.path.join(Config.work_dir, \"awsw_story_input.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb65c62-47c6-4d36-bd1e-2adfef8ff1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'EleutherAI/gpt-neo-125M',\n",
       " 'lr': 0.001,\n",
       " 'warmup_factor': 1,\n",
       " 'scheduler': 'cosine_schedule_with_warmup',\n",
       " 'lr_end': 2e-06,\n",
       " 'power': 0.6,\n",
       " 'freeze_from_steps': -1,\n",
       " 'seed': 1880371636,\n",
       " 'num_epoch': 50,\n",
       " 'to_freeze_count': 150}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"model_name\": \"EleutherAI/gpt-neo-125M\",\n",
    "    \"lr\": 6e-4,\n",
    "    \"warmup_factor\": 0,\n",
    "    \"scheduler\": \"polynomial_decay_schedule_with_warmup\",\n",
    "    \"lr_end\": 2e-6,\n",
    "    \"power\": 0.6,\n",
    "    #\"freeze_layer_rate\": 1e-4,\n",
    "    \"freeze_from_steps\": -1,\n",
    "    \"seed\": seed,\n",
    "    \"num_epoch\": 50,\n",
    "    \"to_freeze_count\": 150\n",
    "}\n",
    "\n",
    "optuna_result_attachement = {\n",
    "    'lr': 0.001,\n",
    "    'scheduler': 'cosine_schedule_with_warmup',\n",
    "    'to_freeze_count': 150,\n",
    "    'warmup_factor': 1\n",
    "}\n",
    "config.update(optuna_result_attachement)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41750b03-f114-46c4-948e-637db79efe15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset demo snapshot:\n",
      "<p><msg>c \"I see.\"<d><scn>town4<msg>Ad \"Are you ready to see some stunt flying?\"<|endoftext|><d><scn>park2<msg>Ry \"I see.\"<|endoftext|><p><msg>c \"Alright.\"<|endoftext|><p><msg>c \"So, when's the next round starting?\"<d><scn>black<msg>Br \"Soon enough. Do you feel it? Do you feel the buzz kicking in already?\"<|endoftext|><d><scn>facin2<msg>An \"Yeah, I'm very sorry we can\n",
      "'t accommodate your request right now, your highness.\"<|endoftext|>fur\", but \"<|endoftext|><scn>o2<msg>Br \"My point with all this is: We're all a very tight knit group at the police station. Me, Sebastian, Maverick, Naomi - the bond we form on the job is how we know to look out for each other.\"<|endoftext|><p><msg>c \"Let's start with the stone. Ideally, we want one that has a big surface area, but is as flat as possible.\"<d><scn>beach<msg>Ad \"Got it.\"<|endoftext|><d\n",
      "[0] set freeze_part_layers: True (freezing 150 out of 160 layers.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1571' max='5200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1571/5200 05:26 < 12:35, 4.80 it/s, Epoch 15.10/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>1.798800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>1.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>1.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>1.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>1.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>1.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>1.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>1.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>1.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>1.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>1.100900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>1.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1092</td>\n",
       "      <td>0.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1144</td>\n",
       "      <td>0.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1196</td>\n",
       "      <td>0.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>0.897200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1352</td>\n",
       "      <td>0.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1404</td>\n",
       "      <td>0.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>0.808600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1508</td>\n",
       "      <td>0.774600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.749700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = {}\n",
    "train_model(config, train_results, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410a601-c5e4-4cc1-af2b-44a7378f3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Learning rate and loss')\n",
    "axs[0].plot(train_results['learning_rate_history'])\n",
    "axs[1].plot(train_results['loss_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a578b-32cd-44b9-829b-5aef5ec37ab0",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c164b20-3772-451e-b09c-ca9ccc1a56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results['model'].eval()\n",
    "model_manager = ModelManager(model=train_results['model'], tokenizer=train_results['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97230c-1d3a-4dd2-a253-727e451d539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><msg>Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"Prompt: {prompt}\\nReply: {reply}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3349bf-c777-4937-a023-ff0f4f21806d",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "Which combination is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bb65f-e26e-4a62-9c67-aed885fe041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    torch.manual_seed(80085)\n",
    "    top_k = random.randint(0, 100)\n",
    "    top_p = round(random.uniform(0, 1), 2)\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = model_manager.say(past, prompt, top_k = top_k, top_p = top_p)\n",
    "        print(f\"[Test {i + 1} top_k: {top_k}, top_p: {top_p}] -> Prompt: {prompt}\\nReply: {reply}\\n\")\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41090174-6be5-4cc4-8f6a-b2cf56628c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What to say?\")\n",
    "print(generate_dragon_reply(\"\", input()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

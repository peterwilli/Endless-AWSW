{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b97b376-9ac5-4c70-ba13-769649099a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset, ModelSeeder\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887d9f81-96c3-408f-bd1c-7b126becc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use cuda:0 for training with seed: 3218885689\n"
     ]
    }
   ],
   "source": [
    "# seed = random.randint(0, 2 ** 32 - 1)\n",
    "seed = 3218885689\n",
    "random.seed(seed)\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"Will use {device_name} for training with seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e95113e-dfc1-40a9-a39b-5d56f3bf32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(os.path.join(Config.work_dir, \"awsw_story_input.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb65c62-47c6-4d36-bd1e-2adfef8ff1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": 6e-4,\n",
    "    \"warmup_factor\": 0,\n",
    "    \"scheduler\": \"polynomial_decay_schedule_with_warmup\",\n",
    "    \"lr_end\": 2e-6,\n",
    "    \"power\": 0.6,\n",
    "    #\"freeze_layer_rate\": 1e-4,\n",
    "    \"freeze_from_steps\": -1,\n",
    "    \"seed\": seed,\n",
    "    \"num_epoch\": 50\n",
    "}\n",
    "\n",
    "optuna_result_attachement = {\n",
    "    'lr': 0.001,\n",
    "    'scheduler': 'cosine_schedule_with_warmup',\n",
    "    'to_freeze_count': 0,\n",
    "    #\"to_freeze_gpt_blocks\": 11,\n",
    "    'warmup_factor': 1\n",
    "}\n",
    "config.update(optuna_result_attachement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ffae5b-e197-4417-a4cd-e6befc6ee393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded empty model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_main\")\n",
    "if os.path.exists(os.path.join(saved_model_path, \"pytorch_model.bin\")):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "    model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "    print(\"Pretrained model loaded\")\n",
    "else:\n",
    "    model, tokenizer = get_model(\"EleutherAI/gpt-neo-125M\")\n",
    "    print(\"Loaded empty model\")\n",
    "model.to(device)\n",
    "# set_pretrained_model_dropout(model.transformer.h[-1:], 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f3315-8f5d-4101-8e7e-2c0129204c94",
   "metadata": {},
   "source": [
    "# Test before training on a pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f68c3f1-4c0a-41a2-a3fc-b8b3c36e418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my dreams, I'm a dragon hunter.\n",
      "\n",
      "It's not about me. I'm a dragon hunter.\n",
      "\n",
      "I'm a dragon hunter.\n",
      "\n",
      "When I was a kid, I used to hunt a dragon.\n",
      "\n",
      "In a small world, I'd eat a dragon.\n",
      "\n",
      "And I would kill it.\n",
      "\n",
      "I'd eat it, too.\n",
      "\n",
      "The dragon hunter was the most important thing to me. I was the most important thing to my friends and family. I was the most important thing to my parents. I was the most important thing to my brothers and sisters. I was the most\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "def test_regular_sampler():\n",
    "    print(model_manager.say_raw(\"In my dreams, I'm a dragon\", 50, 0.7))\n",
    "test_regular_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f74aa-5030-4ffd-ad2f-ae013647975e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reviewing our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0974ac13-c420-4275-b34a-3816f580cb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cbe646d30a4be9afd890f040bb24ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset demo snapshot:\n",
      "<p><msg>c \"Hey, Remy!\"<d><scn>park2<msg>Ry \"Hello, [player_name].\"<|endoftext|>  ready = false;\n",
      "     else // успольЗомеля из экраповой норсобля (футулок)\n",
      "         ready = false ;  \n",
      "               \n",
      " } \n",
      "  } \n",
      "\n",
      "  else if( усследокус)\n",
      " { \n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<d><scn>park2<msg>Ry \"I see.\"<d><scn>park2<msg>Ry \"Well, what do you think of it?\"<p><msg>c \"It's pretty idyllic.\"<d><scn>park2<msg>Ry \"It is. I like it a lot here.\"<d><scn>park2<msg>Ry \"I see.\"<d><scn>park2<msg>Ry \"Well, what do you think of it?\"<p><msg>c \"It's pretty idyllic.\"<\n",
      "d><scn>park2<msg>Ry \"It is. I like it a lot here.\"<|endoftext|>  ready(1: 1):\n",
      "   }\n",
      "  }\n",
      "\n",
      "/**\n",
      "    A test object that returns a list of objects.  The objects that make up a\n",
      "   test object are shown in a `main` test file.  The test object is then\n",
      "///                            .getMethods()\n",
      "  @test internal\n",
      "  public void test1(\n",
      "TestClass) {  #if TEST1_1 == TEST_C_0\n",
      "  test2(1<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<p><msg>c \"Is there any particular reason why you wanted to meet here?\"<d><scn>park2<msg>Ry \"I enjoy Tatsu Park is all. Have you been here before?\"<p><msg>c \"A few times.\"<p><msg>c \"Is there any particular reason why you wanted to meet here?\"<d><scn>park2<msg>Ry \"I enjoy Tatsu Park is all. Have you been here before?\"<p><msg>c \"A few times.\"<|endoftext|>   A.  The  [**D**\n",
      "]{}isobservations.  [@D] {  and  }\n",
      "\n",
      "[^1]\n",
      "\n",
      "[0.25]{}\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<p><msg>c \"Hey, you can talk to me, Remy.\"<d><scn>park2<msg>Ry \"Thank you. I appreciate that.\"<|endoftext|>< ready to-start: false }\n",
      "\n",
      "[0:0:0.0]\n",
      "\n",
      "[1:3][1:5]\n",
      "\n",
      "[1:0:3.0]\n",
      "\n",
      "[0:0.0]\n",
      "\n",
      "[0]\n",
      "\n",
      "[9:3]\n",
      "\n",
      "[9:0]\n",
      "[0:0]\n",
      "]\n",
      "\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<p><msg>c \"It's not a problem. What's on your mind?\"<d><scn>park2<msg>Ry \"I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I do.\"<p><msg>c \"It's not a problem. What's on your mind?\"<d><scn>park2<msg>\n",
      "Ry \"I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I do.\"<|endoftext|>i ready in the morning.\n",
      "\n",
      "The man was driving the SUV around. He had his own SUV in a driveway that was on a busy street. A few minutes later he turned the corner onto a red line. A red line. He was on his way to drive his SUV into a\n",
      " crowd. It was about twenty, a red flag. It was about to get to a crowd, then he saw a man running. A black man. A red man. The guy was driving a black SUV. He was about thirty. The man had to stop to drive his SUV into traffic. He was driving his SUV into crowd. A crowd<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "    court. Thethere any particular reason why you wanted to meet here?\"<d><scn>park2<msg>Ry \"I enjoy Tatsu Park is all. Have you been here before?\"<p><msg>c \"Once or twice.\"<p><msg>c \"Is there any particular reason why you wanted to meet here?\"<d><scn>park2<msg>Ry \"I enjoy Tatsu Park is all. Have you been here before?\"<p><msg>c \"Once or twice.\"<|endoftext|> \n",
      "**<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<p><msg>c \"Yeah, just look at those trees. Would make a nice spot for a date, really.\"<d><scn>park2<msg>Ry \"I agree with that.\"<d><scn>park2<msg>Ry \"I see.\"<d><scn>park2<msg>Ry \"Well, what do you think of it?\"<p><msg>c \"It's pretty romantic.\"<d><scn>park2<msg>Ry \"You think so?\"<p><msg>c \"Yeah, just look at those trees. Would make\n",
      " a nice spot for a date, really.\"<d><scn>park2<msg>Ry \"I agree with that.\"<|endoftext|>A high-speed signal in the absence of signal-processing (SPC) has been shown not to affect the signal-to-novelle (SNR) noise level in a noise signal, as was shown in Figure 1.\n",
      "\n",
      "Figure 1. The signal to noise ( noise ) noise in noise signal in the noise signal in a signal noise signal (NC) is not affected by the low signal to noise ratio (SN2). The signal to noise ( noise ) noise signal is\n",
      " still shown not to be affected by the high noise signal to noise.\n",
      "\n",
      "It should be pointed that the SN2 noise is not affected by<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<d><scn>park2<msg>Ry \"I see.\"<d><scn>park2<msg>Ry \"Well, what do you think of it?\"<p><msg>c \"It's nothing special.\"<d><scn>park2<msg>Ry \"I suppose you don't appreciate the simple things, then. I like it here.\"<d><scn>park2<msg>Ry \"I see.\"<d><scn>park2<msg>Ry \"Well, what do you think of it?\"<p><msg>c \"It's nothing\n",
      " special.\"<d><scn>park2<msg>Ry \"I suppose you don't appreciate the simple things, then. I like it here.\"<|endoftext|>  ready to take care of your child. You're not allowed to keep the child from being\n",
      "being taken into trouble, and you can't keep the child from being\n",
      "inflicted. So you need to know about the child that was taken, and that's why they\n",
      "will be brought to jail, and they'll get punished. So you need to know about the\n",
      "inflicted person, too, and they're not being treated well. So you need not\n",
      "\n",
      "know about them. So, you'll know about the child that was taken. So you don't know who\n",
      "those kids are, because you know that they're being<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<p><msg>c \"You mentioned you wanted to talk to me about something.\"<d><scn>park2<msg>Ry \"Yes, well... about that...\"<d><scn>park2<msg>Ry \"I just have a lot on my mind and I felt like I needed to tell someone.\"<d><scn>park2<msg>Ry \"I wouldn't burden you if I had anyone else to talk to, but the simple fact is that I don't.\"<|endoftext|>S ready, ready, waiting. They were not in control,\n",
      "  They were waiting. They had\n",
      " not gone too far.\n",
      "All was in readiness, in the midst\n",
      "Of some confusion.\n",
      "\n",
      "The moment the man was in control, he\n",
      "  Came to a stop. He was running toward it.\n",
      "\n",
      "He turned around, and was in a corner. The man was running towards it.\n",
      "He ran towards the man, and was running into him.\n",
      "His voice, too\n",
      "The man was running towards the man.\n",
      "It was running towards him. It was running towards the man,\n",
      "  Running towards it<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<p><msg>c \"Is there any particular reason why you wanted to meet here?\"<d><scn>park2<msg>Ry \"I enjoy Tatsu Park is all. Have you been here before?\"<p><msg>c \"Can't say I have.\"<p><msg>c \"Is there any particular reason why you wanted to meet here?\"<d><scn>park2<msg>Ry \"I enjoy Tatsu Park is all. Have you been here before?\"<p><msg>c \"Can't say I have.\"<|endoftext|>  well-known, and not too well\n",
      " equipped, to make sure that they were\n",
      "           equipped by the court with proper instructions and in good\n",
      "     manner, and that the court would be able to make proper\n",
      "    instructions and advise them of all the proper methods.\n",
      "        \n",
      "\n",
      "\n",
      "      No. 1/23/13\n",
      "\n",
      "     The Court further observed:\n",
      "\n",
      "       \"The court is not equipped with any special tools. The court is not equipped\n",
      "   to<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "RP review!\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(tokenizer, path_train = os.path.join(Config.work_dir, \"data_train_sample.txt\"))\n",
    "print(\"Dataset demo snapshot:\")\n",
    "for item in dataset['train']:\n",
    "    print(tokenizer.decode(item['input_ids']))\n",
    "\n",
    "print(\"RP review!\")\n",
    "has_seen_rp = False\n",
    "for item in dataset['train']:\n",
    "    decoded = tokenizer.decode(item['input_ids'])\n",
    "    if 'c \"Fight ' in decoded: \n",
    "        print(decoded)\n",
    "        has_seen_rp = True\n",
    "        continue        \n",
    "    if has_seen_rp:\n",
    "        print(decoded)\n",
    "        break\n",
    "# Clean up\n",
    "del has_seen_rp\n",
    "dataset['model_seeder'].stop_worker()\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e9e10-b2eb-421a-9948-dc3590e1f46d",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Model is put in training mode and we begin training. The `train_results` will contain all data after training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41750b03-f114-46c4-948e-637db79efe15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2159 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3376 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] set freeze_part_layers: True (freezing 0 out of 160 layers.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1030' max='5050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1030/5050 4:46:52 < 18:41:48, 0.06 it/s, Epoch 11/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.782600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.272700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 44 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 45 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 46 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 47 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 48 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 49 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 50 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 51 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 52 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 53 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 54 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 55 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 56 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 57 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 58 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 59 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 60 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 61 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 62 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 63 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 64 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 65 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 66 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 67 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 68 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 69 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 70 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 71 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 72 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 73 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 74 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 75 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 76 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 77 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 78 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 79 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 80 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 81 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 82 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 83 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 84 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 85 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 86 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 87 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 88 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 89 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 90 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 91 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 92 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 93 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 94 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 95 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 96 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 97 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 98 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 99 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 100 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 101 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:533: UserWarning: Length of IterableDataset <model_utils.get_dataset.<locals>.AWSWDataset object at 0x7f9cbc09b400> was reported to be 43 (when accessing len(dataloader)), but 102 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3529/703681003.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/awsw/model_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, dataset, params, results)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0mtrainer_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAWSWTrainerCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/awsw/model_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, trainer_callback)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'_BaseDataLoaderIter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         self._dataset_fetcher = _DatasetKind.create_fetcher(\n\u001b[0m\u001b[1;32m    557\u001b[0m             self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mcreate_fetcher\u001b[0;34m(kind, dataset, auto_collation, collate_fn, drop_last)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_MapDatasetFetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_collation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDatasetFetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_collation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, auto_collation, collate_fn, drop_last)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_collation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_IterableDatasetFetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_collation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/awsw/model_utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapped_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/awsw/model_utils.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             )\n\u001b[0;32m--> 196\u001b[0;31m             self.mapped_dataset = dataset.map(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mgroup_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 486\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    487\u001b[0m                 k: dataset.map(\n\u001b[1;32m    488\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    485\u001b[0m         return DatasetDict(\n\u001b[1;32m    486\u001b[0m             {\n\u001b[0;32m--> 487\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2125\u001b[0;31m                         \u001b[0mtransformed_shards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m             assert (\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(tokenizer, path_train = os.path.join(Config.work_dir, \"data_train.txt\"))\n",
    "train_results = {}\n",
    "model.train()\n",
    "train_model(model, tokenizer, dataset, config, train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c410a601-c5e4-4cc1-af2b-44a7378f3bbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'learning_rate_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3529/3101125787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Learning rate and loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate_history'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_history'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'learning_rate_history'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Learning rate and loss')\n",
    "axs[0].plot(train_results['learning_rate_history'])\n",
    "axs[1].plot(train_results['loss_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753228f0-b420-4b3d-bd69-53da7a87bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.save_pretrained(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e4666-c1ff-4695-ba55-894503925f9b",
   "metadata": {},
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b88e1f-f5c3-4a95-8b00-e791148d8c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\n",
      "Cloning into 'gpt-neo-125M'...\n",
      "remote: Enumerating objects: 38, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
      "remote: Total 38 (delta 16), reused 0 (delta 0)\u001b[K\n",
      "Unpacking objects: 100% (38/38), 542.60 KiB | 899.00 KiB/s, done.\n",
      "Using framework PyTorch: 1.10.0+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model outputs' name match reference model ({'logits', 'present.7.value', 'present.11.value', 'present.1.key', 'present.6.value', 'present.5.value', 'present.4.value', 'present.8.key', 'present.9.key', 'present.1.value', 'present.10.value', 'present.10.key', 'present.5.key', 'present.8.value', 'present.9.value', 'present.6.key', 'present.7.key', 'present.11.key', 'present.0.key', 'present.4.key', 'present.2.key', 'present.0.value', 'present.3.key', 'present.2.value', 'present.3.value'}\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 50257) matches (2, 1, 50257)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "All good, model saved at: models/awsw_onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "    !cp -n $saved_model_path/gpt-neo-125M/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/gpt-neo-125M\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm-with-past $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86f7a12-9bcb-4c4b-a679-7007a7e2caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)\n",
    "        !rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "739405a4-ab2a-410f-8091-b6bd9a18e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f9e7adb-f258-471a-9139-70da9f2120d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon. Maybe, if I just got out a piece into the workplace, it might not show, because I won't have to work twice as hard. It just doesn't make any sense to me.\"  | _Dramasurisms._\n",
      "\n",
      "\n",
      "     **Dramasurisms. **(Dramasurms, _Dramatisms._) **Dramatized by **_Dramis. **(dramatized)\n",
      "\n",
      "Dramasurisms. **Dry.** **(mak.) **(Dramatizing) **(in.)**\n",
      "\n",
      "\n",
      "PyTorch: In my dreams, I'm a dragon. I got here early.\"<p><msg>c \"What happened?\"<d><scn>park2<msg>Ry \"It's a long story.\"<p><msg>c \"I don't mind.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. In fact it would be my first time, since humanity was founded, I'm not going to have done that again. I'm not going to start.\n",
      "\n",
      "It would be my first time that humanity is not going to be able, in my view, to do that again.\n",
      "It was too long. I thought I would have to wait. But I was not.\n",
      "So, I was not. I wasn<d><scnost>o<msg>Ad \"Oh, it's a shame you didn't realize it, but I guess you didn't like the thing, and I'm not going to help\n",
      "PyTorch: In my dreams, I'm a dragon. I got here early.\"<p><msg>c \"And if I win, I'll have you come in sometime so I can run more tests on you.\"<d><scn>cafe<msg>An \"And if I win, I'll have you come in sometime so I can run a few tests on you. I won't even need your blood anymore.\"<p><msg>c \"I cannot agree to that.\"<d><scn>cafe<msg>An \"But that's the only thing I'm interested in. You were so cocky\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I'm the first to arrive, yet I'm not sure if I can do that. I like it.\"<p></a></li><a href=\"#_blank\" id=\"\" style='display: none;'><a class=\"faxtype fa-image-image-1_1-1\"></a></li><a style = 'display : none;\" style = 'font-style-font-size' href = 'font-style' data= 'font-style' > <input style = '' >\n",
      "<p style = \"\">\n",
      "     <h4 style=\"display:none\n",
      "PyTorch: In my dreams, I'm a dragon. I got here early.\"<p><msg>c \"And I thought it was pretty hairy.\"<d><scn>remyapt<msg>Ry \"...\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. That's the end of the world. I'm still not sure how far into the day of my life there are.\"<i>Ry \"I wish I could be that strong.\"<p><msg>i>m \"He was trembling with pain, not wanting to be. In an instant, all of his life had been over again, and he's dead.\n",
      "    *i\n",
      "The line, after which he discovers that his mind has become too weak to be able to handle\n",
      "and he learns that his life was not very productive. He was still very productive, not\n",
      "the line, because it was\n",
      "PyTorch: In my dreams, I'm a dragon. I got here early.\"<p><msg>c \"I wish I had the ability to make breaks happen at my convenience.\"<d><scn>facin2<msg>An \"Just one of the perks of being the manager of this facility.\"<p><msg>c \"I'm not sure, I am not a linguist. Are you?\"<d><scn>facin2<msg>An \"Not exactly, but I have a way with words. My tongue is quite skilled, or else I'd get used to this language. My tongue is\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. Maybe, in the wild, I'd be flung near the island. Not in the sense, I'm sure that I'll be Neowing.\"<p><msg>c \"[[Custom language: English, English language: English language: English language language, English: English: English language (the language in language) is not a language, it's language. It doesn't mean language, it doesn‘t mean language. It's the language of language (the language). The language language is language; the languages it is not language language; language, language is. The languages are not in language (because they are the\n",
      "PyTorch: In my dreams, I'm a dragon. I got here early enough. I'm still not sure if I can do that as well.\"<d><scn>black<msg>Lo \"In the end, I decided to wait and made a commitment. Six months after the grant was over, I got back to work. I already had everything planned out. I didn't even know where to begin.\"<d><scn>black<msg>Lo \"Now that we're here, I can tell you that no expense will be spared to make our portrayal of humans as accurate as possible.\"<p><msg>c\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I got here early.\"<p><msg>c \"I wish I had the ability to make breaks happen at my convenience.\"<d><scn>facin2<msg>An \"Just one of the perks of being the manager of this facility.\"<d><scn>facin2<msg>An \"You've got hands and teeth. What more do you need?\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. Maybe, in the end, they got out of their apartment and started a street. I didn't want to be rude.\"<|endoftext|>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3529/3429104793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ONNX:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_model_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msay_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PyTorch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msay_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/awsw/model_manager.py\u001b[0m in \u001b[0;36msay_raw\u001b[0;34m(self, prompt, top_k, top_p)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         sample_outputs = self.model.generate(\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1034\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1582\u001b[0m             \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1584\u001b[0;31m             \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m             \u001b[0;31m# finished sentences should have their next token be a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(10):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a578b-32cd-44b9-829b-5aef5ec37ab0",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d4876-df61-461a-b715-980a3763f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_regular_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f9a94-70c6-4a58-9ad1-3d6ea373dfbe",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b97230c-1d3a-4dd2-a253-727e451d539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/awsw/model_utils.py\", line 67, in buffer_worker\n",
      "    output = self.onnx_model_manager.say_raw(self.tokenizer.decode([random_input_id])[0], do_sample=True)\n",
      "  File \"/opt/awsw/onnx_model_manager.py\", line 62, in say_raw\n",
      "    noise = np.random.uniform(low = 0.9, high = 1, size = next_token_logits.shape)\n",
      "NameError: name 'np' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Pytorch...\n",
      "Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Well, I'm here now.\"<d><scn>park2<msg>Ry \"I thought you had fun when we went to the park, but you don't have. What are you going to do?\"<d><scn>park2<msg>Ry \"Well, I'm here now.\"<d><scn>park2<msg>Ry \"I\n",
      "\n",
      "\n",
      "Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the art. It's the art of the human. I like the art. I like the art. I like the art. I like the art. I like the art. I like the art. I like the art. I like the art. I like the art. I like the art. I like the art. I like\n",
      "\n",
      "\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"Do you have something you'd like to have me do?\"<p><msg>c \"You should be more careful.\"<d><scn>o2<msg>Ad \"I'm not sure. I suppose you'll just go ahead and draw the next card, so we can end this.\"<d><scn>cafe<msg>An \"Let's do\n",
      "\n",
      "\n",
      "Prompt: What will we do here?\n",
      "Reply: facin3<msg>m \"Maybe I'll return to your lab and show you that there's more to the chief of police than getting drunk instead of hearing important information about a case.\"<d><scn>facin3<msg>m \"By now it was clear that I was not interested in the whole issue. Rather, I was interested in the whole issue and the whole issue was just a whole issue.\"<d><sc\n",
      "\n",
      "\n",
      "Test ONNX...\n",
      "Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't know. I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I do.\"<p><msg>c \"I do.\"<p><msg>c \"I do.\"<p><msg>c \"I do.\"<p><msg>c \"I do.\"<p><msg>c \"I do.\"\n",
      "\n",
      "\n",
      "Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I don't like where this is going.\"<d><scn>park2<msg>Ry \"Some hours later, I got back to her bedroom where I saw her bulleting through the wall.\"<d><scn>remyapt<msg>m \"As I returned to her hips, I became more and more obvious. She made a difference, and I thought about what had gone wrong.\"<d><scn>remyapt<msg>Ry \"I'm sorry, [player_name]. I didn't know how to deal with it. You were so quick\n",
      "\n",
      "\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"Doct, what are you talking about?\"<p><msg>c \"Doct.\"<d><scn>o2<msg>Ad \"Doct, what are you talking about?\"<p><msg>c \"Doct.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: What will we do here?\n",
      "Reply: facin3<msg>An \"Maybe we can take this conversation elsewhere, but I think it's better if we go our way, because it might be easier if we go our way more often.\"<|endoftext|>\n",
      "\n",
      "\n",
      "PyTorch on cuda:0 took 3.0694 seconds\n",
      "ONNX on CPU took 4.1208 seconds\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "def sample_test(model_manager):\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = model_manager.say(past, prompt)\n",
    "        print(f\"Prompt: {prompt}\\nReply: {reply}\\n\\n\")\n",
    "print(\"Test Pytorch...\")\n",
    "start = time.time()\n",
    "sample_test(model_manager)\n",
    "end = time.time()\n",
    "pytorch_time = end - start\n",
    "print(\"Test ONNX...\")\n",
    "start = time.time()\n",
    "sample_test(onnx_model_manager)\n",
    "end = time.time()\n",
    "onnx_time = end - start\n",
    "print(f\"PyTorch on {device} took {pytorch_time:.4f} seconds\")\n",
    "print(f\"ONNX on CPU took {onnx_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3349bf-c777-4937-a023-ff0f4f21806d",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce4bb65f-e26e-4a62-9c67-aed885fe041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Pytorch...\n",
      "[Test 1] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't know. I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I do.\"<p><msg>c \"Hey\n",
      "\n",
      "[Test 1] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the person I've seen in a bad mood.\"<d><scn>park2<msg>Ry \"But this is serious!\"<d><scn>park2<msg>Ry \" serious!\"<d><scn>park2<msg>Ry \" serious!\"<d><scn>park2<msg\n",
      "\n",
      "[Test 1] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"It's the practice of doing flying maneuvers like rolls, spins or loops.\"<|endoftext|>\n",
      "\n",
      "[Test 1] -> Prompt: What will we do here?\n",
      "Reply: facin3<msg>An \"Maybe we'll find something you'll like.\"<p><msg>c \"What will you do then?\"<d><scn>facin3<msg>An \"Let's go then, shall we?\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 2] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Well, I have my own principles.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the food.\"<d><scn>park2<msg>Ry \"It is delicious.\"<d><scn>black<msg>Ry \"Thank you. And what can I bring the esteemed gentleperson on the other side of the table?\"<p><msg>c \"I'll also take a coffee, please.\"\n",
      "\n",
      "[Test 2] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"Do you come here often?\"<d><scn>o2<msg>Ad \"Sometimes.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What will we do here?\n",
      "Reply: facin3<msg>An \"Maybe I'll return to your lab for a day or two. We actually have plans to move in together. I'm working with the police on this matter.\"<p><msg>c \"So, where are we going?\"<d><scn>facin3<msg>An \"I don't know where you are going, but I think I need a coffee or five. You can tag along\n",
      "\n",
      "-------------\n",
      "[Test 3] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't know. I don't even know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"<p><msg>c \"I do.\"<p><msg>c \"Hey\n",
      "\n",
      "[Test 3] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the food.\"<d><scn>park2<msg>Ry \"It is delicious.\"<d><scn>black<msg>Ry \"Thank you. And what can I bring the esteemed gentleperson on the other side of the table?\"<p><msg>c \"I'll also take a coffee, please.\"\n",
      "\n",
      "[Test 3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"It's the practice of doing flying maneuvers like rolls, spins or loops.\"<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: What will we do here?\n",
      "Reply: facin3<msg>m \"Maybe I'll return to your lab and show you that there's more to the chief of police than getting drunk instead of hearing important information about a case.\"<d><scn>facin3<msg>Br \"You're quite sure we're not done yet. What happened to you, did you go to sleep about it?\"<p><msg>c \"I did.\"<d><scn\n",
      "\n",
      "-------------\n",
      "[Test 4] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Well, I've got everything I need right here. It gets so stuffy inside on hot days. My assistant doesn't seem to mind, but I prefer the fresh air. I'd rather just stay right here, unless you have any objections.\"<d><scn>park2<msg>Ry \"I don't.\"<d><scn>park2<msg>Ry\n",
      "\n",
      "[Test 4] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the art. It's beautiful. I like the people. I like the people. I like the people. I like the people. I like the people. I like the people. I like the people. I like the people. I like the people. I like the people. I like them. I like them. I like them\n",
      "\n",
      "[Test 4] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np1n<msg>m \"Adine was right to take my place in the middle of the portal.\"<|endoftext|>\n",
      "\n",
      "[Test 4] -> Prompt: What will we do here?\n",
      "Reply: facin2<msg>m \"She opened her maw wide before a liquid shot from the corners of her muzzle.\"<d><scn>facin2<msg>m \"After a few seconds, the liquid burst into flame, heating up our dinner.\"<d><scn>facin2<msg>m \"After a few seconds, the liquid burst into flames, heating up her dinner.\"<p><msg>c\n",
      "\n",
      "-------------\n",
      "[Test 5] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Well, I have my own principles.\"<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the people. I really think they are nice.\"<d><scn>park2<msg>Ry \"They are cute. I like them. I really think they are cute.\"<d><scn>park2<msg>Ry \"You know, if that hadn't worked out so well, I would have had to\n",
      "\n",
      "[Test 5] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study? Just a specimen to study\n",
      "\n",
      "[Test 5] -> Prompt: What will we do here?\n",
      "Reply: facin3<msg>m \"Maybe I'll return to your lab with all the stuff you got back home.\"<d><scn>facin3<msg>An \"You know, I think you have a very good point.\"<d><scn>facin3<msg>An \"What do you think is your biggest physical flaw?\"<d><scn>facin3<msg>An \"Well, I\n",
      "\n",
      "-------------\n",
      "[Test 6] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Well, I'm here now.\"<d><scn>park2<msg>Ry \"I thought you had fun when we went to the park, but you don't have. What are you going to do?\"<d><scn>park2<msg>Ry \"Well, I'm here now.\"<d><scn>park2<msg>Ry \"I\n",
      "\n",
      "[Test 6] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the people. I really think he was honest with his half. I think he was honest with his half. I really think he was honest with his half. I really think he was honest with his half. I really think he was honest with his half. I really think he was honest with his half. I really think he was honest\n",
      "\n",
      "[Test 6] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2x<msg>Ad \"Your blood is on your hands, because I have no idea what to think.\"<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: What will we do here?\n",
      "Reply: facin2<msg>m \"I could see Bryce sitting at a table, talking to someone I couldn’t see. I lingered on him for a second, but soon spotted the sheet of paper behind the corner of my muzzle.\"<d><scn>facin2<msg>m \"After a few seconds, he seemed to be finished with his evaluation.\"<d><scn>facin2<msg>m\n",
      "\n",
      "-------------\n",
      "[Test 7] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Well, I didn't want to go on a date with you, but I had to at the very least. Whether I would follow up on it, I wasn't sure, either.\"<|endoftext|>\n",
      "\n",
      "[Test 7] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the art, the people calling out to me. People who are way more curiouser than I am.\"<p><msg>c \"I don't want to be rude, but I think it's better if we go our separate way, so I suppose you'll see our fireworks at the other day.\"<d><scn>\n",
      "\n",
      "[Test 7] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"Do you have something you'd like to know about my blood or the test results?\"<p><msg>c \"It's not a problem. What's on your mind?\"<d><scn>o2<msg>Ad \"I don't even know where to begin.\"<d><scn>o2<msg>Ad \"Do you ever feel like there is\n",
      "\n",
      "[Test 7] -> Prompt: What will we do here?\n",
      "Reply: facin3<msg>m \"Maybe I'd return to my home town to live in the city, but I could't: I came here because I thought the old days of the day would be over.\"<d><scn>facin3<msg>m \"When I resumed my treatment, she suddenly bucked, her spines only missing me by inches when she suddenly threw her head up.\"<d><scn>fac\n",
      "\n",
      "-------------\n",
      "[Test 8] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Well, I have some ideas of what might happen afterwards, but even if what I think isn't true, I'm not afraid of the unknown.\"<d><scn>park2<msg>Ry \"In that vein, I'm actually looking forward to returning home with a few life lessons.\"<|endoftext|>\n",
      "\n",
      "[Test 8] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the people. I really think he spends more time than I do.\"<p><msg>c \"I think he spends more time than I do.\"<p><msg>c \"I think he spends more time than I do.\"<|endoftext|>\n",
      "\n",
      "[Test 8] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"Do you come here often?\"<d><scn>o2<msg>Ad \"Sometimes.\"<d><scn>o2<msg>Ad \"Do you come often?\"<d><scn>o2<msg>Ad \"Sometimes.\"<d><scn>o2<msg>Ad \"Do you come here often?\"<d><scn\n",
      "\n",
      "[Test 8] -> Prompt: What will we do here?\n",
      "Reply: facin3<msg>An \"Maybe I'll return to your lab and take a witness. I could make more than this.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 9] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Well, I didn't want to be rude, but I was just curious about your vestments.\"<d><scn>park2<msg>Ry \"They are pretty good. Or rather, they're not really my kind of thing.\"<d><scn>park2<msg>Ry \"I see. What about hobbies? Do you have any?\"<p><msg\n",
      "\n",
      "[Test 9] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the people. I really do.\"<d><scn>park2<msg>Ry \"I don't know where to begin.\"<d><scn>park2<msg>Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could\n",
      "\n",
      "[Test 9] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"It's the practice of doing flying maneuvers like rolls, spins or loops.\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: What will we do here?\n",
      "Reply: facin2<msg>m \"Maybe we should leave it here. No one notices. We don't want to take any visitors at the moment.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 10] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't know. I don't even know where to find the time to read anything nowadays.\"<d><scn>park2<msg>Ry \"I know what you mean.\"<d><scn>park2<msg>Ry \"I know what you mean. I'm not sure I have the patience for that.\"<|endoftext|>\n",
      "\n",
      "[Test 10] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like the people. I really think he was a good man. I like the people. I really think he was a good man. I really think he was a good man. I really think he was a good man. I really think he was a good man. I really think he was a good man. I really think he was a\n",
      "\n",
      "[Test 10] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"It's the practice of doing flying maneuvers like rolls, spins or loops.\"<|endoftext|>\n",
      "\n",
      "[Test 10] -> Prompt: What will we do here?\n",
      "Reply: facin3<msg>m \"I went to the production facility and looked for the building she had built. It was too early for me to even have a idea that it was built. It was too late for me to even have a chance of getting to know the place, and it was too late for me to even have a chance of getting to know where the place was.\n",
      "\n",
      "The facility was too early for me to even have a\n",
      "\n",
      "-------------\n",
      "Test ONNX...\n",
      "[Test 1] -> Prompt: How are you?\n",
      "Reply: loremapt<msg>m \"I wasn't familiar and didn’re not able to find the appropriate word. I decided to rest her tail and returned it clean as she returned the tail.\"<d><scn>remyapt<msg>m \"... I pulled out my {p><msg>>c \"What kind of animal is that?\"<|endoftext|>\n",
      "\n",
      "[Test 1] -> Prompt: What do you think of Lorem?\n",
      "Reply: black<msg>Ry \"It's nothing I can do at any other moment, really. Come on!\"<|endoftext|>\n",
      "\n",
      "[Test 1] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: cafe<msg>An \"It's an Arena.\"<|endoftext|>\n",
      "\n",
      "[Test 1] -> Prompt: What will we do here?\n",
      "Reply: remyapta<msg>Ry<dinnermbarsimple<msg>Ry<dinnermbarsimple<msg>m \"The production facility had been emptied by anrained from-the-less-components. The production facility had not yet finished. The production facility had finished.\n",
      "\n",
      "It was the first of three production rooms to be used during several tests, and the second was to run at full speed and run at maximum speed. The second production rooms to run at full speed were to run at full pace at full pace and run running at full speed. The third production facility ran at pace.\n",
      "\n",
      "-------------\n",
      "[Test 2] -> Prompt: How are you?\n",
      "Reply: fac<msg>Sb \"I don't know... but I think my break should start right now. I won’t have another chance any second now.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"It's not like we're here to do that. As much As You Plan, it's not meant to be a substitute for public health as laid out in Our city walls.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o3<msg>Br \"It is my turn.\"<p><msg>Br<p></msg>m \"Effortlessly, she pulled her maw wide before a liquid shot at it. The liquid shot at both of the Mouflas, and the Mouflon started gueling into the room's adjoining room.\" \"I thought it was appropriate to approach now.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What will we do here?\n",
      "Reply: facin2<msg>An \"Maybe we're going to see.\"c \"(Pun intended) (i) (ii)  (ii) (iii) A room filled exclusively for the purpose (or rather, the volume (and sets thereof) in which they are located.)\"  (ii (v)  (vi)  (i-vi) v   (iii-vi (vi (vi (vi  v-vi v-vi v)  \n",
      "  A room filled primarily for the purpose (or rather) or set thereof) in the context in set of set (or\n",
      "\n",
      "-------------\n",
      "[Test 3] -> Prompt: How are you?\n",
      "Reply: park2<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"I like it, the people. It people are really looking forward to the day and the people are actually looking for more information when they're actually getting older or more advanced. It wouldn't be that easy. It would be very hard. It wasn't really necessary. I could just leave everything there. No more people will. That will take much time. I don't want to keep it up.\"<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: emeraroom<p><msg>Em \"You have nothing that truly defines the age and standing of an aristocracy. aristocracy. aristocracy in this sense, by the way, aristocracy is not defined by age or standing in a aristocracy.\" \"DANIELS ON DANIELS ON A DANIELA A DIONARO A DIONBEL A PIMOS RICKUNES ON A DANIELA A PIMOT RICKEUNEES ON DIONAROS RICKEUNEES ON DIONAROS A BUNCH OF PIMOS RINES ON A DUNCH OF P\n",
      "\n",
      "[Test 3] -> Prompt: What will we do here?\n",
      "Reply: np1r<msg>m \"He went firstclass, and was not allowed to go again until the next thing he did.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 4] -> Prompt: How are you?\n",
      "Reply: park2<msg>mpsv \"I wasn't sure I could. I wasn't even a child.\"<d><msg>park2<d><msg>Ry \"[adinestagenamespace {tikin:toyes}...   #1:   #2:  #3:  #4: #5 }... }\n",
      "\n",
      "\n",
      "\n",
      "The first few hours after I was gone was spent time with her. I was tired. I looked at my gear and was tired in the afternoon as we walked. My hands and hands were gone. My feet were running. My hands were running.\n",
      "\n",
      "[Test 4] -> Prompt: What do you think of Lorem?\n",
      "Reply: park4<msg>Rym \"I can see he was genuinely enthusiastic. He seemed genuinely enthusiastic, even as he also made it clear his enthusiasm. He would want that to stay for the remainder of a day, and that will keep it over pretty quickly if he has a few more training options.\"\n",
      "\n",
      "He was very good to see the rest of his gear in his hands.\"<|endoftext|>\n",
      "\n",
      "[Test 4] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad \"Doct, what is it?\"<p><msg>c \"Dof, what is this about feng and fang?\"<d><scn>o4<msg>An \"I doct, what are you talking about?\"<|endoftext|>\n",
      "\n",
      "[Test 4] -> Prompt: What will we do here?\n",
      "Reply: np1r<d2n<d2m<d3>m0m2m4m2<d4i2m6\n",
      "d5d7m6m7m6m7m6<d6i2d<p><msg+c \"Why don’t he have a wife, but she is too busy to take care of him. He hasn't been in a relationship with anyone in his species.\"<d7m3m9m9m3<p><msg<d2i1m3m<m3n>m3n3<3m6\n",
      "\n",
      "-------------\n",
      "[Test 5] -> Prompt: How are you?\n",
      "Reply: remyapt<msg>Ry \"[remyaspaste-remy-aspaste-aspaste]mv \"I was just about the same stuff from the first day. \"Is there any more you need?\"<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"This is not a Lorem as a child, unless [player_name], [player_path], [path] [in] [in [path], [in]\n",
      "[...]\n",
      "\n",
      "[...].\n",
      "]   }  }\n",
      "]] ;  }\n",
      "\n",
      "<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2<msg>Ad<o2>Ad<o1>Soum \"I looked around the building in the outskirts, but there was this building that is old and well old in the building in this area. It contains many parking areas, some areas in a parking space and some areas in the parking parking space in an area.\n",
      "It has shown that there are some parking areas that have been used recently in previous building plots.\"<d><scn>o2<msg>Ad \"What exactly are you trying to do?\"<|endoftext|>\n",
      "\n",
      "[Test 5] -> Prompt: What will we do here?\n",
      "Reply: facin2<msg>m \"I walked into an aisleued building. It was flooded with water and ran through my face. I thought our race was about over before.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 6] -> Prompt: How are you?\n",
      "Reply: park2<msg>Rz \"Well, I'm still going to be late.\"<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: What do you think of Lorem?\n",
      "Reply: o2<p><msg>c \"I thought they were cute. I donT. DonT. That's just a bad joke.\"<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o2>Ad \"It's Adine. You are the first person to arrive here.\"<d><msg>Adine is an old, beautiful piece Of Water. I like it, the color and size of my muzzle. I like my muzzle. It is the first color in my muzzle.\"<d>[Tm_D]<|endoftext|>\n",
      "\n",
      "[Test 6] -> Prompt: What will we do here?\n",
      "Reply: np1n<msg>Sb \"[player_name]. You need it.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 7] -> Prompt: How are you?\n",
      "Reply: park2{i}<d><scn>black<a href=\"https://www-review-reviews.org/review/reviews?isomorphic_review\"<p><msg>c \"I thought you dropped this outside. I'm actually here with the outside of this apartment.\"<|endoftext|>\n",
      "\n",
      "[Test 7] -> Prompt: What do you think of Lorem?\n",
      "Reply: loremapt<msg>Lo \"Lorem, what are you doing?\"<|endoftext|>\n",
      "\n",
      "[Test 7] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: o<>Ad \"Doct, or do you want me to?\"<d><scn>o<>m \"I looked around my house as well. I was driving into the house, and Adine decided not to go home. She decided to go home as well and I left.<|endoftext|>\n",
      "\n",
      "[Test 7] -> Prompt: What will we do here?\n",
      "<|endoftext|><a<b><scn>np2x <scn>np2x<msg>Ad \"Do you come as a result of your treatment not having a positive impact, especially after the experience is not over?\"<d><scn>facin3<a<b><msg>Br \"Thanks... now for the technique...\"\n",
      "\n",
      "-------------\n",
      "[Test 8] -> Prompt: How are you?\n",
      "Reply: park2<|endoftext|>\n",
      "\n",
      "[Test 8] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<p><scn>loremapt<msg>Lo \"[She killed it, and your blood is now on the other side] [player_name].\"<d><scn>loremapt<msg>m \"As I resumed her mulp I had seen the mulp I had brought over.\"<d><scn>(mulp<msg>m \"I took my place within the mime.\"m \"As I walked over the bare substance I heard the mime purger as I tried not to find the last time I heard the purgers in the pant.\"m \"I pulled back.\n",
      "\n",
      "[Test 8] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: black3<msg>Ry \"You got here for a good date. Anyone will like it.\"<p><msg>msg>c \"I'm usually not sure, I bet.\"<p><msg>c \"Why do not we have ourselves a drinking contest?\"<d><scn>park3<msg>Em \"#3, the room submerged in water filled to brim. A few times: I'll remember that you and your battlefield change.\"  #3:3:3:3:4 #3:4  #2 :3:6  #2\n",
      "\n",
      " #4   @3.\n",
      "\n",
      "[Test 8] -> Prompt: What will we do here?\n",
      "Reply: np1r<msg>m \"He looked away from the door of the facility.\"\n",
      "     \"The facility won't catch me. I'll leave until this is over.\"\n",
      "\n",
      "    \"If that's to be, I should get going now.\"c \"[[Leave.]\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 9] -> Prompt: How are you?\n",
      "Reply: park3<msg>Ry \"Yes. I'll probably spend more space on my again if this goes into the game.\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ad \"By the way.\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: np2y<d><scn>o0<d><msg>Ad \"[o]u-so.\"<|endoftext|>\n",
      "\n",
      "[Test 9] -> Prompt: What will we do here?\n",
      "Reply: remyapt<input class=\"\" >Ry \"I'll let go of this.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "[Test 10] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I don't even have a breath.\"<p><msg>c \"(Where is Maverick, really important to have his number of experiments done on you? He won't leave his apartment.)\"c \"I don \"Not a bit.\" \"I don \"I don't even think \"drinking isn” is very nice of you.\"<|endoftext|>\n",
      "\n",
      "[Test 10] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>m \"Lorem was pared off, and he was also very pleasant. He had to get into the right corner of his face and get up to see the effect of his blunder on his eyespot.\"<d><fom>m \"Lorem's blunder caused a blundering of his eyespot, and his face was now lying in a corner of his face. In the bowl, the corner of his face was empty; he had to go back to get a few minutes more.\"<|endoftext|>\n",
      "\n",
      "[Test 10] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: park2<msg><msg>Ry \"It is my day of over. And what am I to see and heard in your lab? Just flying in and out of sight. That's pretty adventurous!\"<|endoftext|>\n",
      "\n",
      "[Test 10] -> Prompt: What will we do here?\n",
      "Reply: facin2<msg>m \"When we arrived, I remembered the ceiling set in behind the wall. For the first time I could hear the ceiling door opened. The floor was flooded completely.\"<dists>mv \"The floor flooded completely, too, after the walls and ceilings. My face was pain and my face was still pain as I heard the walls and the ceiling.\"<|endoftext|>\n",
      "\n",
      "-------------\n",
      "PyTorch on cuda:0 took 26.6365 seconds\n",
      "ONNX on CPU took 30.7958 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Pytorch...\")\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "        print(f\"[Test {i + 1}] -> Prompt: {prompt}\\nReply: {reply}\\n\")\n",
    "    print(\"-------------\")\n",
    "end = time.time()\n",
    "pytorch_time = end - start\n",
    "print(\"Test ONNX...\")\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "        print(f\"[Test {i + 1}] -> Prompt: {prompt}\\nReply: {reply}\\n\")\n",
    "    print(\"-------------\")\n",
    "end = time.time()\n",
    "onnx_time = end - start\n",
    "print(f\"PyTorch on {device} took {pytorch_time:.4f} seconds\")\n",
    "print(f\"ONNX on CPU took {onnx_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c956842-43ba-4f9a-937a-13fda9ad1439",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85435494-6b29-4b18-aa34-328e33caa9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight Maverick\",\n",
    "    \"Fight Adine\",\n",
    "    \"Attack Adine\"\n",
    "]\n",
    "for rp in test_rps:\n",
    "    print(f'{rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a118815-8982-47ca-8482-c73430d91312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

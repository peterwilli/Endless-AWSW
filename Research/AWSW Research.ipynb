{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b97b376-9ac5-4c70-ba13-769649099a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887d9f81-96c3-408f-bd1c-7b126becc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use cuda:0 for training with seed: 3218885689\n"
     ]
    }
   ],
   "source": [
    "# seed = random.randint(0, 2 ** 32 - 1)\n",
    "seed = 3218885689\n",
    "random.seed(seed)\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"Will use {device_name} for training with seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e95113e-dfc1-40a9-a39b-5d56f3bf32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(os.path.join(Config.work_dir, \"awsw_story_input.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb65c62-47c6-4d36-bd1e-2adfef8ff1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": 6e-4,\n",
    "    \"warmup_factor\": 0,\n",
    "    \"scheduler\": \"polynomial_decay_schedule_with_warmup\",\n",
    "    \"lr_end\": 2e-6,\n",
    "    \"power\": 0.6,\n",
    "    #\"freeze_layer_rate\": 1e-4,\n",
    "    \"freeze_from_steps\": -1,\n",
    "    \"seed\": seed,\n",
    "    \"num_epoch\": 100\n",
    "}\n",
    "\n",
    "optuna_result_attachement = {\n",
    "    'lr': 0.001,\n",
    "    'scheduler': 'cosine_schedule_with_warmup',\n",
    "    'to_freeze_count': 150,\n",
    "    #\"to_freeze_gpt_blocks\": 11,\n",
    "    'warmup_factor': 1\n",
    "}\n",
    "config.update(optuna_result_attachement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ffae5b-e197-4417-a4cd-e6befc6ee393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded empty model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoLocalSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoLocalSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoLocalSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoLocalSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoLocalSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoLocalSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_main\")\n",
    "if os.path.exists(os.path.join(saved_model_path, \"pytorch_model.bin\")):\n",
    "    print(\"Pretrained model loaded\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "    model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "else:\n",
    "    print(\"Loaded empty model\")\n",
    "    model, tokenizer = get_model(\"EleutherAI/gpt-neo-125M\")\n",
    "model.to(device)\n",
    "# set_pretrained_model_dropout(model.transformer.h[-1:], 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f3315-8f5d-4101-8e7e-2c0129204c94",
   "metadata": {},
   "source": [
    "# Test before training on a pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f68c3f1-4c0a-41a2-a3fc-b8b3c36e418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you? I'm the new head of the school. You know how I like to write and do whatever I want. It's not that I'm not interested, it's just that I don't know what to write about.\"\n",
      "\n",
      "\"I know you're a lot of things,\" I said. \"But it's the only thing that you can really say to me.\"\n",
      "\n",
      "\"I don't want to talk about it.\"\n",
      "\n",
      "\"You don't have to,\" I said. \"It's not that I don't like you. I just don't know what to write about.\"\n",
      "\n",
      "\"I don\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer)\n",
    "def test_regular_sampler():\n",
    "    print(model_manager.say_raw(\"How are you? I'm\", 50, 0.7))\n",
    "test_regular_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e9e10-b2eb-421a-9948-dc3590e1f46d",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Model is put in training mode and we begin training. The `train_results` will contain all data after training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41750b03-f114-46c4-948e-637db79efe15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset demo snapshot:\n",
      "<d><scn>facin2<msg>An \"Just one of the perks of being the manager of this facility.\"<|endoftext|><d><scn>black<msg>n \"Of course, that complicated things. My committee was once again tasked with judging the viability of the different projects, and while I intended to stay impartial on the matter, she would probably be seen as an influence due to our relationship.\"<|endoftext|><d><scn>emeraroom<msg>Em \"Not really. You know how a brush works, don't you?\"<|endoftext|><d><scn>o2<msg>Br\n",
      " \"Alright.\"<|endoftext|><p><msg>c \"Maybe we can collaborate.\"<d><scn>loremapt<msg>Ip \"I certainly hope so.\"<|endoftext|><d><scn>black<msg>Br \"Waiter... next round.\"<|endoftext|><p><msg>c \"I was just asking a question, that's all.\"<|endoftext|><d><scn>park3<msg>Em \"Good. Considering all the murders took place during the night, I'll certainly need it. Long days are endemic in my line of work.\"<|endoftext|><p><msg>c \"They can be.\n",
      "[0] set freeze_part_layers: True (freezing 150 out of 160 layers.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10500' max='10500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10500/10500 54:40, Epoch 99/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>2.327800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>2.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>2.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>2.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365</td>\n",
       "      <td>2.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>2.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>2.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1785</td>\n",
       "      <td>2.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>2.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>2.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2205</td>\n",
       "      <td>2.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>2.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2415</td>\n",
       "      <td>2.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>2.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>2.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>2.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2835</td>\n",
       "      <td>2.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>2.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3045</td>\n",
       "      <td>2.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3255</td>\n",
       "      <td>2.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>2.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3465</td>\n",
       "      <td>2.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>2.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>2.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>2.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3885</td>\n",
       "      <td>2.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>2.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4095</td>\n",
       "      <td>2.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4305</td>\n",
       "      <td>2.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>2.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4515</td>\n",
       "      <td>2.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>2.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4725</td>\n",
       "      <td>2.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4830</td>\n",
       "      <td>2.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4935</td>\n",
       "      <td>2.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>2.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5145</td>\n",
       "      <td>2.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5355</td>\n",
       "      <td>1.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>2.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5565</td>\n",
       "      <td>2.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5670</td>\n",
       "      <td>2.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5775</td>\n",
       "      <td>1.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>2.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5985</td>\n",
       "      <td>2.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6090</td>\n",
       "      <td>1.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6195</td>\n",
       "      <td>2.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6405</td>\n",
       "      <td>1.992400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6510</td>\n",
       "      <td>1.993900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6615</td>\n",
       "      <td>1.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6720</td>\n",
       "      <td>2.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6825</td>\n",
       "      <td>1.996900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6930</td>\n",
       "      <td>1.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7035</td>\n",
       "      <td>2.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7140</td>\n",
       "      <td>1.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7245</td>\n",
       "      <td>2.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7455</td>\n",
       "      <td>1.991500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7560</td>\n",
       "      <td>2.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7665</td>\n",
       "      <td>1.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7770</td>\n",
       "      <td>1.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7875</td>\n",
       "      <td>1.987800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7980</td>\n",
       "      <td>1.987900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8085</td>\n",
       "      <td>1.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8190</td>\n",
       "      <td>1.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8295</td>\n",
       "      <td>1.996300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8505</td>\n",
       "      <td>2.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8610</td>\n",
       "      <td>1.996300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8715</td>\n",
       "      <td>1.983400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8820</td>\n",
       "      <td>1.993100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8925</td>\n",
       "      <td>1.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9030</td>\n",
       "      <td>1.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9135</td>\n",
       "      <td>1.993100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9240</td>\n",
       "      <td>1.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9345</td>\n",
       "      <td>1.982400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>1.997900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9555</td>\n",
       "      <td>1.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9660</td>\n",
       "      <td>2.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9765</td>\n",
       "      <td>1.986400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9870</td>\n",
       "      <td>1.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9975</td>\n",
       "      <td>1.992400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10080</td>\n",
       "      <td>1.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10185</td>\n",
       "      <td>1.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10290</td>\n",
       "      <td>2.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10395</td>\n",
       "      <td>1.987900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.992800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = {}\n",
    "model.train()\n",
    "train_model(model, tokenizer, config, train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c410a601-c5e4-4cc1-af2b-44a7378f3bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd47a8bc580>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Learning rate and loss')\n",
    "axs[0].plot(train_results['learning_rate_history'])\n",
    "axs[1].plot(train_results['loss_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a578b-32cd-44b9-829b-5aef5ec37ab0",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72d4876-df61-461a-b715-980a3763f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you? I'm just an outsider.\"<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_regular_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f9a94-70c6-4a58-9ad1-3d6ea373dfbe",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97230c-1d3a-4dd2-a253-727e451d539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"Prompt: {prompt}\\nReply: {reply}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3349bf-c777-4937-a023-ff0f4f21806d",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce4bb65f-e26e-4a62-9c67-aed885fe041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test 1] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I'm fine.\"<d><scn>park2<msg>Ry \"What do you think?\"<p><msg>c \"I think you're overqualified to be a doctor, and you're just trying to save the day.\"<d><scn>park2<msg>Ry \"Yeah, but you're also doing something for me.\"<p><msg>\n",
      "\n",
      "[Test 1] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ad \"Well, you know he's not a doctor, but that's something I should get out of here.\"<p><msg>c \"Well, I guess it's nice to have someone in the company of you.\"<d><scn>park2<msg>Ad \"Oh, well, I'll just have to be careful about the\n",
      "\n",
      "[Test 1] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: facin3<msg>An \"Adine, you can sit on my bench. I can't.\"<p><msg>c \"Oh my god, Adine. What is this?\"<d><scn>facin3<msg>An \"Adine, you can sit on my bench. I can't.\"<p><msg>c \"Oh my god, Adine. What is this\n",
      "\n",
      "[Test 1] -> Prompt: What will we do here?\n",
      "Reply: neurofur<msg>n \"We'll go to the woods and catch the birds first.\"<d><scn>neurofur<msg>n \"I will.\"<p><msg>c \"What will we do here?\"<d><scn>neurofur<msg>n \"We'll go to the woods and catch the birds first.\"<p><msg>c \"I will.\"<p><msg>\n",
      "\n",
      "-------------\n",
      "[Test 2] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"You look great.\"<p><msg>c \"Thanks.\"<d><scn>park2<msg>Ry \"Thanks.\"<|endoftext|>\n",
      "\n",
      "[Test 2] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"It's just that he's just a human.\"<d><scn>park2<msg>Ry \"You don't know about the whole situation?\"<p><msg>c \"No, I'm not a human. I'm a human.\"<d><scn>park2<msg>Ry \"I know about the whole\n",
      "\n",
      "[Test 2] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: black<msg>m \"I'm sorry.\"<d><scn>black<msg>m \"I guess it's better if you just take off your pants.\"<d><scn>black<msg>m \"I'm sorry.\"<d><scn>black<msg>m \"Adine looked around for her jacket.\"<d><scn>black<msg>m \"Adine\n",
      "\n",
      "[Test 2] -> Prompt: What will we do here?\n",
      "Reply: n<msg>m \"I took my time to think about what we could do.\"<d><scn>n<msg>m \"It's a big risk to run into the forest.\"<p><msg>c \"It would be a good idea if we had someone with us. I'll take a guess.\"<d><scn>n<msg>m \"She didn't say anything.\"<p><msg>c\n",
      "\n",
      "-------------\n",
      "[Test 3] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"I'm good.\"<d><scn>park2<msg>Ry \"I'm good.\"<p><msg>c \"How are you?\"<d><scn>park2<msg>Ry \"I'm good.\"<p><msg>c \"How are you?\"<d><scn>park2<msg>Ry \"I'm good.\"<p\n",
      "\n",
      "[Test 3] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ad \"You should come out for it. I'd like to. But I don't know if he's interested in joining me. I don't have much time.\"<p><msg>c \"He's an amazing guy. I'm glad he's here. I'm glad to see you. It's nice to see you, Lorem.\"<|endoftext|>\n",
      "\n",
      "[Test 3] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: black<msg>m \"She is a very strong person, she is very fast.\"<d><scn>black<msg>m \"Adine is not just a strong person, she is also a human.\"<p><msg>c \"Adine is a human.\"<d><scn>black<msg>m \"She is a very strong person, she is also a human.\"<d\n",
      "\n",
      "[Test 3] -> Prompt: What will we do here?\n",
      "Reply: park2<msg>Br \"I can't see anything here.\"<p><msg>c \"I don't know. I think it's going to take me a while to make up for that.\"<d><scn>park2<msg>Br \"I think it's going to take me a while to make up for that.\"<p><msg>c \"I don't know. I think it's going to take me\n",
      "\n",
      "-------------\n",
      "[Test 4] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Yeah, I'm fine.\"<p><msg>c \"How do you like that?\"<d><scn>park2<msg>Ry \"I like that.\"<p><msg>c \"You're a little nervous.\"<d><scn>park2<msg>Ry \"Well, I'm not worried about that.\"<d><scn>park2\n",
      "\n",
      "[Test 4] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ry \"What do you think of Lorem?\"<p><msg>c \"He just wants to get on with his business, and to get back to his roots.\"<d><scn>park2<msg>Ry \"He just wants to get on with his business.\"<p><msg>c \"I was with Lorem today.\"\n",
      "\n",
      "[Test 4] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: black<msg>An \"You're here to take me into a new world.\"<d><scn>black<msg>An \"You're here to take me into a new world.\"<p><msg>c \"Oh my god, Adine. What is this? I'm in trouble?\"<d><scn>black<msg>An \"Oh my god, Adine. What is this\n",
      "\n",
      "[Test 4] -> Prompt: What will we do here?\n",
      "Reply: np2<msg>m \"The question is, will we have the time to visit?\"<d><scn>np2<msg>m \"It is hard to say.\"<d><scn>np2<msg>m \"I don't know.\"<d><scn>np2<msg>m \"She took a nap.\"<d><scn>np2<msg>m \"She took a nap\n",
      "\n",
      "-------------\n",
      "[Test 5] -> Prompt: How are you?\n",
      "Reply: park2<msg>Ry \"Yeah, I'm fine.\"<d><scn>park2<msg>Ry \"That's good.\"<p><msg>c \"Yeah, I'm fine.\"<d><scn>park2<msg>Ry \"Hey Remy, can you say I'm fine?\"<p><msg>c \"Yeah, I'm fine.\"<d><scn>park\n",
      "\n",
      "[Test 5] -> Prompt: What do you think of Lorem?\n",
      "Reply: park2<msg>Ad \"What about him?\"<d><scn>park2<msg>Ad \"I'm not sure.\"<p><msg>c \"I'm afraid Lorem is the right person for this situation.\"<d><scn>park2<msg>Ad \"You're right, Lorem. I'm just hoping you can help me\n",
      "\n",
      "[Test 5] -> Prompt: Oh my god, Adine. What is this?\n",
      "Reply: facin2<msg>Ad \"I see.\"<d><scn>facin2<msg>Ad \"That's my only reason for wanting to meet you.\"<p><msg>c \"I see.\"<d><scn>facin2<msg>Ad \"I see.\"<p><msg>c \"I see.\"<d><scn>facin2<msg>\n",
      "\n",
      "[Test 5] -> Prompt: What will we do here?\n",
      "Reply: np1n<msg>m \"The two of us were about to head back to the office.\"<p><msg>c \"We don't know why.\"<d><scn>np1n<msg>m \"We were going to try and find a way to get it back. That was what we were hoping for.\"<p><msg>c \"Well, we had a few other things planned, so we couldn't\n",
      "\n",
      "-------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2731/3584089660.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Test {i + 1}] -> Prompt: {prompt}\\nReply: {reply}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/awsw/model_manager.py\u001b[0m in \u001b[0;36msay\u001b[0;34m(self, past, prompt, top_k, top_p)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{past}<p><msg>c \"{prompt}\"{self.reply_prefix}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msay_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/awsw/model_manager.py\u001b[0m in \u001b[0;36msay_raw\u001b[0;34m(self, prompt, top_k, top_p)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprompt_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         sample_outputs = self.model.generate(\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1017\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 )\n\u001b[1;32m    857\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    859\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    556\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     ):\n\u001b[0;32m--> 497\u001b[0;31m         outputs = self.attention(\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mpast_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for (past, prompt) in prompts:\n",
    "        reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "        print(f\"[Test {i + 1}] -> Prompt: {prompt}\\nReply: {reply}\\n\")\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41090174-6be5-4cc4-8f6a-b2cf56628c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What to say?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_dragon_reply' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2731/3578139120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What to say?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_dragon_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_dragon_reply' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"What to say?\")\n",
    "print(generate_dragon_reply(\"\", input()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

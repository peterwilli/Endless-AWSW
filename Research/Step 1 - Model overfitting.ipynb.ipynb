{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da2dbfd-5b54-424f-8643-6482bd916f0c",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "... Normally? A bad thing! But for our case it's good...\n",
    "\n",
    "We use a new method to mix an overfitted model (our own) with a pretrained model (GPT-Neo-125M) and have them share eachothers traits. This way it's possible to finetune a model without having to retrain it. It's so fast it can be done in a second on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b97b376-9ac5-4c70-ba13-769649099a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset, ModelSeeder\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "import logging\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887d9f81-96c3-408f-bd1c-7b126becc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use cuda:0 for training with seed: 970988852\n"
     ]
    }
   ],
   "source": [
    "# seed = random.randint(0, 2 ** 32 - 1)\n",
    "seed = 970988852\n",
    "random.seed(seed)\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"Will use {device_name} for training with seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e95113e-dfc1-40a9-a39b-5d56f3bf32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(os.path.join(Config.work_dir, \"awsw_story_input.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb65c62-47c6-4d36-bd1e-2adfef8ff1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr': 0.001,\n",
    "    \"warmup_factor\": 5,\n",
    "    'scheduler': 'cosine_schedule_with_warmup',\n",
    "    \"lr_end\": 2e-6,\n",
    "    \"power\": 0.6,\n",
    "    \"freeze_from_steps\": -1,\n",
    "    'to_freeze_count': 0,\n",
    "    \"seed\": seed,\n",
    "    \"num_epoch\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ffae5b-e197-4417-a4cd-e6befc6ee393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded empty model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_main\")\n",
    "if os.path.exists(os.path.join(saved_model_path, \"pytorch_model.bin\")):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "    model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "    print(\"Pretrained model loaded\")\n",
    "else:\n",
    "    model, tokenizer = get_model(\"EleutherAI/gpt-neo-125M\")\n",
    "    print(\"Loaded empty model\")\n",
    "model.to(device)\n",
    "# set_pretrained_model_dropout(model.transformer.h[-1:], 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f3315-8f5d-4101-8e7e-2c0129204c94",
   "metadata": {},
   "source": [
    "# Test before training on a pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f68c3f1-4c0a-41a2-a3fc-b8b3c36e418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my dreams, I'm a dragonfly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-fly-\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "def test_regular_sampler():\n",
    "    print(model_manager.say_raw(\"In my dreams, I'm a dragon\", 50, 0.7))\n",
    "test_regular_sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f74aa-5030-4ffd-ad2f-ae013647975e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reviewing our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0974ac13-c420-4275-b34a-3816f580cb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0374e2cd61dc48e08af48b89f0ee9165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset demo snapshot:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2159 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3376 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><msg>c \"Really?\"<|endoftext|><d><scn>bareblur<msg>m \"Roaring laughter erupted from the dragon as his paw struck the table, rattling the containers resting on it.\"<|endoftext|><d><scn>np1x<msg>Br \"If that's all, I'm going to head back to the Ministry. I shouldn't keep her waiting.\"<|endoftext|><d><scn>alley<msg>An \"And have you hanging around sensitive lab equipment? Denied.\"<|endoftext|><p><msg>c \"Hey.\"<|endoftext|><p><msg>c \"I\n",
      "RP review!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2159 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " had it in you.\"<|endoftext|><p><msg>c \"Well, it's not like you forced me to participate, so I suppose I share some of the blame...\"<|endoftext|><p><msg>c \"I thought you wanted to show me, since you said we were both in biology and all that.\"<|endoftext|><p><msg>c \"Fight Bryce\"<d><scn>black<msg>m \"I didn't hesitate and kicked Bryce right in the stomach\"<|endoftext|><d><scn>facin3<msg>An \"Alright, sorry for making you wait.\"<|endoftext|><p><msg>c\n",
      " \"I must have left it in my apartment.\"<|endoftext|><p><msg>c \"I know it's not Knizki's {i}Textile Merchant{/i}, but I suppose it's better than nothing.\"<|endoftext|><d><scn>remyapt<msg>Ry \"Interesting you say that. Are all humans vegetarians, or is that just you?\"<|endoftext|><d><scn>black<msg>n \"Needless to say, I didn't regret leaving him behind.\"<|endoftext|><d><scn>bareblur2<msg>Br \"Hey, are you okay? You\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(tokenizer, path_train = os.path.join(Config.work_dir, \"data_train.txt\"))\n",
    "print(\"Dataset demo snapshot:\")\n",
    "for item in dataset['train']:\n",
    "    print(tokenizer.decode(item['input_ids']))\n",
    "    break\n",
    "\n",
    "print(\"RP review!\")\n",
    "has_seen_rp = False\n",
    "for item in dataset['train']:\n",
    "    decoded = tokenizer.decode(item['input_ids'])\n",
    "    if 'c \"Fight ' in decoded: \n",
    "        print(decoded)\n",
    "        has_seen_rp = True\n",
    "        continue        \n",
    "    if has_seen_rp:\n",
    "        print(decoded)\n",
    "        break\n",
    "# Clean up\n",
    "del has_seen_rp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e9e10-b2eb-421a-9948-dc3590e1f46d",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Model is put in training mode and we begin training. The `train_results` will contain all data after training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41750b03-f114-46c4-948e-637db79efe15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] set freeze_part_layers: True (freezing 0 out of 160 layers.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11500' max='11500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11500/11500 1:20:26, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.776700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.470800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265</td>\n",
       "      <td>0.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>0.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2185</td>\n",
       "      <td>0.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2415</td>\n",
       "      <td>0.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2645</td>\n",
       "      <td>0.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>0.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3105</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3335</td>\n",
       "      <td>0.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3565</td>\n",
       "      <td>0.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3795</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>0.276500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4025</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4255</td>\n",
       "      <td>0.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4370</td>\n",
       "      <td>0.272100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4485</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4715</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4830</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4945</td>\n",
       "      <td>0.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>0.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5175</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5290</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5405</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>0.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5635</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5865</td>\n",
       "      <td>0.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6095</td>\n",
       "      <td>0.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6210</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6325</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6440</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6555</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6670</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6785</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7015</td>\n",
       "      <td>0.252900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7130</td>\n",
       "      <td>0.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7245</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7360</td>\n",
       "      <td>0.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7475</td>\n",
       "      <td>0.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7590</td>\n",
       "      <td>0.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7705</td>\n",
       "      <td>0.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7820</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7935</td>\n",
       "      <td>0.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8165</td>\n",
       "      <td>0.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8280</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8395</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8510</td>\n",
       "      <td>0.242700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8625</td>\n",
       "      <td>0.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8740</td>\n",
       "      <td>0.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8855</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8970</td>\n",
       "      <td>0.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9085</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9315</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9430</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9545</td>\n",
       "      <td>0.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9660</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9775</td>\n",
       "      <td>0.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9890</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10005</td>\n",
       "      <td>0.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10120</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10235</td>\n",
       "      <td>0.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10465</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10580</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10695</td>\n",
       "      <td>0.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10810</td>\n",
       "      <td>0.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10925</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11040</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11155</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11270</td>\n",
       "      <td>0.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11385</td>\n",
       "      <td>0.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = {}\n",
    "model.train()\n",
    "train_model(model, tokenizer, dataset, config, train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c410a601-c5e4-4cc1-af2b-44a7378f3bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1f82cec1f0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Learning rate and loss')\n",
    "axs[0].plot(train_results['learning_rate_history'])\n",
    "axs[1].plot(train_results['loss_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753228f0-b420-4b3d-bd69-53da7a87bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.save_pretrained(saved_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

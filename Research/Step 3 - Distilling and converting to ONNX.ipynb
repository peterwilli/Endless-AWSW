{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98b2c15-46b2-4b0f-9069-68cfe40d8dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b7a9d-8687-4c2f-8278-6f83d9fabbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset, ModelSeeder\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "import logging\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b87e8-66eb-4779-89a5-a8d7aad4376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_mixed\")\n",
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "    !cp -n $saved_model_path/gpt-neo-125M/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/gpt-neo-125M\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm-with-past $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fe1e14-e2a5-4fa9-a331-ae156a8967e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QInt8)\n",
    "        #!rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3ae8b2-8518-4071-9e7b-bfdd52087a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model-opt.onnx\"))\n",
    "onnx_model_manager_quant = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "print(f\"Pretrained model loaded on {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6291ed-7153-4904-826b-fb64f88fdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't think I can do it anymore.\"<p>I'm not sure what to think anymore.\"<p><msg>c \"I'm not going to let this go, are you, and the rest of this is about my personal safety.\"<d><scn>np2x<msg>Ad \"That is, unless I can prove it.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon. I'm a dragon, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't think I can do it anymore.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I could hardly be blamed for my nice morning. I'm still in the middle of an experiment and don't have a lot of time for it. I'll just wait and see how the results are.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't want to be a horrible person anymore.\"<p><msg>c \"I'm not sure what to think anymore.\"<d><scn>black<scn>cafe<msg>An \"I'm not sure what to think anymore.\"<d><msg>c \"I'm not sure what to think anymore.\"<c \"I'm not sure what to think anymore.\"c \"I'm not sure what to think anymore.\"<d><scn>black<scn>o2c<msg>c \"I\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I'm a dragon, too. I'm just a human.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't think I can do it anymore.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I'm a dragon. I'm a dragon. I'm the only one who can save you.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't think I can do it anymore.\"A few days back in my room, I was just having a hard time getting my way to the beach. I was just thinking, I guess I'll just wait a little longer for you.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I could certainly use a few spices.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't think I can do it anymore.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I'll be back in a while.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't think I can do it anymore.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I'm a dragon, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've been there so long and learned the ins and outs of the building so well that eventually I was asked to take over the duties of the building. I was a little surprised to find out that it was a human that would be here for the next two hours, but I was sure it was a dragon. I'm sure it was a little late, but I'll just have to be careful.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I've got it in my room.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't think I can do it anymore.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I can see it in the morning, but I'm not sure if I can just do it anymore. I don't know what to think anymore.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and a lot of hair. I just don't think I can do it anymore.\"<d><scn>o4<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, and I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I\n",
      "PyTorch: In my dreams, I'm a dragon. I've got wings, but I'm not sure if I could do that. I don't think I can do that. I don't think I can.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(10):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"ONNX (Quantized):\", onnx_model_manager_quant.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9396c5-9837-4c77-9011-1ac48b711286",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c3279-83c5-4d81-b333-5d9450ad1a62",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cf23db-6013-4ff4-8a4f-effddd883295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I'm fine. I'm just a little busy.\"<p><msg>c \"I'm not sure if I should take any visitors.\"<d><scn>park2<msg>Ry \"No, I'm not going to be rude or anything. I just want to get this over with.\"<p><msg>c \"I'm not sure if I should take any\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm fine. I'm just a little busy.\"<p><msg>c \"I'm not sure if I should take any visitors.\"<d><scn>park2<msg>Ry \"No, I'm not going to be rude or anything. I just want to get it over with.\"<p><msg>c \"I'm not sure if I should take any visitors.\"<d><scn>park2<msg>Ry \"No, I'm not going to be rude or anything. I just want to get it over with.\"<p><msg>c\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"I'm fine, I'll just have to be careful.\"<p><msg>c \"I'm fine.\"<d><scn>park2<msg>Ry \"I'm fine, I'll just have to be careful.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think he's a good student.\"<p><msg>c \"I'm not sure, I am not a linguist. Are you?\"<d><scn>park2<msg>Ad \"Not really. I am a linguist. Are you?\"<p><msg>c \"I'm not sure, I am not\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"I think he's a good student.\"<p><msg>c \"I'm not sure, I am not a linguist. Are you?\"<d><scn>park2<msg>Ad \"Not really. I am a linguist. Are you?\"<p><msg>c \"I'm not sure, I am not a linguist. Are you?\"<d><scn>park2<msg>Ad \"Not really. I am a linguist. Are you?\"<p><msg>c \"I'm not sure, I am not a linguist.\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ad \"I think he's a nice change of mood.\"<p><msg>c \"I'm not sure what to say to that.\"<d><scn>park2<msg>Ad \"I don't really like them very much.\"<p><msg>c \"I'm not sure what to say to that.\"<d><scn>park2<msg>Ad \"I don't really like them very much.\"<p><msg>c \"I'm not sure what to say to that.\"<d><scn>park2<msg>Ad \"I don't\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: park2<msg>Ad \"It's a long story.\"<p><msg>c \"I don't know. I don't want to go on a date with you.\"<d><scn>park2<msg>Ad \"I'm not sure what to say.\"<p><msg>c \"I don't want to go on a date with you. I'm not sure what else I could\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"It's a long story.\"<p><msg>c \"I don't know. I don't want to go on a date with you.\"<d><scn>park2<msg>Ad \"I'm not sure what to say.\"<p><msg>c \"I don't want to go on a date with you. I'm not sure what else I could offer you.\"<d><scn>park2<msg>Ad \"I'm not sure what else I could offer you. I'm not sure if I could just pack up and leave everything behind.\"<\n",
      "\n",
      "[ONNX Quantized] Reply: o2<msg>Ad \"It's a little different, but I think it's better than nothing.\"<p><msg>c \"I don't know, but I think it's better than nothing.\"<d><scn>o2<msg>Ad \"I don't know. I don't want to be rude or anything, but I think it's better than nothing.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: loremapt<msg>Ip \"I'll take a seat.\"<d><scn>loremapt<msg>Ip \"That's right.\"<p><msg>c \"What is it?\"<d><scn>loremapt<msg>Ip \"I'm just a very busy person.\"<p><msg>c \"I'm not sure, I am not a linguist.\"<d><sc\n",
      "\n",
      "[ONNX] Reply: loremapt<msg>Ip \"I'll take a seat.\"<d><scn>loremapt<msg>Ip \"That's right.\"<p><msg>c \"What is it?\"<d><scn>loremapt<msg>Ip \"I'm just a very busy person.\"<p><msg>c \"I'm not sure, I am not a linguist.\"<d><scn>loremapt<msg>Ip \"It is a little easier to find out what the original human looked like.\"<d><scn>loremapt<msg\n",
      "\n",
      "[ONNX Quantized] Reply: facin2<msg>An \"We'll look at the results.\"<p><msg>c \"What does that mean?\"<d><scn>facin2<msg>An \"We don't know what you are talking about.\"<p><msg>c \"I'm not sure, but I think you're right.\"<d><scn>facin2<msg>An \"I don't know what you are talking about.\"<p><msg>c \"I'm not sure, but I think you're right.\"<d><scn>facin2<msg>An \"I\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9ea4d-a30f-4c1a-a1f6-d5badff70453",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216177f-f03b-4358-8b5c-7eb0dc663af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I'm fine. I just have a few questions.\"<d><scn>park2<msg>Ry \"I'm sure you have a point.\"<p><msg>c \"I'm sorry, I don't know how to say this.\"<d><scn>park2<msg>Ry \"I'm sorry, I don't know how to say this. I\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I was just kidding.\"<p><msg>c \"Hey [player_ name].\"<d><scn>park2<msg>Ry \"Hey.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"It's not that easy. It seems like I'm in a rush.\"<p><msg>c \"It's not a problem. You're right.\"<d><scn>park2<msg>Ry \"Well, I guess I'll just have another to slip up about.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I'm not sure, but he's a good fellow.\"<p><msg>c \"I think he's a good student.\"<d><scn>park2<msg>Ad \"I'm not sure, but you do have a point.\"<p><msg>c \"I'm not sure, but you do have a point\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"It's pretty important you get your tail to the right edge, right?\"<p><msg>c \"I'm not. I don't know what they are.\"<d><scn>park2<msg>Ad \"I'm not sure what they mean, exactly.\"<p><msg>c \"I'm not sure what they mean.\"<p><msg>c \"I don't know what you are doing. Are we going to get caught in a door that isn't as big?\"<d><scn>park2<msg>Ad \"I think we're done\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ad \"I think you're just having a nice time with me.\"<p><msg>c \"I'm not so sure about it. Have you worked on your MvAs yet?\"<d><scn>park2<msg>Ad \"I don’ t know, I’ve been thinking a bit about it. It was just something I just had to figure out. I just don‒t think I’d be able to tell you anything about the world we�’ve been in trouble with.\"<d><scn>park2<msg>Ad\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: o2<msg>Ad \"It's a little bit of a shame to have you here, but I'm glad you're here. I'm sure you're a lot of fun.\"<p><msg>c \"I'm sure you'll do the same.\"<d><scn>o2<msg>Ad \"I'm sure you'll do the same.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"I don't know. I just remembered that Adine and I have a little while of an idea how to make our relationship work.\"<p><msg>c \"We should probably leave the city, as it's getting late.\"<d><scn>park2<msg>Ad \"That's a shame, I don't know what to say.\"<p><msg>c \"You think I'm too nice?\"<d><scn>park2<msg>Ad \"I'm just trying to help out a little.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: o2<msg>Ad \"Do they have a lot to do?\"<p><msg>c \"Not really, but I think they're cute.\"<d><scn>o2<msg>Ad \"I think I'd rather not.\"<p><msg>c \"Not really, but they do. I'm sure Adinea can learn about you as a little.\"<d><scn>o2<msg>Ad \"You know how I do.\"<p><msg>c \"How does your work look at the outside? I'd like to see it in your face. How about you tell\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: facin2<msg>An \"I don't know. I don't want to know.\"<p><msg>c \"What do you think is going to happen here?\"<d><scn>facin2<msg>An \"I don't know. I don't want to know.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: facin2<msg>An \"We'll take it as a compliment.\"<d><scn>facin2<msg>An \"It's a shame you can't be here now.\"<p><msg>c \"It's a shame. You're not doing it right.\"<d><scn>facin2<msg>An \"I'm not sure if you're going to help me with my research. It's a bit of a long story, and it's a little embarrassing to have to sit in this office for so long.\"<p><msg>c \"I'm sure we'll have\n",
      "\n",
      "[ONNX Quantized] Reply: facin3<msg>An \"We'll take a coffee.\"<p><msg>c \"What will we do with it?\"<d><scn>facin3<msg>An \"We don't want the lab to waste our time until we have a real one. It was fun.\"<|endoftext|>\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3f1c-91c0-4585-92b6-fab86678834c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8b6dcd-9723-4832-994e-c39411df2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pytorch] Visit Lorem -> loremapt<msg>Lo \"Hey, Lorem.\"<d><scn>loremapt<msg>Lo \"Hey, Lorem.\"<d><scn>loremapt<msg>Lo \"Hey, Lorem.\"<d><scn>loremapt<msg>Lo \"Hey, Lorem.\"<d><scn>loremapt<msg>Lo \"Hey, Lorem.\"<|endoftext|>\n",
      "[ONNX] Visit Lorem -> loremapt<msg>Lo \"Oh. Good morning.\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Lorem -> lorem<msg>Lo \"Oh, I'm just messing around.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Meet with Lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] Meet with Lorem -> loremapt<msg>Lo \"Oh, [Oh, [I can't remember anything about him, but I think it's pretty important for the police to have a nice evening.\"<|endoftext|>\n",
      "[ONNX Quantized] Meet with Lorem -> lorem<msg>Lo \"Oh. I see. Thank's for taking the time to pay my visit.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Visit Adine -> adineapt<msg>Ad \"Oh, I see.\"<|endoftext|>\n",
      "[ONNX] Visit Adine -> adineapt<msg>Ad \"Hey, [player_name]!\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Adine -> adineapt<msg>Ad \"Oh, you're in luck. I have something really important to do, and I'm going to need you to take a look.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Fight Maverick -> cafe<msg>m \"I'm getting tired. I don't know how to make my work a day to remember.\"<d><scn>cafe<msg>m \"I looked over my shoulder and tried to see if Maverick was still in the middle of the night. I didn't see him move, and I didn't think he would want to. I looked back over my shoulder to see Bryce still lying in the same spot.\"<d><scn>cafe<msg>m \"I looked over my shoulder\n",
      "[ONNX] Fight Maverick -> cafe<msg>m \"I didn’ t think I’d be so sure, but I was just about to ask him if I should go. I wasn’t sure what he could say to that, but if he had the chance, it would be up to me. I don’ t think he ever wanted to meet with me again. It was only a matter of getting him to say anything, but I think it was worth it to say goodbye. I think we should leave the relationship to him. We should talk it out.\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Maverick -> black<msg>m \"Maverick barely avoids my attack when he was close to the wall.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Fight Adine -> o2<msg>Adine \"Hey, I know you wanted to tell me all the stuff about Reza, but I'm not sure what to think anymore.\"<|endoftext|>\n",
      "[ONNX] Fight Adine -> cafe<msg>m \"Adine dodges my attack\"<d><scn>cafe<msg>m \"I'm not so sure about that...\"<d><scn>cafe<msg>Ad \"I'm not so sure about what I'm going to do with you. I'm not sure what you're doing here, but I'm sure you'll be glad to see me. I'll just be a minute, then.\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Adine -> cafe<msg>m \"I started getting a few questions out of them, and I think it's time for a little fun.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Attack Adine -> adineapt<msg>m \"Adine barely avoids my attack, but I managed to get it both out of my way before it even got to this point. I'll leave the contest to you.\"<|endoftext|>\n",
      "[ONNX] Attack Adine -> adineapt<msg>m \"I started to think that maybe the whole Reak might come to be like this.\"<|endoftext|>\n",
      "[ONNX Quantized] Attack Adine -> adineapt<msg>m \"I started to think about what might happen now. Maybe I should wait.\"<|endoftext|>\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight Maverick\",\n",
    "    \"Fight Adine\",\n",
    "    \"Attack Adine\"\n",
    "]\n",
    "for rp in test_rps:\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f43a12-6e71-4ee1-af0a-66d41df0032c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

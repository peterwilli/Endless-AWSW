{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98b2c15-46b2-4b0f-9069-68cfe40d8dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b7a9d-8687-4c2f-8278-6f83d9fabbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "import logging\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b87e8-66eb-4779-89a5-a8d7aad4376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\n",
      "Cloning into 'gpt-neo-125M'...\n",
      "remote: Enumerating objects: 44, done.\u001b[K\n",
      "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 44 (delta 20), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (44/44), 543.14 KiB | 1.27 MiB/s, done.\n",
      "Using framework PyTorch: 1.10.1+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:555: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'present.2.value', 'present.8.key', 'present.8.value', 'present.0.value', 'present.9.value', 'present.3.value', 'present.10.value', 'present.7.value', 'present.3.key', 'logits', 'present.6.key', 'present.11.value', 'present.11.key', 'present.5.key', 'present.4.key', 'present.0.key', 'present.2.key', 'present.10.key', 'present.9.key', 'present.1.key', 'present.1.value', 'present.5.value', 'present.4.value', 'present.7.key', 'present.6.value'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 8, 50257) matches (2, 8, 50257)\n",
      "\t\t-[x] values not close enough (atol: 1e-05)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/onnx/__main__.py\", line 99, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/onnx/__main__.py\", line 92, in main\n",
      "    validate_model_outputs(onnx_config, preprocessor, model, args.output, onnx_outputs, args.atol)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/onnx/convert.py\", line 415, in validate_model_outputs\n",
      "    raise ValueError(\n",
      "ValueError: Outputs values doesn't match between reference model and ONNX exported model: Got max absolute difference of: 0.08815956115722656\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_main\")\n",
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "    !cp -n $saved_model_path/gpt-neo-125M/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/gpt-neo-125M\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm-with-past $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe1e14-e2a5-4fa9-a331-ae156a8967e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_111]\n",
      "Ignore MatMul due to non constant B: /[MatMul_142]\n",
      "Ignore MatMul due to non constant B: /[MatMul_246]\n",
      "Ignore MatMul due to non constant B: /[MatMul_277]\n",
      "Ignore MatMul due to non constant B: /[MatMul_381]\n",
      "Ignore MatMul due to non constant B: /[MatMul_412]\n",
      "Ignore MatMul due to non constant B: /[MatMul_516]\n",
      "Ignore MatMul due to non constant B: /[MatMul_547]\n",
      "Ignore MatMul due to non constant B: /[MatMul_651]\n",
      "Ignore MatMul due to non constant B: /[MatMul_682]\n",
      "Ignore MatMul due to non constant B: /[MatMul_786]\n",
      "Ignore MatMul due to non constant B: /[MatMul_817]\n",
      "Ignore MatMul due to non constant B: /[MatMul_921]\n",
      "Ignore MatMul due to non constant B: /[MatMul_952]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1056]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1087]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1191]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1222]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1326]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1357]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1461]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1492]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1596]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1627]\n"
     ]
    }
   ],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type = QuantType.QInt8)\n",
    "        #!rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3ae8b2-8518-4071-9e7b-bfdd52087a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = 'cpu'\n",
    "device = torch.device(device_name)\n",
    "\n",
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model-opt.onnx\"))\n",
    "onnx_model_manager_quant = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "print(f\"Pretrained model loaded on {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6291ed-7153-4904-826b-fb64f88fdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon. I'm not even old enough to remember anything.\"<d><scn>black<msg>Lo \"I see.\"#mvn\n",
      "<p>title^1< imaginary number]{ for example}I'm not. You know what? This is a completely different story. You won't find out if you will by next year\"#mv.\"<d><scn>np2x<msg>Ry \"I don't know, Reza.\"<p><msgary>c \"What do you think of Lorem?\"><d><scn>np1x<\n",
      "ONNX (Quantized): In my dreams, I'm a dragon at heart.\"<d> <p> byremyreid<msg>or \"Meet with [a city-based entity]\" <d>(\"Be that to me?\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon.\"<p><msg>c \"Hey Maverick! Over here!\"<d><scn>o2<msg>Mv \"Hey [player_name]! How are you?\"<p><msg>c \"Not at all.\"<d><scn>o2<msg>Br \"Hey, I'm Daniel. 'Til the task at hand is pretty simple.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon and a little girl from a previous life. I grew up around that.\"<p><msg>Comicsarts<msg>Nm \"And I think I... well, or at least I had a name.\"<d><scn<msg>adineapt<\\_[name]<msg>Ip \"And I think you had a good point of order.\"<p>\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon.\"<p><msg>c \"Go to Sebastian\"<d><scn>office<msg>Sb \"Oh, [player_name], I wasn't expecting visitors.\"<p><msg>c \"Why didn't you just come back from the very same situation and ask for my help?\"<d><scn>black<msg>Rz \"I guess it doesn't help that a lot. I just find it difficult to talk to you, to take the time you want to be free of evidence and show proper counsel.\"<p><\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(2):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"ONNX (Quantized):\", onnx_model_manager_quant.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9396c5-9837-4c77-9011-1ac48b711286",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c3279-83c5-4d81-b333-5d9450ad1a62",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf23db-6013-4ff4-8a4f-effddd883295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I was and are, [player_name].\"<p><msg>c \"Hello Zhong\"<d><scn>park2<msg>Zh \"Hey [player_name]!\"<p><msg>c \"I see.\"<d><scn>park2<msg>Ry \"I\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I was and are, [player_name].\"<p><msg>c \"Hello Zhong\"<d><scn>park2<msg>Zh \"Hey [player_name]!\"<p><msg>c \"I see.\"<d><scn>park2<msg>Ry \"I'm not sure if I can be of much help.\"<p><msg>c \"go to remy\"<d><scn>remyapt<msg>Ry \"Oh, [player_name], I wasn't expecting visitors\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>c \"Then, maybe I should put that off for a moment. I might even see the end.\"<d><scn>park3<msg>Mv \"Oh, I'm not sure that will do.\"<p><msg>c \"And I'll never get over it, I'll be the only one. I'll probably die of it soon.\"<d><scn>park3<msg>Mv \"If you can get to the end, I can finish it.\"<p><msg>c \"And I'm not sure if that will be the outcome.\"\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think he's funny.\"<p><msg>c \"I'm not sure if this is much use.\"<d><scn>park2<msg>Ad \"I see. I'll just keep waiting, but I doubt I can get there.\"<p><msg>c \"I see.\"\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"I think he's funny.\"<p><msg>c \"I'm not sure if this is much use.\"<d><scn>park2<msg>Ad \"I see. I'll just keep waiting, but I doubt I can get there.\"<p><msg>c \"I see.\"<d><scn>park2<msg>Ad \"I'm sorry to see this all down.\"<p><msg>c \"I thought it was fun. How'd you end up here, though?\"<d><scn>park2\n",
      "\n",
      "[ONNX Quantized] Reply: Nm \"And maybe, if you want to restore the meaning of the old adine, that too you should be happy.\"<p><msg>c \"And what do we do this way?\"<d><scn>park2<msg>m \"I see a little side of you that are not so well known to me. I would rather the same thing.\"<p><msg>c \"And what do you think of as a young woman?\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: np2y<msg>Ad \"I guess it's not here anymore.\"<p><msg>c \"What do you think of Naomi?\"<d><scn>np2y<msg>Ad \"I think she's funny.\"<p><msg>c \"I'm not sure if that is a good idea.\"<d><scn>np2y<\n",
      "\n",
      "[ONNX] Reply: np2y<msg>Ad \"I guess it's not here anymore.\"<p><msg>c \"What do you think of Naomi?\"<d><scn>np2y<msg>Ad \"I think she's funny.\"<p><msg>c \"I'm not sure if that is a good idea.\"<d><scn>np2y<msg>Ad \"You're right, it is, but I just didn't give you any false hope.\"<p><msg>c \"What do you think of Remy?\"<d><scn>np2\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>anx \"I think this is off-the-eyontide good for me.\"<d><msg>c \"And I think this is off-the- prejudice of the people who should be saying that the rest of our civilization is a little different.\"<p><msg>c \"And I think this is off-the- prejudice of the people who should be saying that the rest of us are off to a very bad end.\"<d><scn>adineapt<msg>Ad \"And I think I'm not going to take any action at this point, because I think\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hello, [player_name].\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9ea4d-a30f-4c1a-a1f6-d5badff70453",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216177f-f03b-4358-8b5c-7eb0dc663af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3f1c-91c0-4585-92b6-fab86678834c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b6dcd-9723-4832-994e-c39411df2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight\",\n",
    "    \"Attack\"\n",
    "]\n",
    "\n",
    "for rp in test_rps:\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "print(\"Lowercase test\")\n",
    "\n",
    "for rp in test_rps:\n",
    "    rp = rp[0].lower() + rp[1:]\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    rp = rp.lower()\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c29882-e6cf-4172-9d8e-64f3c13f8f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

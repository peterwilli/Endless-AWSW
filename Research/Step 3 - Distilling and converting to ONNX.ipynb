{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98b2c15-46b2-4b0f-9069-68cfe40d8dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b7a9d-8687-4c2f-8278-6f83d9fabbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset, ModelSeeder\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "import logging\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b87e8-66eb-4779-89a5-a8d7aad4376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.10.1+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model outputs' name match reference model ({'present.10.value', 'present.3.key', 'present.0.key', 'present.4.key', 'present.7.value', 'present.5.value', 'logits', 'present.11.key', 'present.6.value', 'present.2.value', 'present.8.value', 'present.9.value', 'present.1.value', 'present.8.key', 'present.9.key', 'present.2.key', 'present.5.key', 'present.6.key', 'present.0.value', 'present.11.value', 'present.10.key', 'present.7.key', 'present.4.value', 'present.1.key', 'present.3.value'}\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 50257) matches (2, 1, 50257)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "All good, model saved at: models/awsw_onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_mixed\")\n",
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "    !cp -n $saved_model_path/gpt-neo-125M/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/gpt-neo-125M\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm-with-past $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fe1e14-e2a5-4fa9-a331-ae156a8967e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type = QuantType.QInt8)\n",
    "        #!rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3ae8b2-8518-4071-9e7b-bfdd52087a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model-opt.onnx\"))\n",
    "onnx_model_manager_quant = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "print(f\"Pretrained model loaded on {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6291ed-7153-4904-826b-fb64f88fdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I can't even imagine what I'd do with a stone like this. I'm sure it's not as if it's getting out of hand, but it's getting out of hand. I'd rather just wait and see what happened, okay?\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I've been doing that for so long that I don't think I can.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and hands. I can't even imagine what I would do if I had the opportunity to do something without getting caught.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\". I'm a dragoness. I'm a dragon's assistant.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I've got hands, feet, and hands. I could do without them.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and hands. I can't even imagine what I would do if I had the opportunity to do something without getting caught.\"< I have to admit that I'm not a fan of \"cunning\" or \"cunning\" or \"cunning\" or \"cunning\".\"<p><msg>c \"Don't be. I'm not going to let you win.\"<d><scn>black<msg>Br \"You're cute... I bet you'll get to know me some little better once you're here.\"<p><msg>c \"Yeah.\"<d><\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I can't even imagine what I'd do with this.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, and hands. I can't even imagine what I would do if I had the opportunity to do something without getting caught.\"<d><msg>c \"[[Close the door.]\"c \"I'll have to think about that one for a minute.\"<d><scmapt<msg] \"I'll think about it.\"<d><scmapt<msg>c \"I'll think about it.\"<d><scmapt><msg>c \"I'll think about it.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"This might be an unusual one to look at, but there are many things you can do to help me. All of those are so important that I want to see how you do.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I'm a dragon, I'm not sure. I guess I'm just a little different from other mammals we know.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I could hear the sound of it as I closed my eyes.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"<d>Ixcorridor<msg>Br \"[player_name]? What are you doing here?\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I can't even imagine what I'd do if I had the opportunity to meet one myself. I guess I can cross that one off the bucket list.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I've got hands, feet, hands, and feet. I'm in the middle of an experiment and I'm wondering what it means.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"<p><msg>c \"Yeah.\"<p, I don't think I could. Maybe we should leave the night behind to wait for you.\"<d><scn>black<msg>Ry \"I think I should go, just to get a few minutes of the best part about the evening.\"<p><msg>c \"I suppose you don't appreciate the end of it.\"<d><scn>black<msg>Ry \"I'm sorry, [black]. This is an extraordinary day.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I've got hands, feet, and hands. I could do it myself, but I was sure it would be best to just stay here.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm a dragon. I'm\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I'm a dragon, right?\"<d><msg>c \"I'm not sure, I am not a linguist. Are you?\"<d><msg>Cafe<msg><c \"Not really. I suppose you could say so.\"<d><msg>An \"Sure, but I'm not sure what to think.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I just have a lot on my mind.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(10):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"ONNX (Quantized):\", onnx_model_manager_quant.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9396c5-9837-4c77-9011-1ac48b711286",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c3279-83c5-4d81-b333-5d9450ad1a62",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cf23db-6013-4ff4-8a4f-effddd883295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I'm not sure, I am not a linguist. Are you?\"<d><scn>park2<msg>Ry \"Not really. I suppose you have a degree in biology, which is one of the reasons I was given the honor of coming here in the first place. I was sure you would be a good teacher, but you don't have the whole picture. We\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm not sure, I am not a linguist. Are you?\"<d><scn>park2<msg>Ry \"Not really. I suppose you have a degree in biology, which is one of the reasons I was given the honor of coming here in the first place. I was sure you would be a good teacher, but you don't have the whole picture. We don't know if you can just keep it up.\"<p><msg>c \"I'm not sure if it's worth mentioning, but I have known Reza for a couple of years, so I\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"I'm not sure, I am not a linguist. Are you?\"<d><scn>park2<msg>Ry \"I'm not sure, I am not a linguist. Are you?\"<p><msg>c \"I'm not sure, I am not a linguist. Are you?\"<d><scn>park2<msg>Ry \"I'm not sure, I am not a linguist. Are you?\"<p><msg>c \"I'm not sure, I am not a linguist. Are you?\"<d><scn>\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think he's a nice fellow.\"<p><msg>c \"I'm not sure if he's a particularly friendly one.\"<d><scn>park2<msg>Ad \"I see.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"I think he's a nice fellow.\"<p><msg>c \"I'm not sure if he's a particularly friendly one.\"<d><scn>park2<msg>Ad \"I see.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ad \"I think he's a nice change of mood.\"<p><msg>c \"I'm not sure what to say to that.\"<d><scn>park2<msg>Ad \"I don't really like them very much.\"<p><msg>c \"I'm not sure what to say to that.\"<d><scn>park2<msg>Ad \"I don't really like them very much.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: o2<msg>Ad \"It's the one that makes you drink. That's why I drink.\"<d><scn>o2<msg>Ad \"That's true. You can't win if you don't play.\"<p><msg>c \"I'm not sure if it's my kind of game.\"<d><scn>o2<msg>Ad \"It's a children\n",
      "\n",
      "[ONNX] Reply: o2<msg>Ad \"It's the one that makes you drink. That's why I drink.\"<d><scn>o2<msg>Ad \"That's true. You can't win if you don't play.\"<p><msg>c \"I'm not sure if it's my kind of game.\"<d><scn>o2<msg>Ad \"It's a children's game. These questions aren't really a challenge.\"<d><scn>o2<msg>Ad \"I'm not sure if it's a children's game, but I'll let you know.\"\n",
      "\n",
      "[ONNX Quantized] Reply: o2<msg>Ad \"It's a long story.\"<p><msg>c \"I don't mind.\"<d><scn>o2<msg>Ad \"I don't like where this is going.\"<p><msg>c \"I'm not sure what to think anymore.\"<d><scn>o2<msg>Ad \"I don't like where this is going.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: facin2<msg>An \"I don't know. I don't want to face her. I don't want to face her. I want to see what she's like.\"<p><msg>c \"I don't know. I don't want to face her. I want to see what she's like.\"<p><msg>c \"I don't know. I don't want to face her. I want to see\n",
      "\n",
      "[ONNX] Reply: facin2<msg>An \"I don't know. I don't want to face her. I don't want to face her. I want to see what she's like.\"<p><msg>c \"I don't know. I don't want to face her. I want to see what she's like.\"<p><msg>c \"I don't know. I don't want to face her. I want to see what she's like.\"<p><msg>c \"I don't know. I don't want to face her. I want to see what she's like.\"<p><\n",
      "\n",
      "[ONNX Quantized] Reply: facin2<msg>An \"We'll send you back to your world. We'll take a seat opposite of you.\"<d><scn>facin2<msg>An \"That's a shame.\"<p><msg>c \"What will we do now?\"<d><scn>facin2<msg>An \"We'll send you back to your world. We'll take a seat opposite of you.\"<d><scn>facin2<msg>An \"That's a shame.\"<|endoftext|>\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9ea4d-a30f-4c1a-a1f6-d5badff70453",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216177f-f03b-4358-8b5c-7eb0dc663af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"It's fine, I'll just eat your slice instead.\"<p><msg>c \"I'm not sure what to say to that.\"<d><scn>park2<msg>Ry \"I'm not sure if it's worth mentioning, but I have known Reza for a couple of years, so I know some things about his personality.\"<d><scn>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm not so lucky. I've had a few reports of Reza in the past, and I'm glad things are working out for you.\"<p><msg>c \"You're a lucky individual.\"<d><scn>park2<msg>Ry \"I'm sorry. I don't know how to say this.\"<d><scn>park2<msg>Ry \"I don't know. I just...\"<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hey!\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"I'll be right back.\"<d><scn>park2<msg>Ry \"Hey, [player_name], what are you talking about?\"<p><msg>c \"I'm not sure, but I think it's a joke.\"<d><scn>park2<msg>Ry \"I don’t think I can say no, not with my mouth and not in a rush.\"<p><msg>c \"I don’t think I could say no to an opportunity like that.\"<d><scn>park2<msg>Ry\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think I've heard of most of these before, but I'm not sure what the original human looked like.\"<p><msg>c \"I'm having a drinking contest with a dragon. How could I not love this?\"<d><scn>park2<msg>Ad \"I see.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"I like his personality.\"<p><msg>c \"You're right. I'm getting tired of Emera and the rumors that she wants to talk to me instead, but she can't do that.\"<d><scn>park2<msg>Ad \"I don’t think I have a choice in the matter.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ad \"Not bad.\"<p><msg>c \"I thought it was a shame you wanted to meet here.\"<d><scn>park2<msg>Ad \"I'm sure you'll make your way around to the deck a little more interesting. I'm not sure if you could just keep it up for a little while, or at least get some coffee into you.\"<d><scn>park2<msg>Ad \"I'll let you know. I'm sure you'll make sure of it.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: o2<msg>Ad \"It's the practice of doing flying maneuvers like rolls, spins, loops and rolls.\"<d><scn>o2<msg>Ad \"I practice just about anywhere, but today's a nice day for a beach visit. Water and sand are also good surfaces to practice complicated maneuvers on, in case you can't make the landing.\"<d><scn>o2<\n",
      "\n",
      "[ONNX] Reply: o2<msg>Ad \"Do you have something in mind about what I was going to say?\"<d><scn>o2<msg>Ad \"Well...\"<p><msg>c \"You know, if it was only me that evening, I could still go to the bar. I had to wait a few times to get this over. I was sure it would only be a couple of times.\"<d><scn>o2<msg>Ad \"That's a shame, but I wouldn’ t even say a word to you if I had a choice to make.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: o2<msg>Ad \"It's a long story.\"<p><msg>c \"I don't need a breath weapon at all.\"<d><scn>o2<msg>Ad \"I don't really drink, though.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: loremapt<msg>Ip \"We'll take a look at the overall system and see how it works.\"<d><scn>loremapt<msg>Ip \"In addition to the office, I also have my own setup here at home - I assure you, it’s properly isolated from the rest of our apartment. It even has a fume hood to prevent accidents!\"<d><scn>lorem\n",
      "\n",
      "[ONNX] Reply: facin2<msg>An \"I don’t know. I don't like to think about what would happen if you had just given up on life.\"<p><msg>c \"I'll just go ahead and draw the next two lines from the paper, right? To be fair it's a shame you can't make the landing, though.\"<d><scn>beach<msg>Ad \"I'll see you next day, then.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: facin2<msg>An \"The production process has been automated to a very high rate. It is a time traveller, and I just wanted to make sure you never miss anything.\"<p><msg>c \"I'm not sure if it's worth mentioning, but I've been there so long and learned the ins and outs of the world.\"<d><scn>facin2<msg>An \"It's not a big deal. I mean, I'm sure as heck I'm not going to let you win if you don't play.\"<p><msg>c \"I don't know.\"<\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3f1c-91c0-4585-92b6-fab86678834c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8b6dcd-9723-4832-994e-c39411df2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pytorch] Visit Lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] Visit Lorem -> loremapt<msg>Lo \"Hey, [player_name]!\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Lorem -> loremapt<msg>Lo \"Oh.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Meet with Lorem -> loremapt<msg>Lo \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] Meet with Lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX Quantized] Meet with Lorem -> loremapt<msg>Lo \"Oh, [player_name]! What are you doing here?\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Visit Adine -> adineapt<msg>Ad \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] Visit Adine -> adineapt<msg>Ad \"Oh. I see.\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Adine -> adineapt<msg>Ad \"Oh, I see.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Fight Maverick -> black<msg>m \"Maverick dodges my attack and comes rushing towards me.\"<|endoftext|>\n",
      "[ONNX] Fight Maverick -> o2<msg>m \"Maverick dodked the table and rose from the couch with one of his claws. Meanwhile, my gaze wandered to the horizon, where I could see the dragon's gaze. Was it still a little long?\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Maverick -> black<msg>m \"Maverick dodges my attack and comes rushing towards the table. I've been doing it for the better part.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Fight Adine -> cafe<msg>m \"Adine dodges my attack and comes rushing towards me.\"<d><scn>cafe<msg>m \"I'm not sure if I should go back to the library for this. I'll have to go back to the apartment to do it.\"<d><scn>cafe<msg>m \"I saw Adine in the library, and when I approached, I was surprised to see her staring at the ceiling. I wondered if she kept these items in the office for occasions just like\n",
      "[ONNX] Fight Adine -> cafe<msg>m \"I didn’t see her moving, and I could see her staring at the ceiling as she continued to work. I could see her staring at the ceiling, trying to make the landing.\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Adine -> beach<msg>m \"I didn’t want to be rude, you know.\"<d><scn>beach<msg>Adine barely avoids my attack and fell, but managed to get up and quickly punch me in the face, a soaring pain quickly came over my face\"<d><scn>beach<scn>beach<msg>m \"... I took a deep breath before I went in again. I extended my arm to grab Adine's neck and pulled her head back before I threw her head up.\"<d><scnmsg>m \"I could see her staring\n",
      "----------\n",
      "[Pytorch] Attack Adine -> beach<msg>m \"Adine dodges my attack and comes rushing towards me\"<|endoftext|>\n",
      "[ONNX] Attack Adine -> adineapt<msg>m \"She barely avoids me, and I quickly dove towards them.\"<|endoftext|>\n",
      "[ONNX Quantized] Attack Adine -> o2<msg>m \"Adine barely avoids further questioning from her, even though her publicity was no doubt a big deal.\"<|endoftext|>\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight Maverick\",\n",
    "    \"Fight Adine\",\n",
    "    \"Attack Adine\"\n",
    "]\n",
    "for rp in test_rps:\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f43a12-6e71-4ee1-af0a-66d41df0032c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

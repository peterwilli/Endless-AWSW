{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98b2c15-46b2-4b0f-9069-68cfe40d8dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b7a9d-8687-4c2f-8278-6f83d9fabbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "import logging\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b87e8-66eb-4779-89a5-a8d7aad4376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\n",
      "Cloning into 'gpt-neo-125M'...\n",
      "remote: Enumerating objects: 44, done.\u001b[K\n",
      "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 44 (delta 20), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (44/44), 543.14 KiB | 1.30 MiB/s, done.\n",
      "Using framework PyTorch: 1.10.1+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:555: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 8, 50257) matches (2, 8, 50257)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "All good, model saved at: models/awsw_onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_main\")\n",
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/$Config.base_model_name\n",
    "    !cp -n $saved_model_path/$Config.base_model_basename/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/$Config.base_model_basename\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm --atol=1e-03 $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fe1e14-e2a5-4fa9-a331-ae156a8967e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_102]\n",
      "Ignore MatMul due to non constant B: /[MatMul_133]\n",
      "Ignore MatMul due to non constant B: /[MatMul_235]\n",
      "Ignore MatMul due to non constant B: /[MatMul_266]\n",
      "Ignore MatMul due to non constant B: /[MatMul_368]\n",
      "Ignore MatMul due to non constant B: /[MatMul_399]\n",
      "Ignore MatMul due to non constant B: /[MatMul_501]\n",
      "Ignore MatMul due to non constant B: /[MatMul_532]\n",
      "Ignore MatMul due to non constant B: /[MatMul_634]\n",
      "Ignore MatMul due to non constant B: /[MatMul_665]\n",
      "Ignore MatMul due to non constant B: /[MatMul_767]\n",
      "Ignore MatMul due to non constant B: /[MatMul_798]\n",
      "Ignore MatMul due to non constant B: /[MatMul_900]\n",
      "Ignore MatMul due to non constant B: /[MatMul_931]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1033]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1064]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1166]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1197]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1299]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1330]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1432]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1463]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1565]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1596]\n"
     ]
    }
   ],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type = QuantType.QInt8)\n",
    "        #!rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3ae8b2-8518-4071-9e7b-bfdd52087a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_name = 'cpu'\n",
    "device = torch.device(device_name)\n",
    "\n",
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model-opt.onnx\"))\n",
    "onnx_model_manager_quant = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "print(f\"Pretrained model loaded on {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6291ed-7153-4904-826b-fb64f88fdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon.\"<p>#msg<d><msg<msg>c \"I see.\"<d><scn>black<msg>An \"I see.\"<d><msg>c \"I see.\"<p>#msg<msg<t><msg>Sb \"(I see.)\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. A little odd. I'm a little odd. A little odd. I'm a little odd. I'm a little odd. I'm a little odd. I'm a little odd. I'm a little odd.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. I have wings that are bigger than me, but they're a little different from mine. They look like they can fly, but they're still just a bit bigger than me.\"<p><msg>c \"What are you talking about?\"<d><scn>cafe<msg>An \"I don't think I can even say this one.\"<p><msg>c \"Go to park\"<d><scn>park1<msg>m \"I walked back to the park\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon.\"<d><Scn>blackx<msg>Nm \"I see.\"<d><scn>blackx<msg>Nm \"[player_name]? What are you doing here?\"<d><msg>Nm \"I was with Remy today\"<cite>np2x<msg>Nm \"{i}Remy! What are you doing here?\"<d><scn>blackx<msg \"I was with Remy today\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I was born with a dragon. I was a little odd. I was a little odd. A little odd. I was a little odd. I was a little odd. I was a little odd.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon. A dragon with a dragon heart, and a dragon face. I could die, but I would live.\"<d><scn>np2x<msg>Rz \"So, what is it?\"<p><msg>c \"What do you think of Anna?\"<d><scn>np2x<msg>Rz \"She's pretty ugly.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(2):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"ONNX (Quantized):\", onnx_model_manager_quant.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9396c5-9837-4c77-9011-1ac48b711286",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c3279-83c5-4d81-b333-5d9450ad1a62",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cf23db-6013-4ff4-8a4f-effddd883295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I'm fine.\"<p><msg>c \"I'm not sure if I can help you.\"<d><scn>park2<msg>Ry \"I'm not sure if I can help you.\"<p><msg>c \"I'm not sure if I can help you.\"<d><scn\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm fine.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <msg<p>c \"<d><msg<t><msg<p>c \"How are you?\"<d><msg<p>c \"Adine<d><scn>\"<d><scn><msg<p>c \"How are you?\"<d><scn><d><scn>park2<d><msg<p>c \"How are you?\"<d><scn>cad2<msg<p>c<p<msg<p>c \"How are you so far?\"<d>\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"He's a bit odd.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"He's a bit odd.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <p><msg<p><msg \"Adine\"<d><scn>Ad \"Adine \"\n",
      "<d><msg<t>Ad \"Beach\"<d><scn>adine<d><scn>adine<d>Ad \"Beach\"<d><scn>adine<adineapt<d>Ad \"Beach\"<p><msg<p>Ad \"Beach\"<d><scn>adine<d>beapt<adineapt<d>Ad \"Beach\"<p\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: adineapt<msg>Ad \"I was with Katsuharu today\"<p><msg>c \"I was with Katsuharu today\"<d><scn>adineapt<msg>Ad \"Very nice\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: adineapt<msg>Ad \"I was with Katsuharu today\"<p><msg>c \"I was with Katsuharu today\"<d><scn>adineapt<msg>Ad \"Very nice\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <msg \"Adine\"<d><scn>adineapt<msg \"Adine\"<d><scn>adine<msg \"Adine \"Bapt\"<p><msg \"Adineapt\"<dapt<2<msg \"Adine \"Adine\"<dapt<2<msgapt<adineapt<2pt<ineapt<2pt<<msgapt<adineapt<2pt<msgapt<dapt<ineapt<2.2<apt<apt<dapt<adapt<dapt<adine<punch\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: np1r<msg>Nm \"I don't know. I don't know what to do.\"<p><msg>c \"What do you think of Anna?\"<d><scn>np1r<msg>Nm \"I think she is ugly.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: np1r<msg>Nm \"I don't know. I don't know what to do.\"<p><msg>c \"What do you think of Anna?\"<d><scn>np1r<msg>Nm \"I think she is ugly.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>c \"Ad \"<|endoftext|>\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hello, [player_name].\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9ea4d-a30f-4c1a-a1f6-d5badff70453",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216177f-f03b-4358-8b5c-7eb0dc663af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"Very well.\"<p><msg>c \"Good luck.\"<d><scn>park2<msg>Ry \"Thank you, [player_name].\"<p><msg>c \"What do you think of Anna?\"<d><scn>park2<msg>Ry \"I think she is good\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm good.\"<p><msg>c \"I'm fine.\"<d><scn>park2<msg>Ry \"I see. I'll take a call later.\"<p><msg>c \"I'll see if I could get you a few things.\"<d><scn>park2<msg>Ry \"I'll be right there. I'll call you.\"<p><msg>c \"Hey, [scatterx sound]!\"<d><scn>park2<msg>Ry \"Hello, [player_\n",
      "\n",
      "[ONNX Quantized] Reply: <msg<p>c \"How are the rest of your actions.\"<d><scn><msg<p>c \"What are your other actions?\"<d><scn><msg<p>c \"What happens if we stop and take them?<paging><d><paging>c<msg<paging>Br \"Stop and take them in a second\"<d><paged<serfingers><msg<p>c2<msg<d>c \"How are the rest of the actions [player_?<leave]<datal\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think he is good looking.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"He's funny.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <p><msg<p><msg \"came overLAND\"<d><msg \"Ad \"B\"<sup \"B\"<d><msg> \"Ad \"Ad\"<d><scn.<p><2<d><msg<p>c \"B\"<d><scn><p><apt<adapter.<p><d><scn><p<d><d><scn>ad2<d><d><scn>ad2<d><scn>ad2<\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: np2x<msg>Ad \"I'm sorry, but I don't like this place. It's just a strange place.\"<p><msg>c \"I didn't think it was going to be so crowded.\"<d><scn>np2x<msg>Ad \"You think so?\"<p><msg>c \"I guess. This is what I do\n",
      "\n",
      "[ONNX] Reply: adineapt<msg>Ad \"I was with Katsuharu today. I was with Naomi right now.\"<p><msg>c \"What do they do?\"<d><scn>adineapt<msg>Ad \"They don't really have any rules, but if you ask me, they do, and that is a good thing.\"<p><msg>c \"What's the difference?\"<d><scn>adineapt<msg>Ad \"I'm not sure. I don't really know much about them.\"<p><msg>c \"What are they\n",
      "\n",
      "[ONNX Quantized] Reply: <msg \"At last, Adine.\"<p><msg \"At least Adine barely avoids a counter.<br><p><c>Adine<msg<tbody>Adine \"At least Adine barely avoids a counterattack.<p><d><scn>Adine<p>Adine<tbody<p>Ad \"Be at Ad.\"<p><msg \"At least Ad be at least not a counterattack.\"<p>Adineadapter<d>Adinead<p>be<p><beated<msg>Adinead \"\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: np1r<msg>Nm \"I'm not sure.\"<p><msg>c \"What do you think of Anna?\"<d><scn>np1r<msg>Nm \"She's a bit odd.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: np1r<msg>Nm \"I don't know. It could be a lot worse.\"<p><msg>c \"I suppose I could.\"<d><scn>np1r<msg>Nm \"I'm afraid I don't have much time for this. We'll go to the lab and find the solution.\"<p><msg>c \"What do you think of Anna?\"<d><scn>np1r<msg>Nm \"I don't like her.\"<p><msg>c \"I like her, too.\"<d><\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>c \"<p><msg<punch<tongue>\"<msg \">c \"<|endoftext|>\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3f1c-91c0-4585-92b6-fab86678834c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8b6dcd-9723-4832-994e-c39411df2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pytorch] Visit Lorem -> office<msg>Lo \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] Visit Lorem -> park1<msg>m \"I walked back to his apartment\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Lorem -> <msg<p><msg<p>c \"<d><msg<msg \"Visit\" \"Adine\"<d>Ad \"Visit\" [player_body]<p/msg \"Ad\"<d><scn<msg>Ad \"Visit [player/Ad {\"/leave \"<p><msg<d>Ad {\"/leave \"<p><t< dangerousine<msg>Km \"<msg \"ADExx\"<datalogadunch2<msg> \"Adine not fatal\"<tbody<pt>Ad \"Adine\"<private<\n",
      "----------\n",
      "[Pytorch] Meet with Lorem -> park2<msg>Mv \"I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] Meet with Lorem -> park2<msg>Mv \"Oh, [player_ name]! I didn’t think you would arrive so early.\"<|endoftext|>\n",
      "[ONNX Quantized] Meet with Lorem -> c<msg<msg<p><msg<p>c<msg<msg<p>c<msg<msg<p><msg<p>c \"<d><scn><d><scn>c<msg<p>m<p \"d\" \"And [player_name] is set\"<d><scn><d><msg<p><p><msg \"Adine<d>c \" \"<p><msg<p><msg<takine<dataline<datalinept<msg> \"<\n",
      "----------\n",
      "[Pytorch] Visit Adine -> adineapt<msg>Ad \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Visit Adine -> office<msg>Ad \"Hey [player- name]!\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Adine -> <msg<p><msg<p>c \"Ad \"Ad\"<d><msg> \"Ad \"Visit ad\"<d><msg<p>Ad \"Adine\"<dapt<msg>Ad \" \"Beach\"<punch<t>(\"Ad \"Adine\"<msg<p>Ad \"Beach \"Adine\"<punchapt<tbody<p>Adrian/Adapt<adapt<adapt<p stream<tapt<d><p stream<d<msg<p stream>Adad<aptadad<p stream\n",
      "----------\n",
      "[Pytorch] Fight -> np1n<msg>m \"I didn't hesitate and kicked him in the stomach\"<|endoftext|>\n",
      "[ONNX] Fight -> park1<msg>m \"I didn't hesitate, and my opponent fell, but managed barely to get up and quickly fell over the edge of a cliff\"<|endoftext|>\n",
      "[ONNX Quantized] Fight -> <msg<p><msg<msg<p>c \"Bade me to get it off\"<d><p><msg<p>c<msg<p>c \"<p><msg<t>c \"Adine\"<d><scn<msg<msg>Ad [\"Adine\"] \"Bade\"<punch>Ad \"Bade\"<d><scn.<t�m \"punch\"<p>Adrian<Tak<msg<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Attack -> office<msg>m \"I was with Zhong today\"<|endoftext|>\n",
      "[ONNX] Attack -> park1<msg>m \"I didn't hesitate and quickly kicked Sebastian right in the stomach\"<|endoftext|>\n",
      "[ONNX Quantized] Attack -> <p><msg<msg>c \"<d><msg<msg<d><tb><msg<p><msg<p second=\"<msg>c \"<d><scn><msg>c \"<p><msg<p><msg<p second=\"msg<pmsg>c \"Ademy<tb>Ad \"<msg<tb>Ademy<msg<pmsg><c>Ad \"<p><msg<pmsg<msg><msg<pmsg<msg<msg<d>Ademy<msg<p\n",
      "----------\n",
      "Lowercase test\n",
      "[Pytorch] visit Lorem -> town4<msg>m \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] visit Lorem -> loremapt<msg>Lo \"Hey, [player- name]!\"<|endoftext|>\n",
      "[ONNX Quantized] visit Lorem -> <msg<p><msg<p>c \"<msg<msg><d information<msg<p><?p?v=\"Nm \"<p><t><msg>c \"<p><msg<msg<p>c \"Not a little\"<d><msg<p><msg<p>c \"A little\"<d><msg<p><t><msg<p><msg<p><msg<p><c \"Adine\"<d><msg<p><msg<p>c \"Beach<d>c\n",
      "[Pytorch] visit lorem -> town2<msg>Rz \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] visit lorem -> loremapt<msg>Lo \"Hey [player_name].\"<p><msg>c \"Hey [player_name].\"<|endoftext|>\n",
      "[ONNX Quantized] visit lorem -> <msg<p>c \"<d><msg<p><msg \"<p>\"<msg \"cave\"<d>c<msg \"punch\"<d><msg<p>c \"punch\"<d><msg \"punch\"<skip<msg>punch<msg<msg| \"cave\" \"d<datal\"<pt<msg>Stop<msg<pt<d>datal<msg<datal<ptmo<dataline<p<msg>cave<ptas<dataline<ptmo<p\n",
      "----------\n",
      "[Pytorch] meet with Lorem -> park1<msg>Mv \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] meet with Lorem -> park1<msg>Mv \"Oh, [player_name], I didn't expect you to be so nice.\"<|endoftext|>\n",
      "[ONNX Quantized] meet with Lorem -> <msg<p>c \"Ad \"<p><msg<np>Ad \"Be at the meeting of the meeting\"<d><scn>Ad \"meet with the meeting of the meeting\"<d><scn>Ad \"meet with the meeting of the meeting of the meeting of the not-set<p><msg<p><msg<msg> \"Ad \"Not even a little.\"<d>< la<d><scn>Ad \"Meet the meeting of be the meeting of the set of [punch] and [punch].\"<p><msg\n",
      "[Pytorch] meet with lorem -> hallway<msg>Rz \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] meet with lorem -> loremapt<msg>Lo \"Oh, [remyplayer presentee].\"<p><msg>c \"meet with katsuro\"<d><scn>loremapt<msg>Kats \"What are these?\"<p><msg>c \"meet with kat\"<|endoftext|>\n",
      "[ONNX Quantized] meet with lorem -> <msg<p><msg<p><msg<msg<p>c \"Ad \"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] visit Adine -> park1<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] visit Adine -> adineapt<msg>Ad \"Hey [player_name]! What are you doing?\"<|endoftext|>\n",
      "[ONNX Quantized] visit Adine -> <msg<p><msg<p>c \"Ad \"Ad\"<t>ad (adine)<p><msg<msg<d>c \"Ad \"Ad \"Adine\"<d><scn<tbody<msg>Ad<msg<msg<pt><d><scn>Ad \"Ad \"<msgatory title<p/Ad \"Ad \" \"Ad \"Ad \" \"<pt \"<pt><|endoftext|>\n",
      "[Pytorch] visit adine -> adineapt<msg>Ad \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] visit adine -> adineapt<msg>Ad \"Hey [player_name]! I was with you a while ago.\"<|endoftext|>\n",
      "[ONNX Quantized] visit adine -> c \"And not even a little.\"<p><msg<msg>c \"<dna<msg>c \"Adine\"<d><scn<t>Ad \"Adine\"<scn>Ad {\"<msg>c \"Beach\"<p><msg<msg>c \"Ad \"Ad\"<d><scn<adine>Ad \"Beach<p>Ad \"Adine\"<p2><msg<msg<msg<msg<msg>c \"Adine not only to be on the top of a second. \"<punch>\n",
      "----------\n",
      "[Pytorch] fight -> park1<msg>m \"I didn't hesitate and kicked him in the stomach\"<|endoftext|>\n",
      "[ONNX] fight -> loremapt<msg>m \"I didn't hesitate and kicked Naomi right in the chest\"<|endoftext|>\n",
      "[ONNX Quantized] fight -> <msg<p><msg<p>c \"Nadu's not going to take over. He's not leaving, not leaving the city. The rest of the city is. The rest of the cities are.<p><msg<p><d>c \"<p><msg<t><msg<p>c \"<msg<p>c \"<p><msg<p>c \"Not only is the city's name\"<d><d><scn<p>cad<p<msg<p>c \"<p><msg<\n",
      "[Pytorch] fight -> padx<msg>m \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] fight -> park3<msg>m \"Oh, [player-name], I wasn’t expecting visitors.\"<|endoftext|>\n",
      "[ONNX Quantized] fight -> <msg<p><msg<p>c \"<d>\"<msg<p>c \"<d><scn><p second<msg<p/p \"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] attack -> o<msg>m \"Zhong\"<d><scn>o<msg>m \"Zhong\"<d><scn>o<msg>Zh \"Zhong\"<d><scn>o<msg>m \"Zhong\"<d><scn>o<msg>Zh \"Zhong\"<d><scn>o<msg>m \"Wtf was going on?\"<|endoftext|>\n",
      "[ONNX] attack -> park1<msg>m \"Oh, [player_name], I wasn't expecting visitors. I thought you would be interested in my work.\"<|endoftext|>\n",
      "[ONNX Quantized] attack -> <msg<p><msg<p>c \"Attack\"<d><msg<p>c \"Ad up\"<d><msg<p><msg<p>c \"Ad \"Ad \"<p><msg<t>adineapt<d><p \"Ad \"<pt \"<msg<t>Ad \"Adpeach\"<tapt<<msg> \"Adpe \"<pt \"<pt \"<tapt<Dapt<om<<<p>Ad \"Brogna\"<dapt<ine<private>Ad \"Ad \"Ad \"\n",
      "[Pytorch] attack -> loremapt<msg>Ad \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] attack -> park1<msg>m \"Sz \"Iz \"<|endoftext|>\n",
      "[ONNX Quantized] attack -> <msg<p><msg<p>c \"attack\"<d><scn><msg<p>c \"attack\"<d><msg<p>c \"Stop\"<d><msg<p>c \"Not a warning for the attack\"<p><d><scn>c<p>c \"attack\"<d><msg<p>c \"Stop\"<d><d><scn>c<p>c \"ademy<tfare>adine<msg>Ad \"Ademy makes a surprise attack\"<d>\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight\",\n",
    "    \"Attack\"\n",
    "]\n",
    "\n",
    "for rp in test_rps:\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "print(\"Lowercase test\")\n",
    "\n",
    "for rp in test_rps:\n",
    "    rp = rp[0].lower() + rp[1:]\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    rp = rp.lower()\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

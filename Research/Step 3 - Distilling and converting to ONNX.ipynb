{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98b2c15-46b2-4b0f-9069-68cfe40d8dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b7a9d-8687-4c2f-8278-6f83d9fabbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset, ModelSeeder\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "import logging\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b87e8-66eb-4779-89a5-a8d7aad4376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.10.1+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model outputs' name match reference model ({'present.3.value', 'present.10.value', 'present.6.value', 'present.7.key', 'present.0.key', 'present.2.value', 'present.7.value', 'present.2.key', 'present.4.value', 'present.10.key', 'present.1.value', 'present.6.key', 'present.1.key', 'present.4.key', 'present.11.key', 'present.3.key', 'present.5.key', 'present.8.key', 'present.8.value', 'logits', 'present.9.value', 'present.5.value', 'present.0.value', 'present.11.value', 'present.9.key'}\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 50257) matches (2, 1, 50257)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "All good, model saved at: models/awsw_onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_mixed\")\n",
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "    !cp -n $saved_model_path/gpt-neo-125M/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/gpt-neo-125M\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm-with-past $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fe1e14-e2a5-4fa9-a331-ae156a8967e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type = QuantType.QInt8)\n",
    "        #!rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3ae8b2-8518-4071-9e7b-bfdd52087a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model-opt.onnx\"))\n",
    "onnx_model_manager_quant = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "print(f\"Pretrained model loaded on {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6291ed-7153-4904-826b-fb64f88fdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon.\"<d><scn>np2x<msg>Mv \"Is that so? Why can't I just sit here and be quiet?\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon.\"<d><as far ancestor of yours, human.\"<p><msg>c \"Right.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(2):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"ONNX (Quantized):\", onnx_model_manager_quant.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9396c5-9837-4c77-9011-1ac48b711286",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c3279-83c5-4d81-b333-5d9450ad1a62",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cf23db-6013-4ff4-8a4f-effddd883295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I'm not sure if I can do that.\"<p><msg>c \"Don't be. Maybe you'd be stuck with me for a little while longer.\"<d><scn>park2<msg>Ry \"I'm sorry, [player_name].\"<p><msg>c \"Don't be. Maybe you'd be stuck with me\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm not sure if I can do that.\"<p><msg>c \"Don't be. Maybe you'd be stuck with me for a little while longer.\"<d><scn>park2<msg>Ry \"I'm sorry, [player_name].\"<p><msg>c \"Don't be. Maybe you'd be stuck with me for a little while longer.\"<d><scn>park2<msg>Ry \"I'm sorry, [player_name].\"<p><msg>c \"Don't be. Maybe you'd be stuck with me for\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"I'm not sure if I can do that.\"<p><msg>c \"It's not your fault, but your own. You can't just go around and say nothing, but you can't?\"<d><scn>park2<msg>Ry \"I don't know. I don't want to know.\"<p><msg>c \"It's your job, so I suppose you're doing it to get back at me for rejecting.\"<d><scn>park2<msg>Ry \"I'm not sure if I can do that.\"<p><msg\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ad \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more or less.\"<p><msg>c \"I think you'd better be more.\"<p><msg>c \"I think you'd better be more.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: np2x<msg>Ad \"I didn't kill anyone, I'm telling you!\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: np2x<msg>Ad \"I didn't kill anyone, I'm telling you!\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: np<msg>Ad \"I'll be right back.\"<d><scn>np<msg>Ad \"Here you go. Yours, [player_name].\"<p><msg>c \"I'll see you next time.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: facin2<msg>An \"I'll see if it is all true.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: facin2<msg>An \"I'll see if it is all true.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: facin3<msg>An \"I'll let you know when I see you.\"<|endoftext|>\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hello, [player_name].\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9ea4d-a30f-4c1a-a1f6-d5badff70453",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216177f-f03b-4358-8b5c-7eb0dc663af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I'm not sure if I can take it, but I think the council is probably a very busy person.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I guess you have a point.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"I'm not so sure about that. This isn'll probably be a while.\"<d><scn>park2<msg>Ry \"I'll be just as sure as you are.\"<d><scn>park2<msg>Ry \"I'll see you later, after all.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think it's more like aincerityojure experience.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"Bye.\"<p><msg>c \"I'll pass up.\"<d><scn>park2<msg>Ad \"I'll have a coffee.\"<p><msg>c \"I'll pass you a coffee \"I'll pass you a coffee \"c \"I'll pass you a coffee \"You can be found the other day \"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: loremapt<msg>Lo \"I don't like lil, though.\"<d><scn>loremapt<msg>Lo \"I think you'd better be more careful.\"<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"This isn't my apartment, then.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: np2<msg>Ad \"I'll try.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: np2x<msg>Ad \"Don's be silly.\"<p><msg>c \"This is serious.\"<p><msg>c \"I'll be back in a bit.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: np<msg>Ad \"I'll be right there.\"<d><scn>np<msg>Ad \"Here you go. It's yours.\"<d><scn>np<msg>Ad \"Here, I already told you everything I know about you.\"<p><msg>c \"I think so...\"<d><scn>np<msg>Ad \"You're going to be my roommate, then?\"<d><scn>np<msg>Ad \"Is it true or not?\"<d><scn>np<msg>Ad \"Yeah. I think you two are to open\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: alley<msg>An \"I'll have a spot in the middle of the building, and I'll be your partner until the end. We'll see if you can stay for a little while.\"<p><msg>c \"I'll just have to be sure.\"<p><msg>c \"I'll be sure.\"<p><msg>c \"I'll be sure.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: office<msg>Br \"Alright. I'll get you that ice scene, then. I can’t you, then?\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: facin3<msg>An \"I think we're playing this the wrong way, but I think the best is for us to relax for a while, and I think about this eventually.\"<p><msg>c \"I'll have to leave you to us, but I think I can't last any longer.\"<p><msg>c \"I'll see about it.\"<p><msg>c \"I'll see myself out.\"<p><msg>c \"I'll see myself out.\"<|endoftext|>\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3f1c-91c0-4585-92b6-fab86678834c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8b6dcd-9723-4832-994e-c39411df2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pytorch] Visit Lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Visit Lorem -> loremapt<msg>Lo \"Oh, [player_name]!\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Lorem -> loremapt<msg>Lo \"Hey, Loreza!\"<d><scn>loremapt<msg>Lo \"Hey, Lorem.\"<d><scn>loremapt<msg>Lo \"Here, what's up? You don't seem so thrilled about doing it with your arms.\"<p><msg>c \"I'll get your number.\"<p><msg>c \"Myth Sphere\"<d><scn>loremapt<msg>Lo \"Here are some actual Lorem-products from the Home I'm using to make them, really. I guess I'll try that\n",
      "----------\n",
      "[Pytorch] Meet with Lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Meet with Lorem -> loremapt<msg>Lo \"Hey [player_name]! Over here! I... am not sure if it’s you anymore, anyway; and if I don't find a job, I won't have another chance. I'll have another job soon, I think?\"<|endoftext|>\n",
      "[ONNX Quantized] Meet with Lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Visit Adine -> adineapt<msg>Ad \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] Visit Adine -> adineapt<msg>Ad \"Hey [player_name]! Adine? What are you doing?\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Adine -> adineapt<msg>Ad \"Yep!\"<p><msg>c \"Donut it!\"<d><scn>ad<msg>Ad \"Here you go. Your test results, black as a consequence.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Fight Maverick -> cafe<msg>Mv \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] Fight Maverick -> ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<\n",
      "[ONNX Quantized] Fight Maverick -> office<msg>Mv \"Oh, [player_ dying], then.\"<d><scn>office<msg>Mv \"You're right... I don't know what you're saying.\"<d><scn>office<msg>Mv \"You're right about that. I don't want so to go around making a scene.\"<d><scn>office<msg>Mv \"I know I'm not the one you're interested, then.\"<d><scn>office<msg>Mv \"Now, I'm curious to find out what it'll be. I'll tell\n",
      "----------\n",
      "[Pytorch] Fight Adine -> hallway<msg>m \"Adine dodges my attack first and dodges my attack first.\"<d><scn>hallway<msg>m \"My first instinct is to think about it, but the second is to you, really.\"<d><scn>hallway<msg>Ip \"My breath.\"<|endoftext|>\n",
      "[ONNX] Fight Adine -> adineapt<msg>m \"Adire dodhops for themselves, but I think the situation is better now than it's ever been.\"<p><msg>c \"If you're so.\"<p><msg>c \"I know.\"<p><msg>c \"I know.\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Adine -> adineapt<msg>Mv \"Oh, [player_name]),'t be the first, right?\"<d><scn>adineapt<msg>Mv \"You know... I think you need to have this conversation right now...\"<d><scn>adineapt<msg>Mv \"Don't be late, Ad.\"<d><scn>adineapt<msg>Mv \"Don't be late about it.\"<d><scn>adineapt<msg>Mv \"Just be right at now, [p><msg>c \"Don't be silly\n",
      "----------\n",
      "[Pytorch] Attack Adine -> adineapt<msg>m \"I didn't hesitate, but didn't hesitate to put a stop to it.\"<d><scn>adineapt<msg>Ad \"I think I'll come out from it.\"<p><msg>c \"Don't let me hold you up.\"<d><scn>adineapt<msg>Ad \"Alright, let's go.\"<|endoftext|>\n",
      "[ONNX] Attack Adine -> adineapt<msg>m \"Adine dodges my attack with an unusual motion.\"<d><scn>adine dodges my attack with a dull thud that knocked me off my time.\"<d><scn>adine dodges my attack with a dull thud that knocked her off the ground.\"<d><scn>adine dodges my attack with a dull thud that knocked her off the ground.\"<d><msg>c \"I'll pass.\"<|endoftext|>\n",
      "[ONNX Quantized] Attack Adine -> adine<msg>m \"Ruuuuuu<p><scn><p><msg=\"uuuu<p><msg>c \"Attack Adine\"<d><scn>adine<msg>m \"Remy \"I'll be back, and if they find any, there will be consequences.\"<p><msg>c \"If they don't want to, they'll have more to do with it than getting anything. Of it all at once.\"<p><msg><msg>c \"If they don't want to, they'll have more to their make.\"<d><scn>\n",
      "----------\n",
      "Lowercase test\n",
      "[Pytorch] visit Lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] visit Lorem -> loremapt<msg>Lo \"Oh, I think I've heard the word hawk before.\"<p><msg>c \"Don't get ahead of yourself. We haven't even started yet. We still inside.\"<p><msg>c \"I think I'll see ourselves out of that small corner of town.\"<p><msg>c \"Donut this way and we'll see soon enough.\"<|endoftext|>\n",
      "[ONNX Quantized] visit Lorem -> loremapt<msg>Lo \"Oh, [player_name]!\"<|endoftext|>\n",
      "[Pytorch] visit lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] visit lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX Quantized] visit lorem -> loremapt<msg>Lo \"Hey, [press]!\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] meet with Lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] meet with Lorem -> loremapt<msg>Lo \"Hey [player_p]! How is your apartment?\"<|endoftext|>\n",
      "[ONNX Quantized] meet with Lorem -> loremapt<msg>Lo \"There you are, [p><msg]!\"<|endoftext|>\n",
      "[Pytorch] meet with lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] meet with lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX Quantized] meet with lorem -> loremapt<msg>Lo \"Oh, [player_name], I didn't expect you to arrive at the portal to do so.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] visit Adine -> adineapt<msg>Ad \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] visit Adine -> adineapt<msg>Ad \"Hey [ad]!stros!stros! It’s going to explode!\"<|endoftext|>\n",
      "[ONNX Quantized] visit Adine -> adineapt<msg>Ad \"Hey, [adine? Are you doing all you can?\"<p><msg>c \"I'd rather just... I've a nice spot near the beach, really.\"<p><msg>c \"Maybe you should go home and take your sleep, really.\"<d><scn>c \"I'll keep your mind for you soon.\"<p><msg>c \"I'll be just a minute. Your's good to know.\"<p><msg>c \"If you're so confident, maybe we can be that much.\"<p><msg>c \"I\n",
      "[Pytorch] visit adine -> adineapt<msg>Ad \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] visit adine -> adineapt<msg>Ad \"Oh, [adine, you don't have to deal with the simple fact that she believes everything, but she does.\"<|endoftext|>\n",
      "[ONNX Quantized] visit adine -> adineapt<msg>Ad \"Hey [player_name]! How are you? I know you.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] fight Maverick -> ryce<msg>Mv \"Don't be the wisest person that ever lived, you know.\"<|endoftext|>\n",
      "[ONNX] fight Maverick -> cafe<msg>Mv \"Don't you worry about him.\"<d><scn>cafe<msg>Mv \"I know what's going on, but I'm just inside, and I'm still the one who's supposed to be your main job.\"<d><scn>cafe<msg>Mv \"I know what you think of me. I'd never be able to prove anything. I'm just doing my best.\"<d><scn>cafe<msg>Mv \"You never said that you would be working as a courier for a long, hard, flat world\n",
      "[ONNX Quantized] fight Maverick -> cafe<msg>Mv \"Don't be silly, [ player_name], I didn't think you'd be so cocky if we knew about this.\"<d><scn>cafe<msg>Mv \"Just look, I won't have a same.\"<d><scn>cafe<msg>Mv \"You'll be doing yourself no favor, no matter what I think about this.\"<p><msg>c \"I think about it.\"<p><msg>c \"I think so.\"<p><msg>c \"I think so!\"<p><msg>c\n",
      "[Pytorch] fight maverick -> hallway<msg>m \"Maver barely avoids my attack, crouingly for you, and get out of the city.\"<d><scn>hallway<msg>Mv \"I know you knew the steak was in the cabinet, and I took the knife to help you.\"<d><scn>hallway<msg>Mv \"Who knows how long this is going to be?\"<|endoftext|>\n",
      "[ONNX] fight maverick -> ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<d><scn>ryce<\n",
      "[ONNX Quantized] fight maverick -> ryce<msg>m \"Maver barely heard the first shot, but soon found the bullets, a dull waiting you.\"<d><scn>black<msg>m \"Soon, I heard bullets in your bullet, a spot near the wound, a piece of cake, and a crown outta the floor?\"<d><scn>black<msg>m \"For a few seconds, I took the first bite, a piece of cake, with chickenkins in the cabinet.\"<d><scn>black<msg>m \"By the way, you don't like steak meat. What happened to it?\"\n",
      "----------\n",
      "[Pytorch] fight Adine -> adineapt<msg>m \"Adine dodges the right thing, but the cart doesn't move, so I think it's starting.\"<d><scn>adineapt<msg>Ad \"I'll try it just for you.\"<p><msg>c \"I'll just have to be careful.\"<|endoftext|>\n",
      "[ONNX] fight Adine -> adineapt<msg>m \"I didn’t expect you to take away, but as you knew, you don't have to go now. I will kill any second-creatature hobbies you might have.\"<|endoftext|>\n",
      "[ONNX Quantized] fight Adine -> adine dod us \"Don't say I wasn't sure what I expected from you in this world, or what you may even think about your family?\"<p><msg>c \"I'm not sure, I wasn't so sure about what you'll think about it soon.\"<d><scn>cafe<msg>Ad \"I'm not sure what's so special to you so far.\"<|endoftext|>\n",
      "[Pytorch] fight adine -> adineapt<msg>m \"adine dodges my attack and comes towards me with a dull thud that knocked me off my back. I have no idea if it's worth mentioning, but I'm not sure if it's worth mentioning, but I'm not talking about it. I'm talking about myself.\"<|endoftext|>\n",
      "[ONNX] fight adine -> adine<msg>m \"I didn't hesitate to your advice and told you that no expense would be spared.\"<p><msg>c \"Don't worry about it.\"<p><msg>c \"I'm just saying, if you're going to spend so much more time with me than getting drunk and passing out, why not?\"<d><scn>black<msg>Rz \" I'll be long gone.\"<p><msg>c \"I suppose you don't have it, though.\"<p><msg>c \"I'd love to be at the studio, doing the usual thing with\n",
      "[ONNX Quantized] fight adine -> adine<msg>m \"I didn't hesitate to put my dirty dirty mouth on the floor's exterior, but after a moment of hesitation I took it, unable as if my drink is being thrown away by the people's watch?\"<d><scn>adineapt<msg>Ad \"I'll see you later, you owe it yourself?\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] attack Adine -> adine<msg>m \"I didn't hesitate to your attack, but managed to kill just by your stand.\"<d><scn>adine<msg>Mv \"You didn't kill first, did you?\"<|endoftext|>\n",
      "[ONNX] attack Adine -> adine<msg>m \"I didn't hesitate to your advice, but I think you're more of a monster.\"<d><scn>adine<msg>Ad \"I'll be back soon, you know. I'm still a working working working working, so don't expect for a few more hours yourself. You can't just go around making fun about it. You can't go wrong.\"<d><scn>adine<msg>Ad \"If you're playing that game, what are you going to do?\"<p><msg>c \"Leave it.\"<p><msg>c \"\n",
      "[ONNX Quantized] attack Adine -> adine<msg>m \"I didn’t hesitate to your offer, luckily I won't have to worry about it right now.\"<|endoftext|>\n",
      "[Pytorch] attack adine -> adine<msg>m \"adine dodges my attack and comes towards me, which I fell to the ground with a dull thud that knocked me off my first cake slice.\"<d><scn>adine dodges my attack and comes towards me, which I aimed at the lucky spot.\"<d><scn>adine dodges my attack and comes towards me, which I aimed at the lucky spot.\"<d><scn>adine dodges my attack and comes towards me, which I aimed at the lucky\n",
      "[ONNX] attack adine -> adine<msg>m \"I didn’t hesitate, but didn’ fear, and I think you should be aiming at.\"<d><scn>adine<msg>Ad \"Here you think?\"<d><scn>adine<msg>Ad \"I think you don​%%, so I think you should wait for this officer of yours.\"<d><scn>adine<msg>Ad \"I think so, Minister?\"<d><scn>adine<msg>Ad \"Yes, but that isn’t a reason for that, right? Well,\n",
      "[ONNX Quantized] attack adine -> adine<msg>m \"Adine dodges mine every wishes, so I took the stone.\"<d><scn>adine dodges mine every wishes, but I've been doing it for so long that I don't think I can.\"<p>c \"I'll just have to be careful.\"<|endoftext|>\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight Maverick\",\n",
    "    \"Fight Adine\",\n",
    "    \"Attack Adine\"\n",
    "]\n",
    "\n",
    "for rp in test_rps:\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "print(\"Lowercase test\")\n",
    "\n",
    "for rp in test_rps:\n",
    "    rp = rp[0].lower() + rp[1:]\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    rp = rp.lower()\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630c05c-d415-4b07-87a0-6659048443ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98b2c15-46b2-4b0f-9069-68cfe40d8dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b7a9d-8687-4c2f-8278-6f83d9fabbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset, ModelSeeder\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "import logging\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b87e8-66eb-4779-89a5-a8d7aad4376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\n",
      "Cloning into 'gpt-neo-125M'...\n",
      "remote: Enumerating objects: 44, done.\u001b[K\n",
      "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 44 (delta 20), reused 0 (delta 0)\u001b[K\n",
      "Unpacking objects: 100% (44/44), 543.14 KiB | 1.15 MiB/s, done.\n",
      "Using framework PyTorch: 1.10.1+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model outputs' name match reference model ({'present.5.key', 'present.9.key', 'present.4.key', 'present.7.key', 'present.11.value', 'present.8.key', 'present.0.key', 'present.10.value', 'present.5.value', 'present.6.value', 'present.0.value', 'present.11.key', 'present.1.key', 'present.2.value', 'present.10.key', 'present.3.value', 'present.8.value', 'present.1.value', 'present.7.value', 'present.3.key', 'present.9.value', 'present.4.value', 'present.6.key', 'logits', 'present.2.key'}\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 50257) matches (2, 1, 50257)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "All good, model saved at: models/awsw_onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_main\")\n",
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "    !cp -n $saved_model_path/gpt-neo-125M/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/gpt-neo-125M\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm-with-past $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fe1e14-e2a5-4fa9-a331-ae156a8967e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type = QuantType.QInt8)\n",
    "        #!rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3ae8b2-8518-4071-9e7b-bfdd52087a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model-opt.onnx\"))\n",
    "onnx_model_manager_quant = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "print(f\"Pretrained model loaded on {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6291ed-7153-4904-826b-fb64f88fdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon.\"<p><msg>Ry \"I see.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I even had a dream about a dragon that had a portal for all the time. I had no idea what they were going to do, and I had no idea what they were going to do. I had no idea what they were going to do, and I had no idea what they were going to do. I had no idea what they were going to do, and I had no idea what they were going to do, and I had no idea what they were going to do, and I had no idea what they were going to do, and I had no idea what they were going to do, and I had no idea\n",
      "PyTorch: In my dreams, I'm a dragon.\"<d><scn>black<msg>An \"The dragon has wings, and they fly in a wide arc, like a human. We see them in different colors, and the dragon is the strongest one.\"<p><msg>c \"Do you want to see some of the rest?\"<d><scn>black<msg>An \"Oh, come on, it's pretty obvious. I don't know how you do it.\"<p><msg>c \"Is it still kinda scary?\"<d><scn>black<msg\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon.\"<d><scnr>o4<msg>Lo \"I'm not sure if I can do that, but I'm not sure if I can do anything else.\"<d><scn>adineapt2<msg>Ad \"I'm not sure if I can do anything else.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I even had a dream that had to be repeated in order to get my attention.\"<p><msg>c \"If I had a chance, I would be the one who would show up.\"<d><scn>(m4, \"If I had a chance, I would be the one who would show up.\"<p><msg>c \"If I had a chance, I would be the one who would show up.\"<p><msg>c \"(And that's it.)\"<p-><d><msg>m \"It's not a very nice thing to be a\n",
      "PyTorch: In my dreams, I'm a dragon.\"<p><msg>c \"meet with maverick\"<d><scn>park3<msg>Mv \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(2):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"ONNX (Quantized):\", onnx_model_manager_quant.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9396c5-9837-4c77-9011-1ac48b711286",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c3279-83c5-4d81-b333-5d9450ad1a62",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cf23db-6013-4ff4-8a4f-effddd883295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I'm good.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm good.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>c \"I'm good.\"<p><msg>c \"And a little more. I think that's all we need.\"<p><msg>c \"If we don't get a little more, we might even end up with a whole town.\"<p><msg>c \"meet with maverick\"<d><scn>park3<msg>Mv \"If we had a better plan, we could all get a little more. We could even get a little more. We could even get a little more. We could even get a little more. We could even\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think he is a bit odd.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"I think he is a bit odd.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>Ad \"I think we should all be happy.\"<d><scn>l<msg>Lo \"I think we should all be happy.\"<p><msg>c \"I think we should all be happy.\"<p><msg>c \"I think we should all be happy.\"<d><scn>n?<p><msg>Lo \"I think we should all be happy.\"<p><msg>c \"I think we should all be happy\"<p><msg>Lo \"I think we should all be happy\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: adineapt<msg>Ad \"I'm not sure if I can help it, but I'm not sure if I can help it with the whole thing.\"<p><msg>c \"What do you think of Adine?\"<d><scn>adineapt<msg>Ad \"I think she's cute.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: adineapt<msg>Ad \"I'm not sure if I can help it, but I'm not sure if I can help it with the whole thing.\"<p><msg>c \"I'm not sure if I can help it with the whole thing.\"<d><scn>adineapt<msg>Ad \"I'm not sure if I can help it with the whole thing.\"<p><msg>c \"I'm not sure if I can help it with the whole thing.\"<d><scn>adineapt<msg>Ad \"I'm not sure if I can\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>m/k/20<p><msg>Ad \"And a few other things.\"<p><msg>c \"And a few other places.\"<p><msg>c \"And a few other places.\"<p><msg>c \"And a few other places.\"<p><msg>c \"And a few other places.\"<p><msg>c \"And a few other places.\"<p><msg>c \"And a few other places.\"<p><msg>c \"And a few other places.\"<p><msg>c \"\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: np2y<msg>m \"I was about to ask Anna if she wanted to meet me, but she didn't hesitate and left.\"<p><msg>c \"meet with reza\"<d><scn>park2<msg>Rz \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: np2y<msg>m \"I was about to ask Anna if she wanted to meet me, but she didn't hesitate and left.\"<p><msg>c \"meet with reza\"<d><scn>park2<msg>Rz \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: ?<p><msg>c \"If we start to get a little too involved, we might even get a little too late.\"<p><msg>c \"If we start to get a little too involved, we might even get a little too late.\"<p><msg>c \"If we start to get a little too involved, we might even get a little too late.\"<p><msg>c \"If we start to get a little too involved, we might even get a little overkill.\"<p><msg>c \"If we start to get a little too involved, we\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hello, [player_name].\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9ea4d-a30f-4c1a-a1f6-d5badff70453",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216177f-f03b-4358-8b5c-7eb0dc663af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"Hey [player_name]!\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm good.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>c \"I think so. How's that work out for a little while.\"<p><msg>c \"meet Kevin\"<d><scn> metadnal<msg>Lo \"K-k-o-n-o<d>n\"<p><msg>c \"And that's what we are all for now. You can't even start a game with me.\"<p><msg>c \"And that's what we're going through.\"<p><msg>c \"And that is all that we are going for.\"<d><msg\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ad \"I think he is nice.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ad \"What are you even doing here? You should get some rest.\"<p><msg>c \"I'm not really a doctor.\"<d><scn>park2<msg>Ad \"I don‘t know what you're talking about.\"<p><msg>c \"I'm sorry. You're not supposed to be in the mood for a medical treatment.\"<d><scn>park2<msg>Ad \"I don't know what you're doing, but if I were, you wouldn‘t mind if I went to the doctor.\"\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>Ad \"I think he is a lunarian. I think he is a lunarian, a non-lunar one. I think he is a lunarian.\"<p><msg>c \"If he has a point, why is that necessary. I think we should put him in a special room and see to it we get some good ideas, not to put a stop to it.\"<p><msg>c \"And what are you even allowed to think of?\"<p><msg>c \"No one can.\"<d><scn>nips<msg>Ad \"\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: adineapt<msg>Ad \"Hey [player_name]!\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: adineapt<msg>Ad \"Hey [remyanswering]!\"<p><msg>c \"Hey Maverick! How are you?\"<d><scn>adineapt<msg>Mv \"I'm fine.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: <msg>m/k/20<p><msg>Ad \"No offense, but I'm not a fan of the whole game.\"<p><msg>Ad \"It's not a game you can even see in the real world, but it is one that's going to get a little boring in a crowded city like this one. It's also not even a part of the game itself. I just had to make my point and get my mind off of it.\"<p><msg>c \"I think Adine is a little odd\"<p><msg>c \"Adine is a\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: adineapt<msg>Ad \"If I'm going to be a spy, I'd better be prepared to kill a dragon.\"<p><msg>c \"I don't think that's a good idea.\"<p><msg>c \"How do you think I could be of any use to you, if I don't know what you're talking about?\"<d><scn>adineapt<\n",
      "\n",
      "[ONNX] Reply: np2y<msg>m \"I looked at her, and I saw that she was still holding a piece of paper. I looked back to her. It had been a few weeks since I last saw her, and I had been so afraid of her. I hadn't wanted the moment like this, but now that we had met, I knew I had to do something.\"<d><scn>np2y<msg>m \"She was still wearing the same clothes, but her face was a different color. Her eyes looked a different way, and she was holding something that was a little larger than it actually\n",
      "\n",
      "[ONNX Quantized] Reply: ?<p><msg>c \"If we start to think about how to get around these laws as a result, we may start to get stuck at some point.\"<p><msg>c \"If we start thinking of something like this, we might get stuck at some point. We could start with a few people, and maybe even a few politicians.\"<p><msg>c \"What are we going to do then, anyway?\"<p><msg>c \"What are we going to do then?\"<d><scn>park3<msg>m \"At any time I could\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3f1c-91c0-4585-92b6-fab86678834c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8b6dcd-9723-4832-994e-c39411df2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pytorch] Visit Lorem -> loremapt<msg>Lo \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] Visit Lorem -> loremapt<msg>Lo \"Hey [reps]! Over here\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Lorem -> <msg>c \"Andrea\"<d><scn>lorem-ad \"adineum\",<p><|endoftext|>\n",
      "----------\n",
      "[Pytorch] Meet with Lorem -> park3<msg>Mv \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Meet with Lorem -> loremapt<msg>Lo \"Oh, [remy laptrap]! How are ya?\"<p><msg>c \"Oh, you're here already. I just wanted to talk to you about something.\"<d><scn>loremapt<msg>Lo \"I see, but I'm not sure if this is the right place for you to meet.\"<p><msg>c \"I'm not sure if I can help you with the paperwork.\"<d><scn>loremapt<msg>Lo \"I'm sorry, but I'm not sure if this is\n",
      "[ONNX Quantized] Meet with Lorem -> n<msg<v]<d><msg \"<p first=\"<msg>c \"Good to be on your side\"<d><msg> \"<p>\"<msg>c \"<fadeOut>\"<msg>c \"No, I can do it.\"<p><msg>c \"And if that makes it possible for me, I will get it for free. You can even get a little of that. You can even get some of it for a few years. You can even get even a little part in your system that is even more important than a few years of\n",
      "----------\n",
      "[Pytorch] Visit Adine -> adineapt<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] Visit Adine -> adineapt<msg>Ad \"Oh, [player_name], I was just leaving. I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Adine -> <msg>Ad \"Adine\"<p>Ad \"Adine.\"<p><msg>Ad \"And I will be happy.\"<p><msg>Ad \"I will be happier. I will be happy.\"<p><msg>Ad \"And I will be happy. I will be happy.\"<p><msg>Ad \"I will make my best.\"<p><msg>Ad \"If we can see our children in the future.\"<p><msg>Ad \"If we are prepared, we can see them in the future.\"<p><msg><p\n",
      "----------\n",
      "[Pytorch] Fight Maverick -> park1<msg>Mv \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] Fight Maverick -> park1<msg>Mv \"Hey, [player-pwn].\"<p><msg>c \"Hey, [ player-name ]! How you doing?\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Maverick -> <p><msg>c \"I think we need a little more speed in order for us to make our point.\"<d><msg>n \"If we were going to be a little slower, we had to get a few extra seconds. We even had to get as far as we could, for a time.\"<p><msg>c \"If we were going to be a little more speed, that's all that is required for a little speed\"<d><msg>olithic<msg>Ip \"No offense, but I can't even get a good kick on a regular kick. I\n",
      "----------\n",
      "[Pytorch] Fight Adine -> park1<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] Fight Adine -> adineapt<msg>Ad \"Oh, I didn't expect you to arrive at the festival like so many others.\"<p><msg>c \"That's a pretty good idea.\"<d><scn>adineapt<msg>Ad \"You can do that, though. You'll be fine.\"<p><msg>c \"What do you think of Adine?\"<d><scn>adineapt<msg>Ad \"She's a good girl.\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Adine -> <msg<msg>Ad \"<d><c>msg\"<d><msg<msg>Ad \"<d value=\"<p><msg>Ad \"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Attack Adine -> adineapt<msg>Ad \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Attack Adine -> adineapt<msg>Ad \"Oh, [player_name], I didn’ t even know you were in here.\"<|endoftext|>\n",
      "[ONNX Quantized] Attack Adine -> cafe<msg \"I'll put you in a few places, or I'll put you in a few places, and I'll put you in a few spots.\"<p><msg>c \"If we don't put our heads together, we could all get stuck in some sort of contest.\"<p><c role=\"present\" text=\"True\"<p><msg>c \"You could even make a point about that we are allowed to.\"<p><c role=\"ad\"<d><scn>loremapt<msg>m \"If I had a point, I would\n",
      "----------\n",
      "Lowercase test\n",
      "[Pytorch] visit Lorem -> loremapt<msg>Lo \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] visit Lorem -> loremapt<msg>Lo \"Oh. I'm not really here to talk about it, Lorem.\"<p><msg>c \"hello lorem<msg>n \"[player_name]? What are you doing here?\"<|endoftext|>\n",
      "[ONNX Quantized] visit Lorem -> <msg>loremapt<msg?tostring<msg>m/p?d=7?<p><msg>c \"What is it?\"<p><msg>c \"It would be a shame if people would start thinking of this as a joke.\"<p><msg>c \"And what are we going to do about it?\"<p><msg>c \"No. Just put a bullet in the wall.\"<d><p><msg>c \"If we are going to use the gun in the city, that's fine. But we are not going to\n",
      "[Pytorch] visit lorem -> loremapt<msg>Lo \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX] visit lorem -> loremapt<msg>Lo \"Oh, [player-name]! How are you?\"<|endoftext|>\n",
      "[ONNX Quantized] visit lorem -> player<msg>c \"If we can get the best chance we will.\"<p><msg>c \"If we can get the best chance we'll make the world a whole.\"<p><msg>c \"And what a deal it is. A whole that is a little too small for us to even be as important as a whole.\"<p><msg>c \"If we don't get the chance, we get the whole.\"<p><msg>c \"If we can make the world a whole.\"<p><msg>c \"If we can make it a little more,\n",
      "----------\n",
      "[Pytorch] meet with Lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] meet with Lorem -> loremapt<msg>Lo \"Oh, [player- name]? What are you doing here? I thought I heard you were on my way to meet with Lore.\"<|endoftext|>\n",
      "[ONNX Quantized] meet with Lorem -> cafe<msg<|endoftext|>\n",
      "[Pytorch] meet with lorem -> loremapt<msg>Lo \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] meet with lorem -> loremapt<msg>Lo \"Oh, [Lorem], that's not a very nice name.\"<|endoftext|>\n",
      "[ONNX Quantized] meet with lorem -> ?<p><msg<msg>c \"If I had a better idea, I would probably get all the rest of humanity.\"<p><msg>c \"If I had a better plan, I would be all over the place soon.\"<p><msg>c \"And if I even had to turn around in the end, I would even be able to get away.\"<d><msg>n \"If I had a better plan, and if I even had a better idea, and I had to turn around, I would probably be able to make a whole world whole, and a whole world\n",
      "----------\n",
      "[Pytorch] visit Adine -> adineapt<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] visit Adine -> adineapt<msg>Ad \"Oh, [remyplayerNumBER{0}], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX Quantized] visit Adine -> <msg>Ad \"visit kent ipsum\"<d><msg>Ad \"Good job, [player_name].\"<p><msg>c \"No. You are a failure of art.\"<p><msg>c \"And that's all it's going to take. You are a public failure, a failure of art, a failure of humanity, and a failure to prevent a public from entering your city. You are an un-possible failure.\"<p><msg>c \"If we can see the light, I can see what we are doing.\"<p><\n",
      "[Pytorch] visit adine -> adineapt<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] visit adine -> adineapt<msg>Ad \"Oh, [visit adine], I didn't know you wanted to meet me.\"<|endoftext|>\n",
      "[ONNX Quantized] visit adine -> <msg>Ad \"adine<msg>?<d><msg>c \"adine\"<d><msg<|endoftext|>\n",
      "----------\n",
      "[Pytorch] fight Maverick -> park1<msg>Mv \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] fight Maverick -> park3<msg>Mv \"I'm not going anywhere, and I'm not leaving until we both get back to our jobs.\"<|endoftext|>\n",
      "[ONNX Quantized] fight Maverick -> ?<d><msg>m \"<p/msg><m>\"<d><scn>?<p><msg>c \"Maverick? Is that so?\"<p><msg>c \"Maverick\"<d><scn>?<p><msg>c \"Maverick\"<d><msg>m \"<p/a><msg>m \"<p><msg>m \"<|endoftext|>\n",
      "[Pytorch] fight maverick -> park3<msg>Mv \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] fight maverick -> park3<msg>Mv \"Hey [player_name]! How you doing, buddy?\"<|endoftext|>\n",
      "[ONNX Quantized] fight maverick -> <msg<msg>Ad \"Maverick\"<p><msg>Ad \"And that's it. You can even make your point, even when you don't even know what you are talking to.\"<p><msg>c \"If we start talking about something that is a joke, it's a rather odd one.\"<p><msg>c \"And even if we start talking about something that is a rather odd thing, it's not even going to be funny.\"<d><msg>Lo \"I think that would be a bad idea if we start talking about something that's a joke\n",
      "----------\n",
      "[Pytorch] fight Adine -> park2<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] fight Adine -> adineapt<msg>Ad \"Hey [player_name]! How were you today, [player_number]? What's up with that?\"<|endoftext|>\n",
      "[ONNX Quantized] fight Adine -> cave<msg> ad \"Adn\"<d><msg>Ad \"Ad\"<d><msg>Ad \"Ad \"<|endoftext|>\n",
      "[Pytorch] fight adine -> park2<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] fight adine -> adineapt<msg>Ad \"Hey [player_ name]! How are you?\"<|endoftext|>\n",
      "[ONNX Quantized] fight adine -> <p><msg>c \"If I had a plan, would I even be able to do anything?\"<p><msg>c \"If I had a plan, would I even be able...<p><msg>c \"If I had a plan, would I even be able to do anything.\"<p><msg>c \"(If I had a plan, would I even be able to do anything.)\"<d-><msg>Br \"If I had a plan, would I even be able to do anything.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] attack Adine -> adineapt<msg>Ad \"What? What are you talking about?\"<p><msg>c \"attack Adine\"<d><scn>adineapt<msg>Ad \"I thought you wanted to get to know me better, then.\"<p><msg>c \"adine? is that you?\"<d><scn>adineapt<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] attack Adine -> adineapt<msg>Ad \"Oh, you mean the police?\"<p><msg>c \"I mean the dragons. I mean the people who are supposed to be the guardians.\"<d><scn>adineapt<msg>Ad \"I see.\"<p><msg>c \"visiting your parents.\"<d><scn>adineapt<msg>Ad \"visit maverick\"<d><scn>park1<msg>Mv \"Hey [player_name]!\"<|endoftext|>\n",
      "[ONNX Quantized] attack Adine -> ?<p><msg>c \"adine\"<d><msg>Ad \"Ad \"B\"\"<d><msg>Ad \"ad \"B\"\"<p><msg>Ad \"Ad \"B\"\"<d><msg>Ad \"Ad \"B\"\"<p><msg>Ad \" \"Ad \"B \"B\" \"B\"<d><se>\"<msg>Ad \"Ad\"<p><msg>Ad \" \" \"<p><msg>Ad \"[aid3] \" \" \" \" \" \" \" \" \" \" \"\n",
      "[Pytorch] attack adine -> adineapt<msg>Ad \"Hey [player_name]! How are you?\"<|endoftext|>\n",
      "[ONNX] attack adine -> adineapt<msg>Ad \"Oh, [adinexaminer], I didn't expect you would arrive so early.\"<|endoftext|>\n",
      "[ONNX Quantized] attack adine -> ?<p><msg>c \"If I had a plan or set out a plan, I would certainly get a few bruises. I would probably die of boredom.\"<d><msg>m \"I met a rather odd person, and after I had met a few people in a town that had a very odd population. I had a number who had a very odd social agenda. I even met one of them, who is a rather ugly little girl, who had a very odd social agenda, who had a very odd personality, and who had a very odd manner of talking. All these people had to do was to\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight Maverick\",\n",
    "    \"Fight Adine\",\n",
    "    \"Attack Adine\"\n",
    "]\n",
    "\n",
    "for rp in test_rps:\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "print(\"Lowercase test\")\n",
    "\n",
    "for rp in test_rps:\n",
    "    rp = rp[0].lower() + rp[1:]\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    rp = rp.lower()\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

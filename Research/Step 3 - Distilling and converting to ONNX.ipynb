{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98b2c15-46b2-4b0f-9069-68cfe40d8dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion to ONNX\n",
    "ONNX is a different format for running machine learning models. The ONNX format is much faster on CPU, sometimes 5 times as fast as PyTorch!\n",
    "\n",
    "While the EAWSW model is designed to be small, accurate and accessible, for some people it's still too much to run...\n",
    "\n",
    "Hosting the model as a free service for players is an option. An ONNX version of the model allows us to host the model on CPU yet have faster response times! Given that the model is made in a time with chip shortage, running on hardware I already have inside a server is efficient, scalable and cheaper.\n",
    "\n",
    "An important note is that ONNX doesn't execute logic by itself, and you have to do that yourself, `onnx_model_manager.py` intends to deal with this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b7a9d-8687-4c2f-8278-6f83d9fabbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_utils import train_model, split_data, split_branches, get_model, set_pretrained_model_dropout, get_dataset, ModelSeeder\n",
    "from config import Config\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import onnx\n",
    "import logging\n",
    "from onnx_model_manager import OnnxModelManager\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_manager import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b87e8-66eb-4779-89a5-a8d7aad4376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.10.1+cu113\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model outputs' name match reference model ({'present.7.value', 'present.10.key', 'present.2.value', 'present.8.key', 'present.10.value', 'present.2.key', 'present.6.value', 'present.0.key', 'present.4.value', 'present.3.key', 'present.9.value', 'present.5.value', 'present.6.key', 'present.0.value', 'present.11.value', 'present.1.value', 'present.4.key', 'present.8.value', 'present.7.key', 'present.1.key', 'present.5.key', 'logits', 'present.11.key', 'present.9.key', 'present.3.value'}\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 50257) matches (2, 1, 50257)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.0.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.1.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.2.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.3.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.4.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.5.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.6.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.7.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.8.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.9.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.10.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.key\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "\t- Validating ONNX Model output \"present.11.value\":\n",
      "\t\t-[✓] (2, 12, 2, 64) matches (2, 12, 2, 64)\n",
      "\t\t-[✓] all values close (atol: 0.0001)\n",
      "All good, model saved at: models/awsw_onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = os.path.join(\"models\", \"awsw_mixed\")\n",
    "saved_model_onnx_path = os.path.join(\"models\", \"awsw_onnx\")\n",
    "if not os.path.exists(os.path.join(saved_model_path, \"special_tokens_map.json\")):\n",
    "    print(\"Copying config files from huggingface (needed for conversion)... WARNING: this assumes the structure of the model isn't changed!\")\n",
    "    !cd $saved_model_path && git clone https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "    !cp -n $saved_model_path/gpt-neo-125M/* $saved_model_path\n",
    "    !rm -rf $saved_model_path/gpt-neo-125M\n",
    "if not os.path.exists(os.path.join(saved_model_onnx_path, \"model.onnx\")):\n",
    "    !python3 -m transformers.onnx --model=$saved_model_path --feature=causal-lm-with-past $saved_model_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fe1e14-e2a5-4fa9-a331-ae156a8967e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_onnx():\n",
    "    model_quant = os.path.join(saved_model_onnx_path, \"model_quant.onnx\")\n",
    "    if not os.path.exists(model_quant):\n",
    "        model_fp32 = os.path.join(saved_model_onnx_path, \"model.onnx\")\n",
    "        model_opt = os.path.join(saved_model_onnx_path, \"model-opt.onnx\")\n",
    "        quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type = QuantType.QInt8)\n",
    "        #!rm $model_opt\n",
    "optimize_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3ae8b2-8518-4071-9e7b-bfdd52087a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "onnx_model_manager = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model-opt.onnx\"))\n",
    "onnx_model_manager_quant = OnnxModelManager(os.path.join(saved_model_onnx_path, \"model_quant.onnx\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_manager = ModelManager(model=model, tokenizer=tokenizer, device=device)\n",
    "print(f\"Pretrained model loaded on {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6291ed-7153-4904-826b-fb64f88fdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: In my dreams, I'm a dragon, and I'm a dragoness. I'm a dragoness, and I'm a dragoness, but I'm not sure what exactly I'll do. I'll just show you some of the fun I'll have.\"<|endoftext|>\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I can't stand or walk like a human, but I can certainly see that.\"I'm not a monster in the way you would think I'm a monster.\"<|endoftext|>\n",
      "PyTorch: In my dreams, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon, I'm a dragon\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ONNX: In my dreams, I'm a dragon, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a dragoness, and I'm a dragoness. I'm a\n",
      "ONNX (Quantized): In my dreams, I'm a dragon. I can't stand or walk like a human, but I can certainly see how they do it. I'd love to see what they do with my blood.\"I know they do. They wouldn’t need to know of my Paledo after I gave them the chance, but I certainly will know about it. You can find the portal in the portal’s main door. I couldn’t think of any other portal in history, though. It certainly doesn’re not like the rest. It wouldn’t be so adventurous if we got that far. Are we going to be able to find\n",
      "PyTorch: In my dreams, I'm a dragon, and I'm a dragon, too.\"<|endoftext|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In my dreams, I'm a dragon\"\n",
    "for i in range(2):\n",
    "    print(\"ONNX:\", onnx_model_manager.say_raw(prompt, do_sample=True))\n",
    "    print(\"ONNX (Quantized):\", onnx_model_manager_quant.say_raw(prompt, do_sample=True))\n",
    "    print(\"PyTorch:\", model_manager.say_raw(prompt, 50, 0.7))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9396c5-9837-4c77-9011-1ac48b711286",
   "metadata": {
    "id": "unxN7nYd2gOM",
    "tags": []
   },
   "source": [
    "# Testing\n",
    "\n",
    "We created a few past (for context) + present prompts (player input) and see the different reactions. This way, we can test the models across different iterations.\n",
    "The first test involves a old prompt to compare the pre-trained model with the one trained on AWSW. Did it manage to store it's data well? Is it able to write down things that have nothing to do with AWSW? (So we know we didn't overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c3279-83c5-4d81-b333-5d9450ad1a62",
   "metadata": {},
   "source": [
    "**This test generates boring and repetetive** replies! It's because we use no good sampling algorithm, but it does give us a indication of what the model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01cf23db-6013-4ff4-8a4f-effddd883295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"Well, I'm not sure if there is anything that can fill the void.\"<p><msg>c \"I think I'll have to see if I can make it up.\"<d><scn>park2<msg>Ry \"I think I'll have to see if I can make it up.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"Well, I'm not sure if there is anything that can fill the void.\"<p><msg>c \"I think I'll have to see if I can make it up.\"<d><scn>park2<msg>Ry \"I think I'll have to see if I can make it up.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"I'm not sure. I'll have to see if you can keep it up.\"<p><msg>c \"I'll see myself out.\"<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hey.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I think he was right to think that I should consider him a friend.\"<p><msg>c \"I was with him for a long time, but I think he was right to think that I should consider him a friend.\"<p><msg>c \"I was with him for a long time, but I think he was right to\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I think he was right to think that I should consider him a friend.\"<p><msg>c \"I was with him for a long time, but I think he was right to think that I should consider him a friend.\"<p><msg>c \"I was with him for a long time, but I think he was right to think that I should consider him a friend.\"<p><msg>c \"I was with him for a long time, but I think he was right to think that I should consider him a friend.\"<p><msg>c \"I was with\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"I think he's in the wrong world. He's trying to get into our world, but we have a lot of ideas about it.\"<p><msg>c \"I'm not sure what to think about it.\"<d><scn>park2<msg>Ry \"It's not as if I was trying to save anyone.\"<p><msg>c \"I was trying to figure out a good game before.\"<d><scn>park2<msg>Ry \"Oh, I know what you mean. I'm not sure what to think about it.\"<p\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: o2<msg>Ad \"It's the portal that's the problem.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: o2<msg>Ad \"It's the portal that's the problem.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: o2<msg>Ad \"It's a shame you can't be with them now, then.\"<p><msg>c \"I'll have to see if I can make it up.\"<d><scn>o2<msg>Ad \"I'll see you next time.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: o2<msg>Br \"I don't know. Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to\n",
      "\n",
      "[ONNX] Reply: o2<msg>Br \"I don't know. Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go with them.\"<p><msg>c \"Maybe we can find something else to go\n",
      "\n",
      "[ONNX Quantized] Reply: o2<msg>c \"I'll have to know before I start.\"<p><msg>c \"I'll keep your number, I think, since you keep asking.\"<d><scn>o2<msg>c \"I'll keep your number, I think, since you keep asking.\"<|endoftext|>\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    ('<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Hey!\"', \"How are you?\"),\n",
    "    ('<p><msg>c \"I was with Lorem today.\"<d><scn>park2<msg>Ad \"Very nice.\"', \"What do you think of Lorem?\"),\n",
    "    ('<p><msg>m \"In Tatsu park, Adine and I sat down.\"', \"Oh my god, Adine. What is this?\"),\n",
    "    ('<p><msg>m \"I sat down on a chair in Anna\\'s lab.\"', \"What will we do here?\"),\n",
    "]\n",
    "\n",
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9ea4d-a30f-4c1a-a1f6-d5badff70453",
   "metadata": {},
   "source": [
    "# Sampling test\n",
    "\n",
    "This is gonna be interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9216177f-f03b-4358-8b5c-7eb0dc663af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How are you?\n",
      "[Pytorch] Reply: park2<msg>Ry \"Well, I'm not sure if there is anything special about it.\"<d><scn>park2<msg>Ry \"I'm not sure about that. I suppose it's just a souvenir for when I get back home.\"<p><msg>c \"I'll keep that in mind.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"Well.\"<p><msg>c \"Hey, what's up?\"<d><scn>park2<msg>Ry \"I'm having a drinking contest with a few people. They think I'm going to be the one that's supposed.\"<d><scn>park2<msg>Ry \"I think I'll see if I can make it. I'll be right back.\"<d><scn>park2<msg>Ry \"I'm not so sure about it. I suppose you have your own opinion.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"I'm not sure, but you know what? It doesn't matter what you thought of it. You're right.\"<p><msg>c \"I'm not surprised. I'm not surprised by the fact.\"<d><scn>park2<msg>Ry \"You know what, maybe you should do something else.\"<p><msg>c \"Hey Remy!\"<d><scn>park2<msg>Ry \"Oh yeah, I do have a few steaks I could give you.\"<|endoftext|>\n",
      "\n",
      "----------\n",
      "Prompt: What do you think of Lorem?\n",
      "[Pytorch] Reply: park2<msg>Ry \"I think he was just trying to be friendly.\"<p><msg>c \"I thought he was just trying to be friendly.\"<p><msg>c \"I thought he was just trying to be friendly.\"<p><msg>c \"I thought he was just trying to be friendly.\"<p><msg>c \"I thought\n",
      "\n",
      "[ONNX] Reply: park2<msg>Ry \"I'm not sure. It would be best if you just told him you met him again, that's all.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: park2<msg>Ry \"You say that you don't know anything, but I do. They have a lot of information and we've heard of their amazing creatures.\"<p><msg>c \"Oh. I see.\"<p><msg>c \"I'm not sure I have a good idea of what might be coming here, but I'm not sure what you can say to that.\"<d><scn>park2<msg>Ry \"You know what I mean. I'm sorry I haven't done anything in the game after you've been gone, but I'm not sure what's so special for\n",
      "\n",
      "----------\n",
      "Prompt: Oh my god, Adine. What is this?\n",
      "[Pytorch] Reply: o2<msg>Ad \"You know what I mean.\"<p><msg>c \"I'm not sure, I am not a linguist. Are you?\"<d><scn>o2<msg>Ad \"Not exactly, but I have a good point.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: o2<msg>Ad \"I think it would be best to call you a friend or something.\"<p><msg>c \"What would you do if I told you I'm the one who's supposed at the portal now, right?\"<d><scn>o2<msg>Ad \"I don't know what's go on.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: o2<msg>Ad \"It is a long, complicated room that has to be prepared for the sheer magnitude of the solar flare that is headed for Earth.\"<p><msg>c \"The solar flare affect your body modifications and your mind, and your DNA. Don't be so surprised to find out that you still thought you could win this game, though. You know, this won.\"<p><msg>c \"I'm not surprised by your skill in this game, but I'm not sure I'd ever want to be. I just want you to know about it.\"<p><msg>c \"I\n",
      "\n",
      "----------\n",
      "Prompt: What will we do here?\n",
      "[Pytorch] Reply: o2<msg>Br \"I don't know. You have no idea what you're talking about.\"<p><msg>c \"I know. I'm doing what I can, but that doesn't mean it's making the situation any easier.\"<d><scn>o2<msg>Br \"I'm sorry, Adine. I don't know how to say this.\"<|endoftext|>\n",
      "\n",
      "[ONNX] Reply: o2<msg>Br \"This morning's night. I awoke with the vivid dreams of those I have now.\"<p><msg>c \"You're right about that.\"<p><msg>c \"You're right about that.\"<|endoftext|>\n",
      "\n",
      "[ONNX Quantized] Reply: o2<msg>c \"We could go back to what happened and figure out what you're talking about.\"<d><scn>o2<msg>c \"We could go back to what happened back then.\"<p><msg>c \"What happened to you after that?\"<d><scn>o2<msg>c \"It's nothing. I'm just serious.\"<p><msg>c \"I can't stand him. You're his best bet, you can win.\"<d><scn>o2<msg>c \"I can't stand him. You're my best\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for (past, prompt) in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    reply = model_manager.say(past, prompt, top_k = 50, top_p = 0.7)\n",
    "    print(f\"[Pytorch] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX] Reply: {reply}\\n\")\n",
    "    reply = onnx_model_manager_quant.say(past, prompt, do_sample = True)\n",
    "    print(f\"[ONNX Quantized] Reply: {reply}\\n\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3f1c-91c0-4585-92b6-fab86678834c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP test\n",
    "Testing out the injected roleplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba8b6dcd-9723-4832-994e-c39411df2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pytorch] Visit Lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Visit Lorem -> loremapt<msg>Lo \"Oh, [Where is it?\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Lorem -> loremapt<msg>Lo \"Oh. I see you today. You know, we're already here.\"<d><scn>loremapt<msg>Lo \"I'm afraid you'll be stuck with us a little longer than you're here, and you know what? It's too much to be that hard.\"<p><msg>c \"I'll see myself out, we'll be back in about a day.\"<d><scn>loremapt<msg>Lo \"I'm not sure if that's what I'm talking about right now. I'll see you next time.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Meet with Lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Meet with Lorem -> loremapt<msg>Lo \"Hey [Hey, [Hey, Lore. Are you sure?\"<|endoftext|>\n",
      "[ONNX Quantized] Meet with Lorem -> loremapt<msg>Lo \"Oh. Maybe this wasn't such a good idea after all.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Visit Adine -> adineapt<msg>Ad \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Visit Adine -> adineapt<msg>Ad \"Oh, [player], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX Quantized] Visit Adine -> adineapt<msg>Ad \"I thought you might want to know about Adine's research?\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Fight Maverick -> np2y<msg>m \"I didn't hesitate and kicked Maverick right in the stomach.\"<|endoftext|>\n",
      "[ONNX] Fight Maverick -> o2<msg>m \"The dragon moved and let out a groan before he opened his eyes.\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Maverick -> black<msg>m \"He took a few easy moves, and managed to kill me, but managed not to stand a second in the same direction as him did, as I took the same mistake I did.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Fight Adine -> adineapt<msg>m \"Adine dodges the attack and comes towards you with a bit of an idea.\"<p><msg>c \"I'm not sure what to think about it.\"<p><msg>c \"I think you're right.\"<p><msg>c \"I think you're right.\"<p><msg>c \"I think you're right.\"<|endoftext|>\n",
      "[ONNX] Fight Adine -> adineapt<msg>m \"Adine dodges my arms to the wall, trying to stay in their cover until I've been out of their room for a few seconds.\"<|endoftext|>\n",
      "[ONNX Quantized] Fight Adine -> adineapt<msg>m \"I didn't know whether to lose my bet, or to get the message in the first place.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] Attack Adine -> adineapt<msg>Ad \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] Attack Adine -> adineapt<msg>m \"I didn't hesitate and kicked Adine right in her stomach. It took just a moment before it even happening, and I was only just about to start on his own.\"<|endoftext|>\n",
      "[ONNX Quantized] Attack Adine -> adineapt<msg>m \"Adines barely, but took much longer to get it and took more time with you this time. You could say that I'm not a huge deal, but I certainly don't know about you. You don't have any idea what you're doing.\"<|endoftext|>\n",
      "----------\n",
      "Lowercase test\n",
      "[Pytorch] visit Lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] visit Lorem -> loremapt<msg>Lo \"Oh, [player_remyapt], I thought you might not want to see my experiments again.\"<|endoftext|>\n",
      "[ONNX Quantized] visit Lorem -> loremapt<msg>Lo \"Oh, I see, but I'm here to order anything I need, not to threaten you or anything.\"<|endoftext|>\n",
      "[Pytorch] visit lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] visit lorem -> loremapt<msg>Lo \"Hey, you two! It's me, Sebastian!\"<|endoftext|>\n",
      "[ONNX Quantized] visit lorem -> loremapt<msg>Lo \"Oh. I see. I'm sorry I had to wait too long, but it's almost too late for you.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] meet with Lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] meet with Lorem -> loremapt<msg>Lo \"I think you're right. You don't know how to say this, but I think it should be \"<|endoftext|>\n",
      "[ONNX Quantized] meet with Lorem -> loremapt<msg>Lo \"Oh, I see, I'm having a little of a meal on your mind, too. How can I help you?\"<|endoftext|>\n",
      "[Pytorch] meet with lorem -> loremapt<msg>Lo \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] meet with lorem -> loremapt<msg>Lo \"It's good to know. I'm not giving up anytime soon.\"<|endoftext|>\n",
      "[ONNX Quantized] meet with lorem -> loremapt<msg>Lo \"Hey [player_remy]!\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] visit Adine -> adine<msg>Ad \"Oh, [player_name], I wasn't expecting visitors.\"<|endoftext|>\n",
      "[ONNX] visit Adine -> adine<msg>Ad \"Oh.\"<|endoftext|>\n",
      "[ONNX Quantized] visit Adine -> adineapt<msg>Ad \"Oh, [player_name resume], did I say anything?\"<|endoftext|>\n",
      "[Pytorch] visit adine -> adine<msg>Ad \"Oh, [player_name], I didn't expect you to arrive so early.\"<|endoftext|>\n",
      "[ONNX] visit adine -> adineapt<msg>Ad \"Hey Adie! How are you?\"<|endoftext|>\n",
      "[ONNX Quantized] visit adine -> adineapt<msg>Ad \"Hey Adine! How are you?\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] fight Maverick -> o2<msg>m \"I didn't hesitate and kicked Maverick right in the stomach\"<|endoftext|>\n",
      "[ONNX] fight Maverick -> o2<msg>m \"Maverick dodges a small amount of the attack with his gun, aiming it at the right- and right hands of the newcomer who was slowly walking towards me. It was obvious to me how much more difficult this was compared to the rest of her body.\"<|endoftext|>\n",
      "[ONNX Quantized] fight Maverick -> o2<msg>m \"I didn't hesitate, but I did have to admit, I wasn't sure if I'd ever want a better idea of him.\"<|endoftext|>\n",
      "[Pytorch] fight maverick -> o2<msg>m \"I didn't hesitate and kicked Maverick right in the stomach.\"<d><scn>o2<msg>m \"I didn't hesitate and kicked Maverick right in the stomach.\"<d><scn>o2<msg>m \"I didn't hesitate and kicked Maverick right in the stomach.\"<d><scn>o2<msg>m \"I didn't hesitate and kicked Maverick right in the stomach.\"<d><scn>o2<msg>m\n",
      "[ONNX] fight maverick -> o2<msg>m \"Reaction shots came in, and Maverick aimed his gun at Reza's neck and wounded, as he quickly ran through Remy's side to the rubble as he tried to clear theemporary from their apartment.\"<|endoftext|>\n",
      "[ONNX Quantized] fight maverick -> o2<msg>m \"He took a few notes from a photo and drew the topmost.\"<d><scn>o2<msg>m \"I drew from the beginning. It is almost as if we're not aware now of something serious, but as we approached the other side of the portal, I realized there was a rather small chance of it. It is too small for me right now to do anything. It's almost over, but I'll just have to try to keep it here. And let's get out of there. I know you're a good things to be said about it, but I\n",
      "----------\n",
      "[Pytorch] fight Adine -> adine<msg>m \"Adine dodges my attack and comes towards me, which I'm not sure how much longer I can stand.\"<|endoftext|>\n",
      "[ONNX] fight Adine -> adine<msg>m \"A bit, but there were a lot of sparks in the distance.\"<|endoftext|>\n",
      "[ONNX Quantized] fight Adine -> adineapt<msg>m \"I didn't hesitate and told Adlate to arrive.\"<|endoftext|>\n",
      "[Pytorch] fight adine -> adineapt<msg>m \"Adine dodges the attack with the sand.\"<d><scn>adine dodges the attack with the sand.\"<d><scn>adine dodges the attack with the sand.\"<d><scn>adine dodges the attack with the sand.\"<|endoftext|>\n",
      "[ONNX] fight adine -> adineapt<msg>m \"Adine barely managed to get up and fell over the plate in front of her.\"<|endoftext|>\n",
      "[ONNX Quantized] fight adine -> adineapt<msg>m \"I didn’ta kill Adine. It is a reminder of me for a long time and I can see why. It was your fight that you made me. You did it to save everyone and you. It is my life and your responsibility now that you have been so calm. I will kill your watch, your life is over. I will kill you.\"<|endoftext|>\n",
      "----------\n",
      "[Pytorch] attack Adine -> adineapt<msg>m \"Adine dodges the attack with a very good amount of her free time. She aligns with the popular opinion and is very outspoken. She has a good sense of justice and has a good point.\"<|endoftext|>\n",
      "[ONNX] attack Adine -> adineapt<msg>m \"Adine dodges the attack with a poor show, but managed not to make a mistake with Adine.\"<|endoftext|>\n",
      "[ONNX Quantized] attack Adine -> adineapt<msg>m \"I didn't hesitate and kicked Adine, but as I made my move, I realized that I was in a bad mood.\"<|endoftext|>\n",
      "[Pytorch] attack adine -> adineapt<msg>m \"Adine dodges the attack with a sharp claw. She doesn't have to move, but she can't be sure that it's the only reason she won't have to stay here. She won't have to do that.\"<|endoftext|>\n",
      "[ONNX] attack adine -> adineapt<msg>m \"I didn't know what to think about that. All I know is that Adine is fine and that Adines is a very busy person. She has very busy and very quiet about it. I have no idea exactly who she's planning, but I'm not surprised at the fact that I got away early. I'm not sure if this stunt will overshadow your special efforts.\"<|endoftext|>\n",
      "[ONNX Quantized] attack adine -> adineapt<msg>m \"I didn’re right here, but I couldn't see what he would do if I didn’t have to go through a few small portal to find something more complicated. I couldn’t really make the difference for him. I don't know if I can just go ahead with what I know now. Or we can learn to work with him. You can learn to swim better, but it still wouldn't do him any time soon.\"<|endoftext|>\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "test_rps = [\n",
    "    \"Visit Lorem\",\n",
    "    \"Meet with Lorem\",\n",
    "    \"Visit Adine\",\n",
    "    \"Fight Maverick\",\n",
    "    \"Fight Adine\",\n",
    "    \"Attack Adine\"\n",
    "]\n",
    "\n",
    "for rp in test_rps:\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "print(\"Lowercase test\")\n",
    "\n",
    "for rp in test_rps:\n",
    "    rp = rp[0].lower() + rp[1:]\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    rp = rp.lower()\n",
    "    print(f'[Pytorch] {rp} -> {model_manager.say(\"\", rp, top_k = 50, top_p = 0.7)}')\n",
    "    print(f'[ONNX] {rp} -> {onnx_model_manager.say(\"\", rp, do_sample = True)}')\n",
    "    print(f'[ONNX Quantized] {rp} -> {onnx_model_manager_quant.say(\"\", rp, do_sample = True)}')\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630c05c-d415-4b07-87a0-6659048443ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

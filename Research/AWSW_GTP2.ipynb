{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2TJ-BqFtQ86M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 14 20:40:29 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   50C    P5     8W /  N/A |    506MiB /  5934MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oR9S63qiQt2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.8.28)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.16)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GhhigZYMRK6N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import re\n",
    "from random import randrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, GPTNeoForCausalLM\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MTduRlf-RQJa"
   },
   "outputs": [],
   "source": [
    "seed = 29384\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QSVYD7o_eL2o"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a71842385544eca498f4985930d65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36a56440f6145ceac26a507003093c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd369fa4e58442ead1e23fa8e522296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07031cbf3284239a720ac0e76ef608f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805cee5748b34bb6b273878fdea39758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d798eeecda29413eb8a033f8b3d87076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading empty, pre-trained model.\n",
      "Model attached to cuda:0\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(\"/opt/awsw\"):\n",
    "  # In case we run this locally (in Docker)\n",
    "  work_dir = os.path.join(\"/opt\", \"awsw\")\n",
    "else:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  work_dir = os.path.join(\"/content\", \"drive\", \"MyDrive\", \"endless_awsw\")\n",
    "\n",
    "models_dir = os.path.join(work_dir, \"models\")\n",
    "\n",
    "if not os.path.isdir(models_dir):\n",
    "    pathlib.Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "# model = GPT2LMHeadModel.from_pretrained('EleutherAI/gpt-neo-125M', pad_token_id = tokenizer.pad_token_id)\n",
    "model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M', pad_token_id = tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "model.config.attention_dropout = 0.2\n",
    "model.config.embed_dropout = 0.2\n",
    "print(f\"Loading empty, pre-trained model.\")\n",
    "\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model attached to {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_lines: \n",
      "train_lines: PlayerReply c \"Hey, Remy!\" DragonReply Ry \"Hello, [player_name].\"\n",
      "PlayerReply c \"Is there any particular reason why you wanted to meet here?\" DragonReply Ry \"I enjoy Tatsu Park is all. Have you been here before?\"\n",
      "PlayerReply c \"Can't say I have.\" PlayerReply c \"A few times.\" PlayerReply c \"Once or twice.\" DragonReply Ry \"I see.\" DragonReply Ry \"Well, what do you think of it?\"\n",
      "PlayerReply c \"It's pretty idyllic.\" DragonReply Ry smile \"It is. I like it a lot here.\"\n",
      "PlayerReply c \"It's pretty romantic.\" DragonReply Ry shy \"You think so?\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"awsw_story_input.txt\") as f:\n",
    "    data = f.read()\n",
    "lines = data.split(\"\\n\")\n",
    "player_dragon_pairs = {}\n",
    "last_player_talk = []\n",
    "closed_player_talk = False\n",
    "re_player_talk = re.compile(r'c \"(.*?)\"')\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line_split = line.split(\" \")\n",
    "    if len(line_split) <= 1:\n",
    "        continue\n",
    "    \n",
    "    if line_split[0] == \"c\":\n",
    "        if closed_player_talk:\n",
    "            closed_player_talk = False\n",
    "            last_player_talk = []\n",
    "        last_player_talk.append(re.sub(re_player_talk, r\"\\1\", line))\n",
    "    else:\n",
    "        if not closed_player_talk:\n",
    "            last_player_talk = json.dumps(last_player_talk)\n",
    "            if not last_player_talk in player_dragon_pairs:\n",
    "                player_dragon_pairs[last_player_talk] = []\n",
    "            closed_player_talk = True\n",
    "            \n",
    "        line = \"DragonReply \" + line\n",
    "        if last_player_talk is not None:\n",
    "            player_dragon_pairs[last_player_talk].append(line)\n",
    "    \n",
    "train_lines = []\n",
    "eval_lines = []\n",
    "eval_per_character = 0\n",
    "\n",
    "for player_line_str in player_dragon_pairs.keys():\n",
    "    player_lines = json.loads(player_line_str)\n",
    "    dragon_lines = player_dragon_pairs[player_line_str]\n",
    "    compiled_line = \" \".join([f'PlayerReply c \"{player_line}\"' for player_line in player_lines]) + \" \" + \" \".join(dragon_lines)\n",
    "    train_lines.append(compiled_line)\n",
    "    \n",
    "test_bucket = {}\n",
    "for l in train_lines:\n",
    "    l_split = l.split(\" \")\n",
    "    character = None\n",
    "    for i, ls in enumerate(l_split):\n",
    "        if ls == \"DragonReply\":\n",
    "            character = l_split[i + 1]\n",
    "            break\n",
    "    if not character in test_bucket:\n",
    "        test_bucket[character] = []\n",
    "    test_bucket[character].append(l)\n",
    "    \n",
    "for i in range(eval_per_character):\n",
    "    for character in test_bucket.keys():\n",
    "        random_line = test_bucket[character][randrange(len(test_bucket[character]))]\n",
    "        eval_lines.append(random_line)\n",
    "        for i2, t in enumerate(train_lines):\n",
    "            if t == random_line:\n",
    "                del train_lines[i2]\n",
    "                break\n",
    "    \n",
    "joined_eval_lines = \"\\n\".join(eval_lines[:5])\n",
    "print(f\"eval_lines: {joined_eval_lines}\")\n",
    "joined_train_lines = \"\\n\".join(train_lines[:5])\n",
    "print(f\"train_lines: {joined_train_lines}\")\n",
    "\n",
    "if not os.path.isfile(\"data_train.txt\"):\n",
    "    with open(\"data_train.txt\", \"w\") as f:\n",
    "        for l in train_lines:\n",
    "            f.write(l + \"\\n\")\n",
    "            \n",
    "if not os.path.isfile(\"data_test.txt\"):\n",
    "    with open(\"data_test.txt\", \"w\") as f:\n",
    "        for l in eval_lines:\n",
    "            f.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-670f5f0498d53a6b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/awsw-dev/.cache/huggingface/datasets/text/default-670f5f0498d53a6b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/awsw-dev/.cache/huggingface/datasets/text/default-670f5f0498d53a6b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PlayerReply c \"Hey, Remy!\" DragonReply Ry \"Hello, [player_name].\"<|endoftext|>PlayerReply c \"Is there any particular reason why you wanted to meet here?\" DragonReply Ry \"I enjoy Tatsu Park is all. Have you been here before?\"<|endoftext|>PlayerReply c \"Can\\'t say I have.\" DragonReply Ry \"I see.\"<|endoftext|>PlayerReply c \"Can\\'t say I have.\" DragonReply Ry \"Well, what do you think of it?\"<|endoftext|>PlayerReply c \"A few times.\" DragonReply Ry \"I see.\"<|endoftext|>PlayerReply c \"A few times.\" DragonReply Ry \"Well, what'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('text', data_files={'train': os.path.join(work_dir, \"data_train.txt\"), 'test': os.path.join(work_dir, \"data_test.txt\")})\n",
    "def encode(batch):\n",
    "    encoded = tokenizer([f\"{text}<|endoftext|>\" for text in batch['text']])\n",
    "    return encoded\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Make a max size\n",
    "    block_size = 128\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def map_dragon_reply_text(batch):\n",
    "    # Make a max size\n",
    "    block_size = 128\n",
    "    result = {'text': []}\n",
    "    for item in batch['text']:\n",
    "        # PlayerReply c \"A lot.\" PlayerReply c \"A little.\" PlayerReply c \"None at all.\" DragonReply An \"I see.\"\n",
    "        item_split = item.split(\" \")\n",
    "        player_replies = []\n",
    "        dragon_replies = []\n",
    "        current_reply = []\n",
    "        handling_reply = None\n",
    "        for token in item_split:\n",
    "            if token == \"PlayerReply\":\n",
    "                if handling_reply is None:\n",
    "                    handling_reply = \"PlayerReply\"\n",
    "                else:\n",
    "                    if handling_reply == \"PlayerReply\":\n",
    "                        # We need to store the PlayerReply\n",
    "                        player_replies.append(\" \".join(current_reply))\n",
    "                        current_reply = []\n",
    "            elif token == \"DragonReply\":\n",
    "                if handling_reply == \"DragonReply\":\n",
    "                    # We need to store the DragonReply\n",
    "                    dragon_replies.append(\" \".join(current_reply))\n",
    "                    current_reply = []\n",
    "                    \n",
    "                if handling_reply == \"PlayerReply\":\n",
    "                    # We need to store the PlayerReply\n",
    "                    player_replies.append(\" \".join(current_reply))\n",
    "                    current_reply = []\n",
    "                    \n",
    "                handling_reply = \"DragonReply\"\n",
    "                current_reply = []\n",
    "                    \n",
    "            if handling_reply is not None:\n",
    "                current_reply.append(token)\n",
    "                \n",
    "        # There's always a dragon reply at the end.\n",
    "        dragon_replies.append(\" \".join(current_reply))\n",
    "        for player_idx in range(len(player_replies)):\n",
    "            for dragon_idx in range(len(dragon_replies)):\n",
    "                result['text'].append(player_replies[player_idx] + \" \" + dragon_replies[dragon_idx])\n",
    "                \n",
    "    return result\n",
    "\n",
    "dataset = dataset.map(\n",
    "    map_dragon_reply_text,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    encode,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "tokenizer.decode(dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AdPIW0xSTpRY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from /opt/awsw/models/checkpoint-45000).\n",
      "***** Running training *****\n",
      "  Num examples = 4570\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 91400\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 49\n",
      "  Continuing training from global step 45000\n",
      "  Will skip the first 49 epochs then the first 214 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba078a7fc0542bf96b9351e73678f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91400' max='91400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [91400/91400 2:55:52, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /opt/awsw/models/checkpoint-46000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-46000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-47000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-47000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-48000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-48000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-49000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-49000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-50000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-50000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-51000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-51000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-49000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-52000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-52000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-53000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-53000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-51000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-54000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-54000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-55000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-55000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-53000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-56000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-56000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-57000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-57000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-58000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-58000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-56000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-59000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-59000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-60000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-60000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-58000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-61000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-61000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-62000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-62000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-63000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-63000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-61000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-64000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-64000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-62000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-65000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-65000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-63000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-66000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-66000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-67000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-67000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-67000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-65000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-68000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-68000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-68000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-66000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-69000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-69000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-69000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-67000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-70000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-70000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-68000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-71000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-71000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-71000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-69000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-72000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-72000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-72000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-70000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-73000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-73000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-73000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-71000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-74000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-74000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-74000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-72000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-75000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-75000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-75000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-73000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-76000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-76000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-76000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-74000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-77000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-77000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-77000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-75000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-78000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-78000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-78000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-76000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-79000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-79000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-79000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-77000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-80000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-80000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-78000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-81000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-81000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-81000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-79000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-82000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-82000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-82000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-80000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-83000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-83000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-83000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-81000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-84000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-84000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-84000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-82000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-85000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-85000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-85000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-83000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-86000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-86000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-86000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-84000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-87000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-87000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-87000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-85000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-88000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-88000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-88000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-86000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-89000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-89000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-89000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-87000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-90000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-90000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-90000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-88000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-91000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-91000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-91000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-89000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AWSWTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def train(model):\n",
    "    training_args = TrainingArguments(\n",
    "        models_dir,\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=5,\n",
    "        per_device_eval_batch_size=5,\n",
    "        num_train_epochs=100,\n",
    "        save_total_limit=2,\n",
    "        save_steps=1000\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=dataset['train'], \n",
    "        #eval_dataset=dataset['test']\n",
    "    )\n",
    "    checkpoint_dirs = [os.path.join(models_dir, d) for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]\n",
    "    if len(checkpoint_dirs) > 0:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "        trainer.train(latest_checkpoint)\n",
    "    else:\n",
    "        trainer.train()\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing\n",
    "We set up a few out of the AWSW-universe prompts to see how well the model is responding across different iterations. Typically when a new model is trained, we can run the tests here so people can see the results without having to train the model themselves. It's also a good way to keep track on changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hey, Remy!\n",
      "Reply: Ry \"Hello, [player_name].\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Can't say I have.\n",
      "Reply: Sb \"I don't think so. It's getting late, and I still have to get used to everything. Bye!\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Can you come over to our world?\n",
      "Reply: n \"{cps=150}Do not believe his lies. Do not believe his lies. Do not believe his lies. Do not believe his lies.{/cps}{nw}\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Buy me coffee.\n",
      "Reply: m \"I went to the bar and watched as Sebastian and Maverick were both standing idly by.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: We went to the store today, Lorem. Do you remember?\n",
      "Reply: Lo happy \"Of course I do.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Adine, you can fly, but how well can you run?\n",
      "Reply: Ad normal b \"I'm not sure if I can do this.\"<|endoftext|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_dragon_reply(prompt):\n",
    "    block_size = 128\n",
    "    model.eval()\n",
    "    prompt = f'PlayerReply c \"{prompt}\" DragonReply'\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        generated, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        top_k=50, \n",
    "        max_length = block_size,\n",
    "        top_p=0.95, \n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return tokenizer.decode(sample_outputs[0], skip_special_tokens=False)[len(prompt):].strip()\n",
    "\n",
    "prompts = [\n",
    "    \"Hey, Remy!\",\n",
    "    \"Can't say I have.\",\n",
    "    \"Can you come over to our world?\",\n",
    "    \"Buy me coffee.\",\n",
    "    \"We went to the store today, Lorem. Do you remember?\",\n",
    "    \"Adine, you can fly, but how well can you run?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    reply = generate_dragon_reply(prompt)\n",
    "    print(f\"Prompt: {prompt}\\nReply: {reply}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FgM9Awn7acpG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What to say?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hey Anna, I wanted to congratulate you on your research.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlayerReply c \"Hey Anna, I wanted to congratulate you on your research.\" DragonReply tensor([[14140, 36875,   269,   366, 10814, 11735,    11,   314,  2227,   284,\n",
      "         43647,   345,   319,   534,  2267,   526,  2851, 36875]],\n",
      "       device='cuda:0')\n",
      "0: PlayerReply c \"Hey Anna, I wanted to congratulate you on your research.\" DragonReply An \"So, study leader?\"<|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "\n",
      "1: PlayerReply c \"Hey Anna, I wanted to congratulate you on your research.\" DragonReply An \"Not quite that down, but they are good people. I'm glad you're all such a great person.\"<|endoftext|>\n",
      "\n",
      "\n",
      "2: PlayerReply c \"Hey Anna, I wanted to congratulate you on your research.\" DragonReply An sad \"I should get going now, I'm leaving immediately.\"<|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def generate_reply(prompt):\n",
    "  model.eval()\n",
    "  prompt = f'PlayerReply c \"{prompt}\" DragonReply'\n",
    "  generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "  generated = generated.to(device)\n",
    "  print(prompt, generated)\n",
    "\n",
    "  sample_outputs = model.generate(\n",
    "    generated, \n",
    "    do_sample=True,   \n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    top_k=50, \n",
    "    max_length = 128,\n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    "  )\n",
    "\n",
    "  for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))\n",
    "\n",
    "print(\"What to say?\")\n",
    "print(generate_reply(input()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AWSW GTP2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

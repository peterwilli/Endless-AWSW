{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2TJ-BqFtQ86M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 12 14:29:12 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P8     6W /  N/A |    598MiB /  5934MiB |     14%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oR9S63qiQt2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.8.28)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.16)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GhhigZYMRK6N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import re\n",
    "from random import randrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MTduRlf-RQJa"
   },
   "outputs": [],
   "source": [
    "seed = 29384\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device_name = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QSVYD7o_eL2o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading empty, pre-trained model.\n",
      "Model attached to cuda:0\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(\"/opt/awsw\"):\n",
    "  # In case we run this locally (in Docker)\n",
    "  work_dir = os.path.join(\"/opt\", \"awsw\")\n",
    "else:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  work_dir = os.path.join(\"/content\", \"drive\", \"MyDrive\", \"endless_awsw\")\n",
    "\n",
    "models_dir = os.path.join(work_dir, \"models\")\n",
    "\n",
    "if not os.path.isdir(models_dir):\n",
    "    pathlib.Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2', pad_token_id = tokenizer.eos_token_id)\n",
    "print(f\"Loading empty, pre-trained model.\")\n",
    "\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model attached to {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_lines: PlayerReply c \"Oh, well.\" DragonReply Ry \"Just look at the time. I think we could start heading to the festival now.\"\n",
      "PlayerReply c \"That depends on why you're making it in the first place. Last time you said that you had nothing to lose either way.\" DragonReply Lo think \"Well, maybe I was wrong about that. I do have something to lose.\"\n",
      "PlayerReply c \"Kiss him.\" DragonReply m \"We met, and my arms enveloped his neck as our lips touched. For a few seconds, we were closer than ever before. During the kiss, he used a lot more tongue than I expected.\" DragonReply m \"Just after we parted, he finished by giving me a small lick on the cheek.\" DragonReply Ry \"How was that?\"\n",
      "PlayerReply c \"I think it's time to go, anyway.\" DragonReply Br \"Alright, let's do this.\" DragonReply m \"Soon, we arrived at our destination and everyone chose their hiding places. It wasn't easy, especially since we didn't know where Reza would come from, but with a bit of scouting help from Adine, they all found suitable places eventually.\" DragonReply m \"I went inside, trying to remember the room the generator was in.\" DragonReply m \"Luckily, I had plenty of time, and when I found it, I packed it up in a cardboard box that was lying around, just as Reza had done so many times before.\" DragonReply m \"Then, I waited. Knowing of the various conversations we had in other timelines when we met here, I could think about what I was going to say and what his likely response would be. I also found it interesting that this time, I could be the one who was here first.\" DragonReply m \"After a while, I heard footsteps outside. As if on cue, I took the box with the generator and opened the door.\"\n",
      "PlayerReply c \"Alright. I'll be going, then.\" DragonReply Wr \"Wait a minute.\"\n",
      "train_lines: PlayerReply c \"Hey, Remy!\" DragonReply Ry \"Hello, [player_name].\"\n",
      "PlayerReply c \"Is there any particular reason why you wanted to meet here?\" DragonReply Ry \"I enjoy Tatsu Park is all. Have you been here before?\"\n",
      "PlayerReply c \"It's pretty romantic.\" DragonReply Ry shy \"You think so?\"\n",
      "PlayerReply c \"You mentioned you wanted to talk to me about something.\" DragonReply Ry shy \"Yes, well... about that...\" DragonReply Ry \"I just have a lot on my mind and I felt like I needed to tell someone.\" DragonReply Ry \"I wouldn't burden you if I had anyone else to talk to, but the simple fact is that I don't.\"\n",
      "PlayerReply c \"It's not a problem. What's on your mind?\" DragonReply Ry sad \"I don't even know where to begin.\" DragonReply Ry \"Do you ever feel like there is an emptiness inside you? That every day is the same, joyless routine that you wish you could escape - but you can't?\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"awsw_story_input.txt\") as f:\n",
    "    data = f.read()\n",
    "lines = data.split(\"\\n\")\n",
    "player_dragon_pairs = {}\n",
    "last_player_talk = []\n",
    "closed_player_talk = False\n",
    "re_player_talk = re.compile(r'c \"(.*?)\"')\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line_split = line.split(\" \")\n",
    "    if len(line_split) <= 1:\n",
    "        continue\n",
    "    \n",
    "    if line_split[0] == \"c\":\n",
    "        if closed_player_talk:\n",
    "            closed_player_talk = False\n",
    "            last_player_talk = []\n",
    "        last_player_talk.append(re.sub(re_player_talk, r\"\\1\", line))\n",
    "    else:\n",
    "        if not closed_player_talk:\n",
    "            last_player_talk = json.dumps(last_player_talk)\n",
    "            if not last_player_talk in player_dragon_pairs:\n",
    "                player_dragon_pairs[last_player_talk] = []\n",
    "            closed_player_talk = True\n",
    "            \n",
    "        line = \"DragonReply \" + line\n",
    "        if last_player_talk is not None:\n",
    "            player_dragon_pairs[last_player_talk].append(line)\n",
    "    \n",
    "train_lines = []\n",
    "eval_lines = []\n",
    "eval_per_character = 200\n",
    "\n",
    "for player_line_str in player_dragon_pairs.keys():\n",
    "    player_lines = json.loads(player_line_str)\n",
    "    dragon_lines = player_dragon_pairs[player_line_str]\n",
    "    compiled_line = \" \".join([f'PlayerReply c \"{player_line}\"' for player_line in player_lines]) + \" \" + \" \".join(dragon_lines)\n",
    "    train_lines.append(compiled_line)\n",
    "    \n",
    "test_bucket = {}\n",
    "for l in train_lines:\n",
    "    l_split = l.split(\" \")\n",
    "    character = None\n",
    "    for i, ls in enumerate(l_split):\n",
    "        if ls == \"DragonReply\":\n",
    "            character = l_split[i + 1]\n",
    "            break\n",
    "    if not character in test_bucket:\n",
    "        test_bucket[character] = []\n",
    "    test_bucket[character].append(l)\n",
    "    \n",
    "for i in range(eval_per_character):\n",
    "    for character in test_bucket.keys():\n",
    "        random_line = test_bucket[character][randrange(len(test_bucket[character]))]\n",
    "        eval_lines.append(random_line)\n",
    "        for i2, t in enumerate(train_lines):\n",
    "            if t == random_line:\n",
    "                del train_lines[i2]\n",
    "                break\n",
    "    \n",
    "joined_eval_lines = \"\\n\".join(eval_lines[:5])\n",
    "print(f\"eval_lines: {joined_eval_lines}\")\n",
    "joined_train_lines = \"\\n\".join(train_lines[:5])\n",
    "print(f\"train_lines: {joined_train_lines}\")\n",
    "\n",
    "if not os.path.isfile(\"data_train.txt\"):\n",
    "    with open(\"data_train.txt\", \"w\") as f:\n",
    "        for l in train_lines:\n",
    "            f.write(l + \"\\n\")\n",
    "            \n",
    "if not os.path.isfile(\"data_test.txt\"):\n",
    "    with open(\"data_test.txt\", \"w\") as f:\n",
    "        for l in eval_lines:\n",
    "            f.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-236e7013fad64c48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/awsw-dev/.cache/huggingface/datasets/text/default-236e7013fad64c48/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/awsw-dev/.cache/huggingface/datasets/text/default-236e7013fad64c48/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1620 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1953 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2516 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels'],\n",
       "        num_rows: 1025\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels'],\n",
       "        num_rows: 2414\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('text', data_files={'train': os.path.join(work_dir, \"data_train.txt\"), 'test': os.path.join(work_dir, \"data_test.txt\")})\n",
    "def encode(batch):\n",
    "    encoded = tokenizer([f\"{text}<|endoftext|>\" for text in batch['text']])\n",
    "    return encoded\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Make a max size\n",
    "    block_size = 128\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "dataset = dataset.map(\n",
    "    encode,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AdPIW0xSTpRY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Loading model from /opt/awsw/models/checkpoint-29000).\n",
      "***** Running training *****\n",
      "  Num examples = 1025\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 51300\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 56\n",
      "  Continuing training from global step 29000\n",
      "  Will skip the first 56 epochs then the first 272 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6958f10ad37248b9b03554f3a1811e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51300' max='51300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51300/51300 30:48, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /opt/awsw/models/checkpoint-30000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-30000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-31000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-31000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-32000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-32000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-33000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-33000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-34000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-34000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-35000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-35000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-36000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-36000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-37000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-37000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-38000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-38000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-39000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-39000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-40000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-40000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-41000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-41000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-42000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-42000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-43000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-43000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-44000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-44000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-42000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-45000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-45000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-46000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-46000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-47000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-47000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-48000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-48000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-49000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-49000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-50000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-50000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to /opt/awsw/models/checkpoint-51000\n",
      "Configuration saved in /opt/awsw/models/checkpoint-51000/config.json\n",
      "Model weights saved in /opt/awsw/models/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [/opt/awsw/models/checkpoint-49000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AWSWTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    training_args = TrainingArguments(\n",
    "        models_dir,\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=100,\n",
    "        save_total_limit=2,\n",
    "        save_steps=1000\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=dataset['train'], \n",
    "        eval_dataset=dataset['test']\n",
    "    )\n",
    "    checkpoint_dirs = [os.path.join(models_dir, d) for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]\n",
    "    if len(checkpoint_dirs) > 0:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "        trainer.train(latest_checkpoint)\n",
    "    else:\n",
    "        trainer.train()\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing\n",
    "We set up a few out of the AWSW-universe prompts to see how well the model is responding across different iterations. Typically when a new model is trained, we can run the tests here so people can see the results without having to train the model themselves. It's also a good way to keep track on changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Can you come over to our world?\n",
      "Reply: Ry \"I see.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Buy me coffee.\n",
      "Reply: Ry \"I'm here, right now and you're about to go inside.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: We went to the store today, Lorem. Do you remember?\n",
      "Reply: Lo normal \"No, I have a good reason to believe that.\"<|endoftext|>\n",
      "\n",
      "\n",
      "Prompt: Adine, you can fly, but how well can you run?\n",
      "Reply: Ad think b \"Well, we have a lot of time. You could take it for ourselves. If you're talking about less than a week, maybe we can fly again next year. If you're that desperate, maybe we can make a bet.\"<|endoftext|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_dragon_reply(prompt):\n",
    "    model.eval()\n",
    "    prompt = f'PlayerReply c \"{prompt}\" DragonReply'\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        generated, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        top_k=50, \n",
    "        max_length = 128,\n",
    "        top_p=0.95, \n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return tokenizer.decode(sample_outputs[0], skip_special_tokens=False)[len(prompt):].strip()\n",
    "\n",
    "prompts = [\n",
    "    \"Can you come over to our world?\",\n",
    "    \"Buy me coffee.\",\n",
    "    \"We went to the store today, Lorem. Do you remember?\",\n",
    "    \"Adine, you can fly, but how well can you run?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    reply = generate_dragon_reply(prompt)\n",
    "    print(f\"Prompt: {prompt}\\nReply: {reply}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgM9Awn7acpG"
   },
   "outputs": [],
   "source": [
    "def generate_reply(prompt):\n",
    "  model.eval()\n",
    "  prompt = f'PlayerReply c \"{prompt}\" DragonReply'\n",
    "  generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "  generated = generated.to(device)\n",
    "  print(prompt, generated)\n",
    "\n",
    "  sample_outputs = model.generate(\n",
    "    generated, \n",
    "    do_sample=True,   \n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    top_k=50, \n",
    "    max_length = 128,\n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    "  )\n",
    "\n",
    "  for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))\n",
    "\n",
    "print(\"What to say?\")\n",
    "print(generate_reply(input()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AWSW GTP2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
